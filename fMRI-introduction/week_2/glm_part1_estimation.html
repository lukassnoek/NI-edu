
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The GLM, part 1: estimation &#8212; NI-edu</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="The GLM, part 2: inference" href="../week_3/glm_part2_inference.html" />
    <link rel="prev" title="Using the GLM to model fMRI data" href="../../section_intros/2_glm.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/fmri.gif" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">NI-edu</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../index.html">
   Welcome to NI-edu
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../getting_started/about.html">
   About this course
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../getting_started/installation.html">
   Installation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  fMRI-introduction
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../section_intros/1_python.html">
   Python for (f)MRI analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../other/python_recap.html">
     Python recap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_1/python_for_mri.html">
     Working with MRI data in Python (T)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../section_intros/2_glm.html">
   Using the GLM to model fMRI data
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     The GLM: estimation (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_3/glm_part2_inference.html">
     The GLM: inference (T)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../section_intros/3_design_of_experiments_T.html">
   Design of experiments
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_3/design_of_experiments.html">
     Design of experiments (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_3/neurodesign.html">
     Neurodesign (T)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../section_intros/4_preprocessing.html">
   Preprocessing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_4/temporal_preprocessing.html">
     Temporal preprocessing (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_4/spatial_preprocessing.html">
     Spatial preprocessing (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_4/fmriprep.html">
     Fmriprep (T)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../section_intros/5_multilevel.html">
   First &amp; run-level analyses
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_5/linux_and_the_command_line.html">
     Linux and the CMD (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_5/first_level_analyses.html">
     First level analyses (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_5/run_level_analyses.html">
     Run-level analyses (T)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../section_intros/6_grouplevel.html">
   Group-level analyses
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_6/group_level_analyses.html">
     Group-level analyses (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_6/MCC.html">
     Multiple comparison correction (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_6/ROI_analysis.html">
     ROI analysis (T)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../section_intros/7_nilearn.html">
   Introduction to Nilearn
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_7/nilearn.html">
     Introduction to Nilearn (T)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../week_7/nilearn_stats.html">
     Statistics with Nilearn (T)
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  fMRI-pattern-analysis
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../fMRI-pattern-analysis/week_1/design_and_pattern_estimation.html">
   Design and pattern estimation (T)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../fMRI-pattern-analysis/week_2/decoding_analyses.html">
   Machine learning/decoding (T)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../fMRI-pattern-analysis/week_3/rsa.html">
   Representational Similarity Analysis (T)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Misc
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../misc/bibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../misc/for_educators.html">
   For educators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../misc/CONTRIBUTING.html">
   Contributing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../misc/CONDUCT.html">
   Code of Conduct
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/fMRI-introduction/week_2/glm_part1_estimation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/lukassnoek/NI-edu"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/lukassnoek/NI-edu/issues/new?title=Issue%20on%20page%20%2FfMRI-introduction/week_2/glm_part1_estimation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/lukassnoek/NI-edu/edit/master/NI-edu/fMRI-introduction/week_2/glm_part1_estimation.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/lukassnoek/NI-edu/master?urlpath=tree/NI-edu/fMRI-introduction/week_2/glm_part1_estimation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://neuroimaging.lukas-snoek.com/hub/user-redirect/git-pull?repo=https://github.com/lukassnoek/NI-edu&urlpath=tree/NI-edu/NI-edu/fMRI-introduction/week_2/glm_part1_estimation.ipynb&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recap-of-linear-regression">
   Recap of linear regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notation">
     Notation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modeling-the-intercept-offset">
     Modeling the intercept (offset)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameters-in-linear-regression">
     Parameters in linear regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refresher-matrix-multiplication">
     Refresher: matrix multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-ols-models">
     Fitting OLS models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#residuals-and-model-fit">
     Residuals and model fit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-linear-regression">
     Summary: linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#glm-in-fmri-analyses">
   GLM in fMRI analyses
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-target">
     The target
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-predictors-or-what-should-we-use-to-model-our-target">
     The predictors, or: what should we use to model our target?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-independent-variables">
       Defining independent variables
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#resampling">
       Resampling
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-on-fmri-data-interpretation-parameters">
     Regression on fMRI data &amp; interpretation parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-the-bold-response-in-glm-models">
     Using the BOLD response in GLM models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-canonical-hrf">
       The canonical HRF
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#convolution">
       Convolution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-scaling">
       Linear scaling
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#resampling-revisited">
       Resampling revisited
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#initial-upsampling-of-predictors">
       Initial upsampling of predictors
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-an-hrf-informed-model">
       Fitting an HRF-informed model
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#temporal-basis-functions">
       Temporal basis functions
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>The GLM, part 1: estimation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recap-of-linear-regression">
   Recap of linear regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notation">
     Notation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modeling-the-intercept-offset">
     Modeling the intercept (offset)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameters-in-linear-regression">
     Parameters in linear regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refresher-matrix-multiplication">
     Refresher: matrix multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fitting-ols-models">
     Fitting OLS models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#residuals-and-model-fit">
     Residuals and model fit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-linear-regression">
     Summary: linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#glm-in-fmri-analyses">
   GLM in fMRI analyses
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-target">
     The target
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-predictors-or-what-should-we-use-to-model-our-target">
     The predictors, or: what should we use to model our target?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-independent-variables">
       Defining independent variables
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#resampling">
       Resampling
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-on-fmri-data-interpretation-parameters">
     Regression on fMRI data &amp; interpretation parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-the-bold-response-in-glm-models">
     Using the BOLD response in GLM models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-canonical-hrf">
       The canonical HRF
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#convolution">
       Convolution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-scaling">
       Linear scaling
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#resampling-revisited">
       Resampling revisited
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#initial-upsampling-of-predictors">
       Initial upsampling of predictors
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-an-hrf-informed-model">
       Fitting an HRF-informed model
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#temporal-basis-functions">
       Temporal basis functions
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="the-glm-part-1-estimation">
<h1>The GLM, part 1: estimation<a class="headerlink" href="#the-glm-part-1-estimation" title="Permalink to this headline">¶</a></h1>
<p>This week will be all about how most univariate fMRI analyses are done: using the <strong>GLM</strong>. Specifically, we’ll focus on the “estimation” part of the GLM (i.e., how do we estimate parameters?).</p>
<p>The GLM, or the General Linear Model, is a statistical model that underlies a range of statistical models that you’re probably already familiar with: (M)ANOVA, <em>t</em>-test, <em>F</em>-test, and most importantly ordinary <em>linear regression</em>. Mathematically, all these different tests are specific instantiations of the GLM (check <a class="reference external" href="https://lindeloev.github.io/tests-as-linear/">this blog post</a> if you want to know more about this equivalence).</p>
<p>Basically, the type of fMRI analysis you are going to learn in this course (often called ‘univariate analysis’ or ‘Statistical Parametric Mapping’) is just an linear regression model applied to time-series data. Given that you have some basic familiarity with these concepts (we which recap in section 1), you will see during this tutorial that univariate fMRI analyses using the GLM are actually very straightforward.</p>
<p>As a consequence of the importance of the GLM, this week’s lab is probably going to take quite long again. So, you’ll have to work hard this week, but it’ll definitely pay off. Also, the material will seem quite mathematical, but it often serves a symbolic purpose: to show you how results are influenced by different parts of the formulas within the GLM. Moreover, after showing and explaining you the formulas, we’ll work it out in code examples (which are often easier to understand!). Also, after explaining a certain aspect of the GLM, we’ll ask you to think about it and practice with it in ToThink and ToDo questions (like last week).</p>
<p><strong>What you’ll learn</strong>: after this week’s lab …</p>
<ul class="simple">
<li><p>you know how to estimate and interpret parameters in a GLM and evaluate its model fit;</p></li>
<li><p>you understand the importance of incorporating knowledge from the BOLD-response in the GLM;</p></li>
<li><p>you are able to implement and apply the GLM to (univariate) fMRI data;</p></li>
</ul>
<p><strong>Estimated time needed to complete</strong>: 6-10 hours <br></p>
<div class="section" id="recap-of-linear-regression">
<h2>Recap of linear regression<a class="headerlink" href="#recap-of-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>To refresh your memory on linear regression, we’ll walk you through a recap of the technique’s most important concepts.
We are going to work through a simple example.</p>
<p>In the code below, <code class="docutils literal notranslate"><span class="pre">y</span></code> will denote our <em>dependent variable</em> (the variable we try to model/explain) and <code class="docutils literal notranslate"><span class="pre">X</span></code> will denote our <em>independent variable(s)</em> (the variables we’re using to try to explain <code class="docutils literal notranslate"><span class="pre">y</span></code>). Throughout the entire tutorial will use <code class="docutils literal notranslate"><span class="pre">X</span></code> to refer to our matrix of independent variables (also called “predictors” or “regressors”, or simply “design matrix”) and use <code class="docutils literal notranslate"><span class="pre">y</span></code> to refer to our dependent variable (also sometimes called “target”).</p>
<p>Moreover, the independent variables are often grouped in a single matrix (a 2D array) — which is sometimes called the “design matrix” (because it ‘designs’ the way we want to model our dependent variable). As stated before, in this tutorial we store our design matrix - the set of our independent variables - in the variable <code class="docutils literal notranslate"><span class="pre">X</span></code> (or slight variations on that, like <code class="docutils literal notranslate"><span class="pre">X_new</span></code> or something). Importantly, it is often assumed (e.g. by statistics functions/software) that the design matrix takes the shape of <span class="math notranslate nohighlight">\(N\ \mathrm{(observations)} \times P\ \mathrm{(predictors)}\)</span>. So, the rows refer to the sampled observations (also often called “samples”, “instances”, or simply “data points”). The columns refer to the separate independent variables that we use to model the dependent variable. For the dependent variable, it is often assumed that this is a single row-vector of shape <span class="math notranslate nohighlight">\(N \times 1\)</span>.</p>
<div class="section" id="notation">
<h3>Notation<a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h3>
<p>Next, let’s define some more conventions in notation. We will denote the total number of observations with <strong><span class="math notranslate nohighlight">\(N\)</span></strong>. Moreover, we’ll denote <strong><span class="math notranslate nohighlight">\(i\)</span></strong> as the index of samples. To give an example, the formula below gives you the sum of our target variable:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c68b2b22-3134-4b57-900f-7d42bfa0027f">
<span class="eqno">(3)<a class="headerlink" href="#equation-c68b2b22-3134-4b57-900f-7d42bfa0027f" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathrm{sum}(\mathbf{y}) = \sum_{i=1}^{N} y_{i} 
\end{align}\]</div>
<p>Lastly, we denote the total number of predictors <strong><span class="math notranslate nohighlight">\(P\)</span></strong> and  <strong><span class="math notranslate nohighlight">\(j\)</span></strong> as the index of our predictors. So, for example, if we wanted to sum over our predictors for a given sample <strong><span class="math notranslate nohighlight">\(i\)</span></strong>, we’d write:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8e889331-c7f4-44a0-ab15-0fe952e388a8">
<span class="eqno">(4)<a class="headerlink" href="#equation-8e889331-c7f4-44a0-ab15-0fe952e388a8" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathrm{sum}(\mathbf{X}_{i}) = \sum_{j=1}^{P} \mathbf{X}_{ij} 
\end{align}\]</div>
<p>To practice with this notation, let’s do a ToDo!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First some imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class='alert alert-warning'>
    <b>ToDo</b> (1 point): From the variable <tt>arr</tt> below (an array of shape $100 \times 25$), calculate the mean over all samples ($N$) for the predictor at index $j = 4$ (i.e., the fourth predictor). Store the result in a variable named <tt>mean_predictor_4</tt>.
<p>Remember: Python has 0-based indexing!</p>
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Implement the ToDo below. &quot;&quot;&quot;</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">25</span><span class="p">))</span>
<span class="c1"># Implement your ToDo here</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo. &#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_mean_predictor_4</span>
<span class="n">test_mean_predictor_4</span><span class="p">(</span><span class="n">mean_predictor_4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s look at an example. Throughout the example below, we will gradually explain the components of linear regression. For the example, we will use randomly generated data to create a dependent variable with 30 observations (“samples”; <span class="math notranslate nohighlight">\(N = 30\)</span>) and a single independent variable (<span class="math notranslate nohighlight">\(P = 1\)</span>) with, of course, also 30 observations. So both the independent and dependent variable are of shape <span class="math notranslate nohighlight">\(30 \times 1\)</span>. Alright, let’s get started!</p>
<p>For our example let’s create some randomly generated data. As discussed, we’ll create two variables (of shape <span class="math notranslate nohighlight">\(30\times 1\)</span>), which have a prespecified correlation of 0.8 (normally, you don’t know this before doing the analysis of course, but we specify it here for the sake of the example).</p>
<p>We’ll denote our independent variable <code class="docutils literal notranslate"><span class="pre">X</span></code> and our dependent variable <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">prespecified_covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">.8</span><span class="p">],</span>
                                    <span class="p">[</span><span class="mf">.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="n">cov</span><span class="o">=</span><span class="n">prespecified_covariance</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot; By default, when you slice out a single column (or row), numpy returns</span>
<span class="sd">an array of shape (some_number,) instead of (some_number, 1). However, for our</span>
<span class="sd">examples, we often actually want shape (some_number, 1) so essentially we want to </span>
<span class="sd">&quot;add&quot; an extra axis. This is done by the np.newaxis command. Mess around with</span>
<span class="sd">it yourself to see how it works! &quot;&quot;&quot;</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="c1"># Here, we slice the first column (0) and immediately add a new axis!</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="c1"># same here</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The shape of X is: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The shape of y is: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The shape of X is: (30, 1)
The shape of y is: (30, 1)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="modeling-the-intercept-offset">
<h3>Modeling the intercept (offset)<a class="headerlink" href="#modeling-the-intercept-offset" title="Permalink to this headline">¶</a></h3>
<p>As you probably were told in your previous statistics classes, you should always “model the intercept” when running any (regression) model. Technically, the intercept models some of the signal using a constant term. The parameter corresponding to the intercept (as calculated by the linear regression model), then, refers to <em>the average value of your <span class="math notranslate nohighlight">\(y\)</span> variable when all predictors in <span class="math notranslate nohighlight">\(X\)</span> are 0</em>. So, conceptually, the intercept models the mean when of the dependent variable when controlling for our (other) predictors.</p>
<p>To “model the intercept”, you should add an extra “constant predictor” to your design matrix (<code class="docutils literal notranslate"><span class="pre">X</span></code>). This “constant predictor” means simply an array of shape <span class="math notranslate nohighlight">\(N \times 1\)</span> with a constant value, usually all ones. (You’ll figure out <em>why</em> you should do this later in the tutorial.)</p>
<p>Remember from week 1 how to create an array with ones? We can just use <code class="docutils literal notranslate"><span class="pre">np.ones(shape_of_desired_array)</span></code>!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_obs</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_obs</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># creates intercept of shape (N, 1)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we want to add it to our design matrix (<code class="docutils literal notranslate"><span class="pre">X</span></code>). We can do this using the numpy function <code class="docutils literal notranslate"><span class="pre">np.hstack</span></code> (which is short for “horizontal stack”, i.e. “stacking columns horizontally”). This function takes a tuple with arrays which show have the same amount of rows (for our data: both have 30 rows) and returns the a new array in which the arrays from the tuple are stacked (stacked shape should be <span class="math notranslate nohighlight">\(30 \times 2\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tuple_with_arrays</span> <span class="o">=</span> <span class="p">(</span><span class="n">intercept</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">X_with_icept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">tuple_with_arrays</span><span class="p">)</span>

<span class="c1"># Note: you could also simply do ...</span>
<span class="c1"># X_with_icept = np.hstack((np.ones((y.size, 1)), X))</span>
<span class="c1"># ... but arguably this is less &#39;readable&#39; than the implementation above</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of X is now: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">X_with_icept</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of X is now: (30, 2)
</pre></div>
</div>
</div>
</div>
<p>Let’s take a look at the X matrix (“design matrix”) we have now. As you’ll see, we have two columns: the first one is our intercept-predictor, and the second one is our ‘regular’ predictor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_with_icept</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1.         1.65246505]
 [1.         3.84037019]
 [1.         2.90681268]
 [1.         1.58544098]
 [1.         2.77619077]
 [1.         2.26439631]
 [1.         3.42732047]
 [1.         2.27222802]
 [1.         3.44118297]
 [1.         2.77565017]
 [1.         3.68214561]
 [1.         1.98577328]
 [1.         2.36158674]
 [1.         3.41250018]
 [1.         3.08642782]
 [1.         3.78163148]
 [1.         3.91918676]
 [1.         3.64080489]
 [1.         3.98584491]
 [1.         1.19071843]
 [1.         3.46268411]
 [1.         3.17361546]
 [1.         3.15323524]
 [1.         2.15472268]
 [1.         2.69083045]
 [1.         2.82662461]
 [1.         4.1943598 ]
 [1.         3.01266793]
 [1.         1.90963012]
 [1.         2.44916186]]
</pre></div>
</div>
</div>
</div>
<p>Now, let’s take a look at the data. We’ll create a scatter-plot for this (we’ll leave out the intercept):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_with_icept</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_15_0.png" src="../../_images/glm_part1_estimation_15_0.png" />
</div>
</div>
</div>
<div class="section" id="parameters-in-linear-regression">
<h3>Parameters in linear regression<a class="headerlink" href="#parameters-in-linear-regression" title="Permalink to this headline">¶</a></h3>
<p>As you can see, there seems to be some positive linear relationship between <span class="math notranslate nohighlight">\(X\)</span> (just the independent variable without the intercept) and <span class="math notranslate nohighlight">\(y\)</span>. In other words, an increase in <span class="math notranslate nohighlight">\(X\)</span> will lead to an increase in <span class="math notranslate nohighlight">\(y\)</span>. But, at this moment, <em>how much exactly</em> <span class="math notranslate nohighlight">\(y\)</span> changes for a increase in <span class="math notranslate nohighlight">\(X\)</span> is unknown. By doing a linear regression with <span class="math notranslate nohighlight">\(X\)</span> as our predictor of <span class="math notranslate nohighlight">\(y\)</span>, we can quantify this!</p>
<p>The parameter, i.e. the “thing” that quantifies the influence of <span class="math notranslate nohighlight">\(X\)</span> on <span class="math notranslate nohighlight">\(y\)</span>, calculated by this model is often called the <strong>beta-parameter(s)</strong> (but sometimes they’re denoted as theta, or any other greek symbol/letter). The beta-parameter quantifies exactly how much <span class="math notranslate nohighlight">\(y\)</span> changes if you increase <span class="math notranslate nohighlight">\(X\)</span> by 1. Or, in other words, it quantifies how much influence <span class="math notranslate nohighlight">\(X\)</span> has on <span class="math notranslate nohighlight">\(y\)</span>. In a formula (<span class="math notranslate nohighlight">\(\delta\)</span> stands for “change in”)*:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c60222e0-1056-419a-bf22-534d29b04102">
<span class="eqno">(5)<a class="headerlink" href="#equation-c60222e0-1056-419a-bf22-534d29b04102" title="Permalink to this equation">¶</a></span>\[\begin{align}
\beta_{j} = \frac{\delta y}{\delta X_{j}} 
\end{align}\]</div>
<p>As you probably realize, each predictor in <span class="math notranslate nohighlight">\(X\)</span> (i.e., <span class="math notranslate nohighlight">\(X_{j}\)</span>) has a parameter (<span class="math notranslate nohighlight">\(\beta_{j}\)</span>) that quantifies how much influence that predictor has on our target variable (<span class="math notranslate nohighlight">\(y\)</span>). This includes the intercept, our vector of ones (which is in textbooks often denoted by <span class="math notranslate nohighlight">\(\beta_{0}\)</span>; they often don’t write out <span class="math notranslate nohighlight">\(\beta_{0}X_{0}\)</span> because, if a vector of ones is used, <span class="math notranslate nohighlight">\(\beta_{0}\cdot 1\)</span> simplifies to <span class="math notranslate nohighlight">\(\beta_{0}\)</span>).</p>
<p>Thus, linear regression describes a model in which a set of beta-parameters are calculated to characterize the influence of each predictor in <span class="math notranslate nohighlight">\(X\)</span> on <span class="math notranslate nohighlight">\(y\)</span>, that together explain <span class="math notranslate nohighlight">\(y\)</span> as well as possible (but the model is usually not perfect, so there will be some <em>error</em>, or “unexplained variance”, denoted by <span class="math notranslate nohighlight">\(\epsilon\)</span>). As such, we can formulate the linear regression model as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-dfad6899-776d-42e5-8167-9eb68f2f80ab">
<span class="eqno">(6)<a class="headerlink" href="#equation-dfad6899-776d-42e5-8167-9eb68f2f80ab" title="Permalink to this equation">¶</a></span>\[\begin{align}
y = \beta_{0} + X_{1}\beta_{1} + X_{2}\beta_{2} ... + X_{P}\beta_{P} + \epsilon
\end{align}\]</div>
<p>which is often written out as (and is equivalent to the formula above):</p>
<div class="amsmath math notranslate nohighlight" id="equation-0ccadf66-c309-436d-8cf6-e8693c6df6e9">
<span class="eqno">(7)<a class="headerlink" href="#equation-0ccadf66-c309-436d-8cf6-e8693c6df6e9" title="Permalink to this equation">¶</a></span>\[\begin{align}
y = \sum_{j=1}^{P}X_{j}\beta_{j} + \epsilon
\end{align}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\epsilon\)</span> is the variance of <span class="math notranslate nohighlight">\(y\)</span> that cannot be explained by our predictors (i.e, the <em>error</em>).</p>
<p>But how does linear regression estimate the beta-parameters? The method most often used is called <strong>‘ordinary least squares’</strong> (OLS; or just ‘least squares’). This method tries to find a “weight(s)” for the independent variable(s) such that when you multiply the weight(s) with the independent variable(s), it produces an estimate of <span class="math notranslate nohighlight">\(y\)</span> (often denoted as <span class="math notranslate nohighlight">\(\hat{y}\)</span>, or “y-hat”) that is as ‘close’ to the true <span class="math notranslate nohighlight">\(y\)</span> as possible. In other words, least squares tries to ‘choose’ the beta-parameter(s) (<span class="math notranslate nohighlight">\(\hat{\beta}\)</span>) such that the difference between <span class="math notranslate nohighlight">\(X\)</span> multiplied with the beta(s) (i.e. our best guess of <span class="math notranslate nohighlight">\(y\)</span>, denoted as <span class="math notranslate nohighlight">\(\hat{y}\)</span>) and the true <span class="math notranslate nohighlight">\(y\)</span> is minimized*.</p>
<p>Let’s just formalize this formula for the ‘best estimate of <span class="math notranslate nohighlight">\(y\)</span>’ (i.e. <span class="math notranslate nohighlight">\(\hat{y}\)</span>):</p>
<div class="amsmath math notranslate nohighlight" id="equation-71a8076b-fd42-4887-8049-c59b76dc2f75">
<span class="eqno">(8)<a class="headerlink" href="#equation-71a8076b-fd42-4887-8049-c59b76dc2f75" title="Permalink to this equation">¶</a></span>\[\begin{align}
\hat{y}_{i} = \sum_{j=1}^{P}X_{ij}\hat{\beta}_{j} 
\end{align}\]</div>
<p>Before we’re going into the estimation of these beta-parameters, let’s practice with calculating <span class="math notranslate nohighlight">\(\hat{y}\)</span>!</p>
<div class='alert alert-warning'>
    <b>ToDo</b> (1 point): Below, we've defined a design matrix with two predictors (<tt>this_X</tt>) and an array with beta-estimates (<tt>these_betas</tt>; just pretend that these betas were estimated by us beforehand). Now, given this data, can you calculate the predicted $y$-values (i.e., $\hat{y}$)? Store these predicted $y$-values in an array named <tt>this_y_hat</tt>.
<p>Hint: your <tt>this_y_hat</tt> array should be of shape <tt>(100,)</tt>.</p>
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Just generate some random normal data with mean 0, std 1, and shape 100x2 (NxP)</span>
<span class="n">this_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">these_betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo&#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_X_times_betas</span>
<span class="n">test_X_times_betas</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">these_betas</span><span class="p">,</span> <span class="n">this_y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="refresher-matrix-multiplication">
<h3>Refresher: matrix multiplication<a class="headerlink" href="#refresher-matrix-multiplication" title="Permalink to this headline">¶</a></h3>
<p>In the GLM (and statistics in general), you’ll very likely come across concepts and operations from linear (matrix) algebra, such as matrix multiplication and the matrix inverse. In week 1, we briefly discussed how matrix/vector multiplication can be done with Python/numpy: using the <code class="docutils literal notranslate"><span class="pre">.dot</span></code> numpy attribute or with the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> operator. In this course, you’ll encounter several matrix operations as part of the GLM. In fact, we can implement the operation from the previous ToDo (calculating <span class="math notranslate nohighlight">\(\hat{y}\)</span>) using matrix multiplication as well.</p>
<p>See the figure below for a visual explanation of matrix multiplication:
<img alt="dot_prod" src="https://hadrienj.github.io/assets/images/2.2/dot-product.png" />
<em>Image by Hadrien Jean (<a class="reference external" href="https://hadrienj.github.io/assets/images/2.2/dot-product.png">link</a>)</em></p>
<p>If your understanding of matrix algebra, and especially matrix multiplication, is still somewhat rusty, check out <a class="reference external" href="https://www.youtube.com/watch?v=fkZj8QoYjq8">this video</a> by Jeanette Mumford. Now, suppose that the left matrix in the above figure represents our matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and that the right vector represents our estimated parameters <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>. Now, we can calculate <span class="math notranslate nohighlight">\(\hat{y}\)</span> as the matrix operator <span class="math notranslate nohighlight">\(X\hat{\beta}\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9d1f0f17-cefd-4f55-9b93-1b51fb51d812">
<span class="eqno">(9)<a class="headerlink" href="#equation-9d1f0f17-cefd-4f55-9b93-1b51fb51d812" title="Permalink to this equation">¶</a></span>\[\begin{align}
\hat{y} = 
  \begin{bmatrix}
    X_{1,1} &amp; X_{1,2} \\
    X_{2, 1} &amp; X_{2, 2} \\
    \vdots &amp; \vdots \\
    X_{N, 1} &amp; X_{N, 2}
  \end{bmatrix} 
  \begin{bmatrix}
    \hat{\beta}_{1} \\
    \hat{\beta}_{2}
  \end{bmatrix}
\end{align}\]</div>
<p>Let’s check this in code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">this_y_hat_dot</span> <span class="o">=</span> <span class="n">this_X</span> <span class="o">@</span> <span class="n">these_betas</span>  <span class="c1"># this_X.dot(these_betas) is also correct</span>
<span class="nb">print</span><span class="p">(</span><span class="n">this_y_hat_dot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ -0.01338524   1.6703706    2.21587559  10.21759739   6.73838803
  -8.73396662   3.42869211  -4.48842891   0.95290386   3.43650722
  -1.71391492   2.1629619    1.34852454  -2.22062007   3.99755808
   6.55005865  -3.79261597   2.34949198  -1.58847781  -1.00590812
   1.43788029   3.79819378  -4.96641193   0.84168045  -1.23324889
  -5.92010843   1.65225542  -2.18412034  -5.04079461   6.83379015
  -2.44856964  12.2700849   -5.58141723   9.15228886  -3.40630421
  -2.71636339  -4.49867819   2.09344214  -0.83014248   4.14900705
  -1.23808648   6.5956657    3.06043025   2.24424643   3.21012702
  -8.85509716  11.75506195   1.90635652  -1.03938628  -3.27900914
  -5.57655277   0.35407235   1.95194714   5.57227764   5.26441588
  -0.53012277  11.8957242   -0.46059776   5.79536197   2.4409444
  -5.42215061   6.39560171  -2.40432132   3.53601676   1.18398727
   6.910171    12.85646364  -1.58706604   4.27244301   7.18808465
   0.4676259    3.05310707 -11.94182401 -10.92527308   0.27265972
  -5.66052486   3.81234307  -3.5567501    1.81194083   7.42500474
   9.59297064   8.12223579   1.24938396  -3.32538443  -4.11217587
   1.81883774   8.68157606  -4.30168507  -7.90885419  -5.89588956
  -3.42402275  -4.19931869  -2.28244723   1.76317986   5.1354938
  -2.41745528  -8.15283856  -1.30210807  12.01285321   0.32663551]
</pre></div>
</div>
</div>
</div>
<p>In other words, the non-matrix algebra notation …</p>
<div class="amsmath math notranslate nohighlight" id="equation-d86fda95-56a6-4392-9078-8471aeb9a042">
<span class="eqno">(10)<a class="headerlink" href="#equation-d86fda95-56a6-4392-9078-8471aeb9a042" title="Permalink to this equation">¶</a></span>\[\begin{align}
\hat{y}_{i} = \sum_{j=1}^{P}X_{ij}\hat{\beta}_{j} 
\end{align}\]</div>
<p>… is exactly the same as the the following matrix algebra notation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4c3a2fc3-1199-4c64-8729-349b45e75eb0">
<span class="eqno">(11)<a class="headerlink" href="#equation-4c3a2fc3-1199-4c64-8729-349b45e75eb0" title="Permalink to this equation">¶</a></span>\[\begin{align}
\hat{y}_{i} = \mathbf{X}_{i}\mathbf{\hat{\beta}} 
\end{align}\]</div>
<p>You can usually recognize the implementations in formulas using algebra by the use of bold variables (such as <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, which denote matrices) here above.</p>
<p><em>You will calculate <code class="docutils literal notranslate"><span class="pre">y_hat</span></code> quite a lot throughout this lab; please use the matrix algebra method to calculate <code class="docutils literal notranslate"><span class="pre">y_hat</span></code>, because this will likely prevent errors in the future!</em> So, use this …</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">betas</span>
</pre></div>
</div>
<p>instead of …</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="fitting-ols-models">
<h3>Fitting OLS models<a class="headerlink" href="#fitting-ols-models" title="Permalink to this headline">¶</a></h3>
<p>Thus far, we’ve ignored how OLS actually calculates (or “fits”) the unknown parameters <span class="math notranslate nohighlight">\(\beta\)</span>. Roughly speaking, OLS finds parameters that minimize the <em>sum of squared errors</em> (hence the name ‘[ordinary] least squares’!):</p>
<div class="amsmath math notranslate nohighlight" id="equation-16707ff7-50c8-413e-a1f1-60af04c6b3d5">
<span class="eqno">(12)<a class="headerlink" href="#equation-16707ff7-50c8-413e-a1f1-60af04c6b3d5" title="Permalink to this equation">¶</a></span>\[\begin{align}
\min_{\beta} \sum_{i=1}^{N}(y_{i} - X_{i}\hat{\beta})^2 
\end{align}\]</div>
<p>This formula basically formalizes the approach of OLS “find the beta(s) that minimize the difference of my prediction of <span class="math notranslate nohighlight">\(y\)</span> (calculated as <span class="math notranslate nohighlight">\(X \cdot \beta\)</span>) and the true <span class="math notranslate nohighlight">\(y\)</span>”. Now, this still begs the question, <em>how does OLS estimate these parameters?</em> It turns out there is an “analytical solution” to this problem, i.e., there is a way to compute beta estimates that are guaranteed (given some assumptions) to give the parameters that minimize the summed squared error. This solution involves a little matrix algebra and is usually formulated as a series of matrix operations:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6e98a5f9-b35e-4154-b649-ccb29088952e">
<span class="eqno">(13)<a class="headerlink" href="#equation-6e98a5f9-b35e-4154-b649-ccb29088952e" title="Permalink to this equation">¶</a></span>\[\begin{align}
\hat{\beta} = (\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}y
\end{align}\]</div>
<p>(For the mathematically inclined, see <a class="reference external" href="https://economictheoryblog.com/2015/02/19/ols_estimator/">this</a> or <a class="reference external" href="https://socialstatisticsfun.wordpress.com/2012/12/18/deriving-the-ols-estimator-continued/">this</a> blog for the derivation of the OLS solution.) In this formula, <span class="math notranslate nohighlight">\(\mathbf{X}^{T}\)</span> refers to the transpose of the design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>We don’t expect you to understand every aspect of this formula, but you should understand the objective of least squares (minimizing the difference between <span class="math notranslate nohighlight">\(\hat{y}\)</span> and true <span class="math notranslate nohighlight">\(y\)</span>) and what role the beta-parameters play in this process (i.e. a kind of weighting factor of the predictors).</p>
<p>Let’s look at how we’d implement the OLS solution in code. We’ll use the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> operator for matrix multiplication and the <code class="docutils literal notranslate"><span class="pre">inv</span></code> function from the <code class="docutils literal notranslate"><span class="pre">numpy.linalg</span></code> module for the matrix inversion (i.e., the <span class="math notranslate nohighlight">\((X^{T}X)^{-1}\)</span> part).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">inv</span>

<span class="n">est_betas</span> <span class="o">=</span> <span class="n">inv</span><span class="p">(</span><span class="n">X_with_icept</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_with_icept</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_with_icept</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of estimated betas: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">est_betas</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">est_betas</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of estimated betas: (2, 1)
[[4.25897963]
 [0.88186203]]
</pre></div>
</div>
</div>
</div>
<p>“What? Why are there two beta-parameters?”, you might think. This is of course because you also use the intercept as a predictor, which also has an associated beta-value (weighting factor). Here, the first beta refers to the intercept of the model (because it’s the first column in the design-matrix)! The second beta refers to our ‘original’ predictor. Thus, the model found by least squares for our generated data is (i.e. that leads to our best estimate of <span class="math notranslate nohighlight">\(y\)</span>, i.e. <span class="math notranslate nohighlight">\(\hat{y}\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4952efec-434a-46ac-8b37-0dd56212a9b3">
<span class="eqno">(14)<a class="headerlink" href="#equation-4952efec-434a-46ac-8b37-0dd56212a9b3" title="Permalink to this equation">¶</a></span>\[\begin{align}
\hat{y} = X_{1} \cdot 4.259 + X_{2} \cdot 0.882 
\end{align}\]</div>
<p>And since our intercept (here <span class="math notranslate nohighlight">\(X_{1}\)</span>) is a vector of ones, the formula simplifies to:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7596c7a3-b391-4145-a7c8-2f47fd0759d0">
<span class="eqno">(15)<a class="headerlink" href="#equation-7596c7a3-b391-4145-a7c8-2f47fd0759d0" title="Permalink to this equation">¶</a></span>\[\begin{align}
\hat{y} = 4.259 + X_{2} \cdot 0.882
\end{align}\]</div>
<p>Now, let’s calculate our predicted value of <span class="math notranslate nohighlight">\(y\)</span> (<span class="math notranslate nohighlight">\(\hat{y}\)</span>) by implementing the above formula by multiplying our betas with the corresponding predictors (intercept and original predictor). Here, because we have two predictors, we simply add the two “<code class="docutils literal notranslate"><span class="pre">predictor</span> <span class="pre">*</span> <span class="pre">beta</span></code>” terms to get the final <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">X_with_icept</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">X_with_icept</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The predicted y-values are: </span><span class="se">\n\n</span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The predicted y-values are: 

array([5.7162258 , 7.64565627, 6.82238735, 5.65711982, 6.70719685,
       6.25586475, 7.2814034 , 6.26277124, 7.29362822, 6.70672011,
       7.50612402, 6.01015768, 6.3415733 , 7.26833396, 6.98078313,
       7.59385684, 7.71516161, 7.46966721, 7.77394491, 5.30902899,
       7.31258926, 7.05767059, 7.03969805, 6.15914774, 6.63192082,
       6.75167254, 7.95782627, 6.91573708, 5.94300991, 6.41880247])
</pre></div>
</div>
</div>
</div>
<p>Now, let’s plot the predicted <span class="math notranslate nohighlight">\(y\)</span> values (<span class="math notranslate nohighlight">\(\hat{y}\)</span>) against the true <span class="math notranslate nohighlight">\(y\)</span> values (<span class="math notranslate nohighlight">\(y\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">x_lim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X_with_icept</span> <span class="o">@</span> <span class="n">est_betas</span> <span class="c1"># using the matrix algebra approach!</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;True y&#39;</span><span class="p">,</span> <span class="s1">&#39;Predicted y&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_30_0.png" src="../../_images/glm_part1_estimation_30_0.png" />
</div>
</div>
<p>Actually, let’s just plot the predicted y-values as a line (effectively interpolating between adjacent predictions) - this gives us the linear regression plot as you’ve probably seen many times in your statistics classes!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">y_min_pred</span> <span class="o">=</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_max_pred</span> <span class="o">=</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_lim</span><span class="p">,</span> <span class="p">[</span><span class="n">y_min_pred</span><span class="p">,</span> <span class="n">y_max_pred</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Predicted y&#39;</span><span class="p">,</span> <span class="s1">&#39;True y&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linear regression of X onto y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_32_0.png" src="../../_images/glm_part1_estimation_32_0.png" />
</div>
</div>
</div>
<div class="section" id="residuals-and-model-fit">
<h3>Residuals and model fit<a class="headerlink" href="#residuals-and-model-fit" title="Permalink to this headline">¶</a></h3>
<p>Alright, so now we have established the beta-values that lead to the best prediction of <span class="math notranslate nohighlight">\(y\)</span> - in other words, the best fit of our model. But how do we quantify the fit of our model? One way is to look at the difference between <span class="math notranslate nohighlight">\(\hat{y}\)</span> and y, which is often referred to as the model’s <strong>residuals</strong>. This difference between <span class="math notranslate nohighlight">\(\hat{y}\)</span> and <span class="math notranslate nohighlight">\(y\)</span> - the residuals - is the exact same thing as the <span class="math notranslate nohighlight">\(\epsilon\)</span> in the linear regression model, i.e. the <strong>error</strong> of the model. Thus, for a particular dependent variable <span class="math notranslate nohighlight">\(y\)</span>, the residuals (<span class="math notranslate nohighlight">\(\epsilon\)</span>) of a particular fitted model with parameters <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> are computed as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c7ee11b9-afc4-480e-8751-7a6f2fcd6ffe">
<span class="eqno">(16)<a class="headerlink" href="#equation-c7ee11b9-afc4-480e-8751-7a6f2fcd6ffe" title="Permalink to this equation">¶</a></span>\[\begin{align}
\epsilon = y - \mathbf{X}\hat{\beta} 
\end{align}\]</div>
<p>To visualize the residuals (plotted as red dashed lines):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">y_min_pred</span> <span class="o">=</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_max_pred</span> <span class="o">=</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">est_betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_lim</span><span class="p">,</span> <span class="p">[</span><span class="n">y_min_pred</span><span class="p">,</span> <span class="n">y_max_pred</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linear regression of X onto y&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>
<span class="n">custom_lines</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">custom_lines</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;True y&#39;</span><span class="p">,</span> <span class="s1">&#39;Predicted y&#39;</span><span class="p">,</span> <span class="s1">&#39;Residuals&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_34_0.png" src="../../_images/glm_part1_estimation_34_0.png" />
</div>
</div>
<p>In fact, the model fit is often summarized as the <strong>mean of the squared residuals</strong> (also called the ‘mean squared error’ or MSE), which is thus simply the (length of the) red lines squared and averaged. In other words, the MSE refers to the average squared difference between our predicted <span class="math notranslate nohighlight">\(y\)</span> and the true <span class="math notranslate nohighlight">\(y\)</span>*:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0cb1fa27-a0c2-4cb9-ad00-9977ffdc5b28">
<span class="eqno">(17)<a class="headerlink" href="#equation-0cb1fa27-a0c2-4cb9-ad00-9977ffdc5b28" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathrm{MSE} = \frac{1}{N}\sum_{i=1}^{N} (y_{i} - \hat{y}_{i})^2
\end{align}\]</div>
<p>* The “<span class="math notranslate nohighlight">\(\frac{1}{N}\sum_{i=1}^{N}\)</span>” is just a different (but equally correct) way of writing “the average of all residuals from sample 1 to sample N”.</p>
<div class='alert alert-warning'>
    <b>ToDo</b> (1 point): Calculate the MSE for our previous model predictions (<tt>y_hat</tt>) based on our linear regression model predicting <tt>y</tt> from <tt>X_with_intercept</tt>. <em>Do not use a for-loop for this.</em> You know how to do this without a loop, using vectorized numpy array operations. Store the result in a variable named <tt>mse</tt>.
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implement your ToDo here</span>
<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo. &#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_mse_calculation</span>
<span class="n">test_mse_calculation</span><span class="p">(</span><span class="n">mse</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Another metric for model fit in linear regression is “R-squared” (<span class="math notranslate nohighlight">\(R²\)</span>). R-squared is calculated as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1397542a-27de-48e2-b2a9-6518de86f548">
<span class="eqno">(18)<a class="headerlink" href="#equation-1397542a-27de-48e2-b2a9-6518de86f548" title="Permalink to this equation">¶</a></span>\[\begin{align}
R^2 = 1 - \frac{\sum_{i=1}^{N}(y_{i} - \hat{y}_{i})^2}{\sum_{i=1}^{N}(y_{i} - \bar{y})^2}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{y}\)</span> represents the mean of <span class="math notranslate nohighlight">\(y\)</span>. As you can see, the formula for R-squared consists of two parts: the numerator (<span class="math notranslate nohighlight">\(\sum_{i=1}^{N}(y_{i} - \hat{y}_{i})^2\)</span>) and the denominator (<span class="math notranslate nohighlight">\(\sum_{i=1}^{N}(y_{i} - \bar{y}_{i})^2\)</span>). The denominator represents the <em>total</em> amount of squared error of the actual values (<span class="math notranslate nohighlight">\(y\)</span>) relative to the mean (<span class="math notranslate nohighlight">\(\bar{y}\)</span>). The numerator represents the <em>reduced</em> squared errors when incorporating knowledge from our (weighted) independent variables (<span class="math notranslate nohighlight">\(X_{i}\hat{\beta}\)</span>). So, in a way you can interpret R-squared as <em>how much better my model is including <code class="docutils literal notranslate"><span class="pre">X</span></code> versus a model that only uses the mean</em>. Another conventional interpretation of R-squared is the proportion of variance our predictors (<span class="math notranslate nohighlight">\(X\)</span>) together can explain of our target (<span class="math notranslate nohighlight">\(y\)</span>).</p>
<p>As expected, the code is quite straightforward:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># remember, y_hat equals X * beta</span>
<span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">r_squared</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The R² value is: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">r_squared</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The R² value is: 0.549
</pre></div>
</div>
</div>
</div>
<div class='alert alert-warning'>
    <b>ToDo</b> (1 point): Below, we've defined a design matrix (<tt>X_test</tt>, including an intercept) and a dependent variable (<tt>y_test</tt>). Run a linear regression model and calculate R-squared. Store the R-squared value (which should be a single number, a float) in a variable named <tt>r_squared_test</tt>.
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;data_todo_rsquared.npz&#39;</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">data_tmp</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">],</span> <span class="n">data_tmp</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo &#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_rsquared_todo</span>
<span class="n">test_rsquared_todo</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">r_squared_test</span><span class="p">)</span>    
</pre></div>
</div>
</div>
</div>
<div class='alert alert-info'>
<b>ToThink</b> (1 point): As discussed earlier, it's important to model the intercept in regression models. This is because it often greatly <em>improves model fit</em>! In this ToThink, you have to explain <em>why</em> modelling the intercept (usually) improves model fit. 
<p>To give you some clues, we re-did the linear regression computation from above, but now without the intercept in the design matrix. We plotted the data (<tt>X_no_icept</tt>, <tt>y</tt>) and the model fit to get some intuition about the use of an intercept in models.</p>
<p>In the text-cell below the plot, explain (concisely!) why modelling the intercept (usually) improves model fit (this is manually graded, so no test-cell).</p>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_no_icept</span> <span class="o">=</span> <span class="n">X_with_icept</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">beta_no_icept</span> <span class="o">=</span> <span class="n">inv</span><span class="p">(</span><span class="n">X_no_icept</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_no_icept</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_no_icept</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">y_hat_no_icept</span> <span class="o">=</span> <span class="n">beta_no_icept</span> <span class="o">*</span> <span class="n">X_no_icept</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">y_min_pred</span> <span class="o">=</span> <span class="n">beta_no_icept</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_max_pred</span> <span class="o">=</span> <span class="n">beta_no_icept</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_lim</span><span class="p">,</span> <span class="p">[</span><span class="n">y_min_pred</span><span class="p">,</span> <span class="n">y_max_pred</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linear regression of X (without intercept!) onto y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="p">(</span><span class="n">y_hat_no_icept</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">custom_lines</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;True y&#39;</span><span class="p">,</span> <span class="s1">&#39;Predicted y&#39;</span><span class="p">,</span> <span class="s1">&#39;Residuals&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_45_0.png" src="../../_images/glm_part1_estimation_45_0.png" />
</div>
</div>
<p>YOUR ANSWER HERE</p>
</div>
<div class="section" id="summary-linear-regression">
<h3>Summary: linear regression<a class="headerlink" href="#summary-linear-regression" title="Permalink to this headline">¶</a></h3>
<p>Alright, hopefully this short recap on linear regression has refreshed your knowledge and understanding of important concepts such as predictors/design matrix (<span class="math notranslate nohighlight">\(X\)</span>), target (<span class="math notranslate nohighlight">\(y\)</span>), least squares, beta-parameters, intercept, <span class="math notranslate nohighlight">\(\hat{y}\)</span>, residuals, MSE, and <span class="math notranslate nohighlight">\(R^2\)</span>.</p>
<p>In sum, for a linear regression analysis you need some predictors (<span class="math notranslate nohighlight">\(X\)</span>) to model some target (<span class="math notranslate nohighlight">\(y\)</span>). You perform ordinary least squares to find the beta-parameters that minimize the sum of squared residuals. To assess model fit, you can look at the mean squared error (average mis-prediction) or <span class="math notranslate nohighlight">\(R^2\)</span> (total explained variance).</p>
<p>If you understand the above sentence, you’re good to go! Before we go on to the real interesting stuff (modelling fMRI data with linear regression), let’s test how well you understand linear regression so far.</p>
<div class='alert alert-warning'>
<b>ToDo</b> (2 points):
<p>Now, you’re going to implement your own linear regression on a new set of variables, but with a twist: you’re going to use 5 predictors this time - we’ve generated the data for you already. You’ll notice that the code isn’t much different from when you’d implement linear regression for just a single predictor (+ intercept). In the end, you should have calculated MSE and <span class="math notranslate nohighlight">\(R^2\)</span>, which should be stored in variables named <tt>mse_todo</tt> and <tt>r2_todo</tt> respectively.</p>
<p><em>Note, though, that it <strong>isn’t</strong> possible to plot the data (either X, y, or y_hat) because we have more than one predictor now; X is 5-dimensional (6-dimensional if you include the intercept) - and it’s impossible to plot data in 5 dimensions!</em></p>
<p>To give you some handles on how to approach the problem, you can follow these steps:</p>
<ol class="simple">
<li><p>Check the shape of your data: is the shape of X <tt>(N, P)</tt>? is the shape of y <tt>(N, 1)</tt>?</p></li>
<li><p>Add an intercept to the model, use: <tt>np.hstack</tt>;</p></li>
<li><p>Calculate the beta-parameters using the formula you learned about earlier;</p></li>
<li><p>Evaluate the model fit by calculating the MSE and R-squared;</p></li>
</ol>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here, we load the data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;ToDo.npz&#39;</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Check the shape of X and y</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2. Add the intercept (perhaps define N first, so that your code will be more clear?) using np.hstack()</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 3. Calculate the betas using the formula you know</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4. Calculate the MSE (store it in a variable named mse_todo)</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 5. Calculate R-squared  (store it in a variable named r2_todo)</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the ToDo above, MSE part. &#39;&#39;&#39;</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">mse_todo</span><span class="p">,</span> <span class="mf">0.656</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Well done!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the ToDo above, R2-part part. &#39;&#39;&#39;</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">r2_todo</span><span class="p">,</span> <span class="mf">0.3409</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Well done!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class='alert alert-info'>
<b>ToThink</b> (1 point): Let's check whether you understand what a particular beta-parameter means.
<ul class="simple">
<li><p>Some of the betas are negative (i.e., <span class="math notranslate nohighlight">\(\hat{\beta}&lt; 0\)</span>); what does this tell you about the effect of that particular condition/predictor? (.5 point; first text-cell)</p></li>
<li><p>The intercept-parameter (i.e., <span class="math notranslate nohighlight">\(\beta_{0}\)</span>) should be about 6.6. What does this value tell us about the signal?</p></li>
</ul>
<p>Write your answers in the two text-cells below.</p>
</div><p>YOUR ANSWER HERE</p>
<p>YOUR ANSWER HERE</p>
<p>If you’ve finished the ToDo exercise and you’re confident that you understand linear regression, you’re ready to start with the fun part: applying the GLM to fMRI data!</p>
</div>
</div>
<div class="section" id="glm-in-fmri-analyses">
<h2>GLM in fMRI analyses<a class="headerlink" href="#glm-in-fmri-analyses" title="Permalink to this headline">¶</a></h2>
<p>Univariate fMRI analyses basically use the same linear regression model as we’ve explained above to model the activation of voxels (with some minor additions) based on some design-matrix.</p>
<div class="section" id="the-target">
<h3>The target<a class="headerlink" href="#the-target" title="Permalink to this headline">¶</a></h3>
<p>However, compared to “regular” data, one major difference is that <em>the dependent variable (<span class="math notranslate nohighlight">\(y\)</span>) in fMRI analyses is timeseries data</em>, which means that the observations of the dependent variable (activation of voxels) vary across time.</p>
<p>How does such a time-series data look like? Let’s look at a (simulated) time-series from a single voxel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import some stuff if you haven&#39;t done that already</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">inv</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">voxel_signal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;example_voxel_signal.npy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time points (volumes)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (arbitrary units)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">x_lim</span><span class="p">,</span> <span class="n">y_lim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="p">(</span><span class="mi">990</span><span class="p">,</span> <span class="mi">1020</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example of voxel signal&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_63_0.png" src="../../_images/glm_part1_estimation_63_0.png" />
</div>
</div>
<p>So, the voxel timeseries (i.e. activation over time; often called ‘signal’) is our dependent variable (<span class="math notranslate nohighlight">\(y\)</span>). Thus, the different time points (with corresponding activity values) make up our observations/samples!</p>
<div class='alert alert-info'>
<b>ToThink</b> (0 points): Suppose that the TR ("time to repetition", i.e. how long it takes to measure each volume) of our acquisition was 2 seconds, can you deduce the duration of the experiment (in seconds) from the plot above? (Not graded, so you don't have to write anything down!)
</div><p>So, in the plot above, the data points represent the activity (in arbitrary units) of a single voxel across time (measured in volumes). This visualization of the time-series data as discrete measurements is not really intuitive. Usually, we plot the data as continuous line over time (but always remember: fMRI data is a discretely sampled signal – <em>not</em> a continuous one). Let’s plot it as a line:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time points (volumes)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (arbitrary units)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example of voxel signal&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_67_0.png" src="../../_images/glm_part1_estimation_67_0.png" />
</div>
</div>
<p>Alright, this looks better. Now, let’s look at our potential predictors (<span class="math notranslate nohighlight">\(X\)</span>).</p>
</div>
<div class="section" id="the-predictors-or-what-should-we-use-to-model-our-target">
<h3>The predictors, or: what should we use to model our target?<a class="headerlink" href="#the-predictors-or-what-should-we-use-to-model-our-target" title="Permalink to this headline">¶</a></h3>
<div class="section" id="defining-independent-variables">
<h4>Defining independent variables<a class="headerlink" href="#defining-independent-variables" title="Permalink to this headline">¶</a></h4>
<p>So, we know what our target is (the time-series data), but what do we use to model/explain our signal? Well, in most neuroimaging research, your predictors are defined by your experimental design! In other words, your predictors consist of <em>whatever you think influenced your signal</em>.</p>
<p>This probably sounds nonsensical, which is likely caused by the fact that we derive our independent variables (predictors) in most (observational) psychological research differently. This is because in (observational) psychological studies <em>both the independent variables and the dependent variables are <strong>measured</strong></em>. In other words, our predictors are just other variables that you measured in your study.</p>
<p>In neuroimaging research, however, we often derive our predictors not from measures variables but from properties of the particular experiment that we use in the MRI-scanner (or during EEG/MEG acquisiton, for that matter). In other words, we can use any property of the experiment that we believe explains our signal.</p>
<p>Alright, probably still sounds vague. Let’s imagine a (hypothetical) experiment in which we show subjects images of either circles or squares during fMRI acquisition lasting 800 seconds, as depicted in the image below:</p>
<p><img alt="img" src="https://docs.google.com/drawings/d/e/2PACX-1vQwC4chpnzsDEzKhrKH_WHhMX7vJswY4H0pkyIxdlxI_I2GG5e8i6lsiWUO0SUk7NBgdV-vXD5PIleJ/pub?w=950&amp;h=397" /></p>
<p>Note that the interstimulus interval (ISI, i.e., the time between consecutive stimuli) of 50 seconds, here, is quite unrealistic; often, fMRI experiments have a much shorter ISI (e.g., around 3 seconds). Here, we will use an hypothetical experiment with an ISI of 50 seconds because that simplifies things a bit and will make figures easier to interpret.</p>
<p>Anyway, let’s talk about what predictors we could use given our experimental paradigm. One straighforward suggestion about properties that influence our signal is that our signal is influenced by the stimuli we show the participant during the experiment. As such, we could construct a predictor that predicts some response in the signal when a stimulus (here: a square or a circle) is present, and no response when a stimulus is absent.</p>
<p>Fortunately, we kept track of the onsets (in seconds!) of our stimuli during the experiment:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">onsets_squares</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">210</span><span class="p">,</span> <span class="mi">310</span><span class="p">,</span> <span class="mi">410</span><span class="p">,</span> <span class="mi">510</span><span class="p">,</span> <span class="mi">610</span><span class="p">,</span> <span class="mi">710</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">onsets_circles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">260</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">460</span><span class="p">,</span> <span class="mi">560</span><span class="p">,</span> <span class="mi">660</span><span class="p">,</span> <span class="mi">760</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In other words, the first circle-stimulus was presented at 60 seconds after the scan started and the last square-stimulus was presented 710 seconds after the can started.</p>
<p>For now, we’ll ignore the difference between square-stimuli and circle-stimuli by creating a predictor that lumps the onsets of these two types of stimuli together in one array. This predictor thus reflects the hypothesis that the signal is affected by the presence of a stimulus (regardless of whether this was a square or a circle). (Later in the tutorial, we’ll explain how to <em>compare</em> the effects of different conditions.)</p>
<p>We’ll call this predictor simply <code class="docutils literal notranslate"><span class="pre">onsets_all</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">onsets_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">onsets_squares</span><span class="p">,</span> <span class="n">onsets_circles</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onsets_all</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 10 110 210 310 410 510 610 710  60 160 260 360 460 560 660 760]
</pre></div>
</div>
</div>
</div>
<p>Now, we need to do one last thing: convert the <code class="docutils literal notranslate"><span class="pre">onsets_all</span></code> vector into a proper predictor. Right now, the variable contains only the onsets, but a predictor should be an array with the same shape as the target.</p>
<p>Given that our predictor should represent the hypothesis that the signal responds to the presence of a stimulus (and doesn’t respond when a stimulus is absent), we can construct our predictor as a vector of all zeros, except at indices corresponding to the onsets of our stimuli, where the value is 1.</p>
<p>We do this below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictor_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">800</span><span class="p">)</span>  <span class="c1"># because the experiment lasted 800 seconds</span>
<span class="n">predictor_all</span><span class="p">[</span><span class="n">onsets_all</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># set the predictor at the indices to 1</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of predictor: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">predictor_all</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Contents of our predictor array:</span><span class="se">\n</span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">predictor_all</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of predictor: (800,)

Contents of our predictor array:
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0.])
</pre></div>
</div>
</div>
</div>
<p>We can even plot it in a similar way as we did with the voxel signal:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictor_all</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time points (seconds)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (arbitrary units)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example of stimulus predictor&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_76_0.png" src="../../_images/glm_part1_estimation_76_0.png" />
</div>
</div>
</div>
<div class="section" id="resampling">
<h4>Resampling<a class="headerlink" href="#resampling" title="Permalink to this headline">¶</a></h4>
<p>However, if you look back at the plot of the voxel signal, you might notice that there is a problem in our stimulus-predictor - it seems to be on a different timescale than the signal. And that’s true! The signal from the voxel is measured in volumes (in total 400, with a TR of 2 seconds) while the stimulus-onsets are defined in seconds (ranging from 10 to 760)!</p>
<p>This “issue” can be solved by downsampling our predictor to the time resolution of our signal, i.e., one datapoint every two seconds (given that our TR is 2 seconds). In the plot below, we show you with dashed red lines which datapoints would constitute our predictor <em>after</em> downsampling (we only show the first 100 seconds for clarity).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictor_all</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time points (seconds)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (arbitrary units)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;How to downsample the predictor&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_78_0.png" src="../../_images/glm_part1_estimation_78_0.png" />
</div>
</div>
<p>Resampling in Python can be done using the <code class="docutils literal notranslate"><span class="pre">interp1d</span></code> function from the <code class="docutils literal notranslate"><span class="pre">scipy.interpolate</span></code> package. It works by first creating a mapping between the <em>scale</em> of the original array and the <em>values</em> of the original array, and then converting the original array to a <em>different scale</em>. We’ll show you how this would be done with our <code class="docutils literal notranslate"><span class="pre">predictor_all</span></code> array.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">resampler</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">original_scale</span><span class="p">,</span> <span class="n">original_array</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>  <span class="c1"># interp1d returns a (new) function</span>
<span class="n">downsampled_array</span> <span class="o">=</span> <span class="n">resampler</span><span class="p">(</span><span class="n">desired_scale</span><span class="p">)</span> 
</pre></div>
</div>
<p>Note that when creating the mapping by calling <code class="docutils literal notranslate"><span class="pre">interp1d</span></code>, the function returns a <em>new function</em>, which we store in a variable called <code class="docutils literal notranslate"><span class="pre">resampler</span></code>. Now, we can call with new function with our desired scale for our array, which will result the original array downsampled at the desired scale (note that this works exactly the same for <em>up</em>sampling). Also note that we we use specifically “linear” resampling (by using <code class="docutils literal notranslate"><span class="pre">kind='linear'</span></code>). In practice, most neuroimaging packages use non-linear resampling/interpolation when downsampling predictors, but the details of different kinds of resampling are beyond the scope of this course.</p>
<p>Anyway, let’s try this on our predictor by resampling it from the original scale (0-800 seconds) to the scale of our signal (i.e., at 0, 2, 4, 8 … 800 seconds, assuming a TR of 2 seconds). Note the function <code class="docutils literal notranslate"><span class="pre">np.arange</span></code>, which can be used to create evenly spaced arrays (with <code class="docutils literal notranslate"><span class="pre">np.arange(start,</span> <span class="pre">stop,</span> <span class="pre">step)</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.interpolate</span> <span class="kn">import</span> <span class="n">interp1d</span>
<span class="n">original_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># from 0 to 800 seconds</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original scale has </span><span class="si">%i</span><span class="s2"> datapoints (0-800, in seconds)&quot;</span> <span class="o">%</span> <span class="n">original_scale</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="n">resampler</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">original_scale</span><span class="p">,</span> <span class="n">predictor_all</span><span class="p">)</span>

<span class="n">desired_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Desired scale has </span><span class="si">%i</span><span class="s2"> datapoints (0, 2, 4, ... 800, in volumes)&quot;</span> <span class="o">%</span> <span class="n">desired_scale</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="n">predictor_all_ds</span> <span class="o">=</span> <span class="n">resampler</span><span class="p">(</span><span class="n">desired_scale</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downsampled predictor has </span><span class="si">%i</span><span class="s2"> datapoints (in volumes)&quot;</span> <span class="o">%</span> <span class="n">predictor_all_ds</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original scale has 800 datapoints (0-800, in seconds)
Desired scale has 400 datapoints (0, 2, 4, ... 800, in volumes)
Downsampled predictor has 400 datapoints (in volumes)
</pre></div>
</div>
</div>
</div>
<p>That seemed to have worked! Let’s print it to be sure:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">predictor_all_ds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre></div>
</div>
</div>
</div>
<p>Awesome! Now, we have a predictor (<span class="math notranslate nohighlight">\(X\)</span>) and a target (<span class="math notranslate nohighlight">\(y\)</span>) of the same shape, so we can apply linear regression! But before we do this, let’s plot the predictor and the signal in the same plot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictor_all_ds</span> <span class="o">+</span> <span class="n">voxel_signal</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (in volumes)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Voxel timeseries&#39;</span><span class="p">,</span> <span class="s1">&#39;Predictor&#39;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Signal and the associated stimulus predictor&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_84_0.png" src="../../_images/glm_part1_estimation_84_0.png" />
</div>
</div>
<p>Often, the design matrix is actually specified with higher precision (e.g., on the scale of milliseconds) than we did in the previous example (i.e., seconds) to accomodate onsets that are not “locked” to full seconds (e.g., <span class="math notranslate nohighlight">\(t=10\)</span>, <span class="math notranslate nohighlight">\(t=60\)</span>, but never <span class="math notranslate nohighlight">\(t=10.296\)</span>). We’ll come back to this issue later in the tutorial.</p>
<p>But first, let’s practice!</p>
<div class='alert alert-warning'><b>ToDo</b> (2 points): Below, we've defined an array of onsets (<tt>onsets2</tt>) belonging to a hypothetical experiment with a duration of 60 seconds and a TR of 3. Create a predictor at the timescale of seconds (should have a length of 60) and name this <tt>pred2</tt>. Then, downsample the predictor to the scale of the experiment (i.e., resample the predictor to the timepoints $t=0, t=3, t=6, t=9, ... , t=57$). Store this downsampled predictor in a variable named <tt>pred2_ds</tt>.
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implement your ToDo Here</span>
<span class="n">onsets2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">12</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">42</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># First create pred2</span>


<span class="c1"># Then downsample it to create pred2_ds</span>


<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Tests the previous ToDo (part 1)&quot;&quot;&quot;</span>
<span class="k">try</span><span class="p">:</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">pred2</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">60</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;pred2 is not the right size!&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span><span class="n">pred2</span><span class="p">[</span><span class="n">onsets2</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictor did not contains 1s at the indices corresponding to the onsets!&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Well done! (test 1 / 2)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; Tests the previous ToDo (part 2)&quot;&quot;&quot;</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span><span class="n">pred2_ds</span><span class="p">,</span> <span class="n">pred2</span><span class="p">[::</span><span class="mi">3</span><span class="p">])</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Something went wrong with downsampling ...&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Well done! (test 2 / 2)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>At this moment, we have everything that we need to run linear regression: a predictor (independent variable) and a signal (target/dependent variable) at the same scale and the same number of data points! This regression analysis allows us to answer the question whether the activity of the signal is significantly different when a stimulus is presented (i.e., at times when the predictor contains ones) than when no stimulus is presented (i.e., at times when the predictor contains zeros).</p>
<p>Or, phrased differently (but mathematically equivalent): what is the effect of a unit increase in the predictor (<span class="math notranslate nohighlight">\(X = 0 = \mathrm{no\ stimulus} \rightarrow X = 1 = \mathrm{stimulus}\)</span>) on the target (the signal)? We will answer this question in the next section!</p>
</div>
</div>
<div class="section" id="regression-on-fmri-data-interpretation-parameters">
<h3>Regression on fMRI data &amp; interpretation parameters<a class="headerlink" href="#regression-on-fmri-data-interpretation-parameters" title="Permalink to this headline">¶</a></h3>
<p>As said before, applying regression analysis on fMRI data is done largely the same as on regular non-timeseries data. In the next ToDo, you’re going to do exactly that.</p>
<div class='alert alert-warning'>
    <b>ToDo</b> (2 points): We'll start by adding an intercept to <tt>predictor_all_ds</tt> for you, creating a new variable called <tt>X_simple</tt>. Now, run linear regression on the <tt>voxel_signal</tt> data and save the beta-parameters in a new variable named <tt>betas_simple</tt>. Finally, calculate MSE and $R^ 2$ for this model and store these values in new variables named <tt>mse_simple</tt> and <tt>r2_simple</tt>.
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">predictor_all_ds</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># do not remove this! This adds a singleton dimension, such that you can call np.hstack on it</span>
    <span class="n">predictor_all_ds</span> <span class="o">=</span> <span class="n">predictor_all_ds</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="n">icept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">predictor_all_ds</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_simple</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">icept</span><span class="p">,</span> <span class="n">predictor_all_ds</span><span class="p">))</span>

<span class="c1"># Start your ToDo here</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo. &#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_regression_signal_simple</span>

<span class="k">if</span> <span class="s1">&#39;X_simple&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not find a variable named &#39;X_simple&#39;; did you name it correctly?&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="s1">&#39;betas_simple&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not find a variable named &#39;betas_simple&#39;; did you name it correctly?&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="s1">&#39;mse_simple&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not find a variable named &#39;mse_simple&#39;; did you name it correctly?&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="s1">&#39;r2_simple&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not find a variable named &#39;r2_simple&#39;; did you name it correctly?&quot;</span><span class="p">)</span>

<span class="n">test_regression_signal_simple</span><span class="p">(</span><span class="n">mse_simple</span><span class="p">,</span> <span class="n">r2_simple</span><span class="p">,</span> <span class="n">predictor_all_ds</span><span class="p">,</span> <span class="n">voxel_signal</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If you’ve done the ToDo correctly, you should have found the the following beta-parameters: 1000.657 for the intercept and 1.023 for our stimulus-predictor. This means that our linear regression model for that voxel is as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ceebcdbb-2086-4ef8-998c-7c25aeaeb295">
<span class="eqno">(19)<a class="headerlink" href="#equation-ceebcdbb-2086-4ef8-998c-7c25aeaeb295" title="Permalink to this equation">¶</a></span>\[\begin{align}
y_{voxel} = \beta_{intercept} + X_{stim}\beta_{stim} + \epsilon = 1000.647 + X_{stim}1.023 + \epsilon
\end{align}\]</div>
<p>This simply means that for a unit increase in <span class="math notranslate nohighlight">\(X\)</span> (i.e., <span class="math notranslate nohighlight">\(X = 0 \rightarrow X = 1\)</span>), <span class="math notranslate nohighlight">\(y\)</span> increases with 1.023. In other words, on average the signal is 1.023 higher when a stimulus is present compared to when a stimulus is absent!</p>
<p>To aid interpretation, let’s plot the signal (<span class="math notranslate nohighlight">\(y\)</span>) and the predicted signal (<span class="math notranslate nohighlight">\(\hat{y} = \beta X\)</span>) in the same plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">niedu.utils.nii</span> <span class="kn">import</span> <span class="n">plot_signal_and_predicted_signal</span> 
<span class="c1"># You may ignore the red text (a warning caused by some outdated code in some</span>
<span class="c1"># of the underlying packages we use)</span>
<span class="n">plot_signal_and_predicted_signal</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">,</span> <span class="n">predictor_all_ds</span><span class="p">,</span> <span class="n">x_lim</span><span class="p">,</span> <span class="n">y_lim</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_96_1.png" src="../../_images/glm_part1_estimation_96_1.png" />
</div>
</div>
<p>The orange line represents the predicted signal, which is based on the original predictor (<span class="math notranslate nohighlight">\(X\)</span>) multiplied (or “scaled”) by the associated beta-parameters (<span class="math notranslate nohighlight">\(\beta\)</span>). Graphically, you can interpret the beta-parameter of the stimulus-predictor (<span class="math notranslate nohighlight">\(\beta_{stim}\)</span>) as the maximum height of the peaks in the orange line* and the beta-parameter of the intercept (<span class="math notranslate nohighlight">\(\beta_{intercept}\)</span>) as the difference from the flat portion of the orange line and 0 (i.e. the “offset” of the signal).</p>
<hr class="docutils" />
<p>* This holds true only when the maximum value of the original predictor is 1 (which is true in our case)</p>
<p>Let’s zoom in on a portion of the data to show this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">niedu.utils.nii</span> <span class="kn">import</span> <span class="n">plot_signal_and_predicted_signal_zoom</span>
<span class="n">plot_signal_and_predicted_signal_zoom</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">,</span> <span class="n">predictor_all_ds</span><span class="p">,</span> <span class="n">x_lim</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="n">y_lim</span><span class="o">=</span><span class="p">(</span><span class="mi">995</span><span class="p">,</span> <span class="mi">1010</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_99_0.png" src="../../_images/glm_part1_estimation_99_0.png" />
</div>
</div>
<p>Anyway, there seems to be an effect on voxel activity when we show a stimulus — an increase of 1.023 in the signal on average (about 0.1% percent signal change) — but you’ve also seen that the model fit is quite bad (<span class="math notranslate nohighlight">\(R^2 = 0.004\)</span>, about 0.4% explained variance) …</p>
<p>What is happening here? Is our voxel just super noisy? Or is something wrong with our model? We’ll talk about this in the next section!</p>
</div>
<div class="section" id="using-the-bold-response-in-glm-models">
<h3>Using the BOLD response in GLM models<a class="headerlink" href="#using-the-bold-response-in-glm-models" title="Permalink to this headline">¶</a></h3>
<p>Let’s go back to our original idea behind the predictor we created. We assumed that in order to model activity in response to our stimuli, our predictor should capture an increase/decrease in activity <em>at the moment of stimulus onset</em>. But this is, given our knowledge of the BOLD-response, kind of unrealistic to assume: it is impossible to measure instantaneous changes in neural activity in response to stimuli or tasks with fMRI, <em>because the BOLD-response is quite slow and usually peaks around 5-7 seconds <strong>after</strong> the ‘true’ neuronal activity (i.e. at cellular level)</em>.</p>
<p>In the above model, we have not incorporated either the lag (i.e. ~6 seconds) or the shape of the BOLD-response: we simply modelled activity as an instantaneous response to a stimulus event.</p>
<p>You can imagine that if you incorporate this knowledge about the BOLD-response into our model, the fit will likely get better! In this section, we’ll investigate different ways to incorporate knowledge of the BOLD-response in our predictors.</p>
<div class="section" id="the-canonical-hrf">
<h4>The canonical HRF<a class="headerlink" href="#the-canonical-hrf" title="Permalink to this headline">¶</a></h4>
<p>The easiest and most often-used approach to incorporating knowledge about the BOLD-response in univariate analyses of fMRI data is to assume that each voxel responds to a stimulus in a fixed way. In other words, that voxels always respond (activate/deactivate) to a stimulus in the same manner. This is known as using a “canonical haemodynamic response function (HRF)”. Basically, an HRF is a formalization of how we think the a voxel is going to respond to a stimulus. A <em>canonical</em> HRF is the implementation of an HRF in which you use the same HRF for each voxel, participant, and condition. There are other implementations of HRFs (apart from the canonical), in which you can adjust the exact shape of the HRF based on the data you have; examples of these HRFs are <em>temporal basis sets</em> and <em>finite impulse reponse models</em> (FIR), which we’ll discuss later.</p>
<p>There are different types of (canonical) HRFs; each models the assumed shape of the BOLD-response slightly differently. For this course, we’ll use the most often used canonical HRF: the double-gamma HRF, which is a combination of two different gamma functions (one modelling the overshoot and one modelling the post-stimulus undershoot).</p>
<p>We’ll use the “double gamma” HRF implementation of the <a class="reference external" href="https://nilearn.github.io">nilearn</a> Python package. They provide different “versions” of the HRF (which differ slightly in their shape); we’ll use the “Glover” version, which is based on a double-<a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>: <code class="docutils literal notranslate"><span class="pre">glover_hrf</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.glm.first_level.hemodynamic_models</span> <span class="kn">import</span> <span class="n">glover_hrf</span>
</pre></div>
</div>
</div>
</div>
<p>This function takes a couple of arguments (the most important being: TR, oversampling factor, and length of the HRF in seconds), and returns a double gamma HRF. It is important to make sure that your HRF is on the same timescale as our design (predictors). In the previous section, we made sure we defined our predict at the time scale of seconds first before downsampling it to a time resolution of 2 seconds (i.e., the TR).</p>
<p>Thus, given a TR of 2, we can use a “oversampling factor” of 2 to get the HRF on the timescale of seconds:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">TR</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">osf</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">length_hrf</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># sec</span>

<span class="n">canonical_hrf</span> <span class="o">=</span> <span class="n">glover_hrf</span><span class="p">(</span><span class="n">tr</span><span class="o">=</span><span class="n">TR</span><span class="p">,</span> <span class="n">oversampling</span><span class="o">=</span><span class="n">osf</span><span class="p">,</span> <span class="n">time_length</span><span class="o">=</span><span class="n">length_hrf</span><span class="p">,</span> <span class="n">onset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">canonical_hrf</span> <span class="o">/=</span> <span class="n">canonical_hrf</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of canonical hrf variable: </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">canonical_hrf</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Size of canonical hrf variable: 32
</pre></div>
</div>
</div>
</div>
<p>As you can see, the length of the <code class="docutils literal notranslate"><span class="pre">canonical_hrf</span></code> variable is 32 (seconds). Now, let’s plot it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (in seconds!) after stimulus onset&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Double gamma HRF&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_107_0.png" src="../../_images/glm_part1_estimation_107_0.png" />
</div>
</div>
</div>
<div class="section" id="convolution">
<h4>Convolution<a class="headerlink" href="#convolution" title="Permalink to this headline">¶</a></h4>
<p>The figure of the HRF shows how we’d expect that an idealized (noiseless) response to a single event would look like. But how should we incorporate this HRF into our model? Traditionally, this is done using a mathematical operation called <strong>convolution</strong>. Basically, it “slides” the HRF across our 0-1 coded stimulus-vector from left to right and elementwise multiplies the HRF with the stimulus-vector. This is often denoted as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c2412687-52e9-4a09-be47-c09d8d53d9a3">
<span class="eqno">(20)<a class="headerlink" href="#equation-c2412687-52e9-4a09-be47-c09d8d53d9a3" title="Permalink to this equation">¶</a></span>\[\begin{align}
X_{\mathrm{conv}} = \mathrm{HRF} * X_{\mathrm{original}}
\end{align}\]</div>
<p>in which <span class="math notranslate nohighlight">\(*\)</span> is the symbol for convolution, <span class="math notranslate nohighlight">\(X_{\mathrm{original}}\)</span> is the original stimulus-vector, and <span class="math notranslate nohighlight">\(X_{\mathrm{conv}}\)</span> the result of the convolution.</p>
<p>Let’s plot an example to make it clearer. Suppose we have an onset-vector of length 100 (i.e., the experiment was 100 seconds long) with three stimulus presentations: at <span class="math notranslate nohighlight">\(t = 10\)</span>, <span class="math notranslate nohighlight">\(t = 40\)</span>, and <span class="math notranslate nohighlight">\(t = 70\)</span>. The stimulus-vector (upper plot), double-gamma HRF (right plot), and the result of the convolution of the stimulus-vector and the HRF (lower plot) looks as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_stimulus_onsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">70</span><span class="p">]</span>
<span class="n">random_stim_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">random_stim_vector</span><span class="p">[</span><span class="n">random_stimulus_onsets</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">colspan</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">random_stim_vector</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (seconds)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Stimulus events&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">rowspan</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">canonical_hrf</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;HRF&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (seconds)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">convolved_stim_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">random_stim_vector</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">colspan</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">convolved_stim_vector</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Convolved stimulus-vector&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (seconds)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_109_0.png" src="../../_images/glm_part1_estimation_109_0.png" />
</div>
</div>
<p>The result — the convolved stimulus-vector — is basically the output of a the multiplication of the HRF and the stimulus-events when you would “slide” the HRF across the stimulus vector. As you can see, the convolved stimulus-vector correctly shows the to-be-expected lag and shape of the BOLD-response! Given that this new predictor incorporates this knowledge of the to-be expected response, it will probably model the activity of our voxel way better. Note that the temporal resolution of your convolved regressor is necessary limited by the resolution of your data (i.e. the TR of your fMRI acquisition). That’s why the convolved regressor doesn’t look as “smooth” as the HRF.</p>
<p>As you can see in the code for the plot above, numpy provides us with a function to convolve two arrays:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">array_1</span><span class="p">,</span> <span class="n">array_2</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, we can convolve the HRF with out stimulus-predictor. Importantly, we want to do this convolution operation in the resolution of our onsets (here: seconds), not in the resolution of our signal (TR) (the reason for this is explained clearly in Jeanette Mumford’s <a class="reference external" href="https://www.youtube.com/watch?v=5JNX34gYG7Q">video on the HRF</a>.)
Therefore, we need to perform the convolution on the variable <code class="docutils literal notranslate"><span class="pre">predictor_all</span></code> (<em>not</em> the downsampled variable: <code class="docutils literal notranslate"><span class="pre">predictor_all_ds</span></code>)!</p>
<p>We’ll do this below (we’ll reuse the <code class="docutils literal notranslate"><span class="pre">canonical_hrf</span></code> variable defined earlier):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39;We need to &quot;squeeze&quot; out the extra singleton axis, because that&#39;s</span>
<span class="sd">what the np.convolve function expects, i.e., arrays of shape (N,) and NOT (N, 1)</span>
<span class="sd">To go from (N, 1) --&gt; (N,) we&#39;ll use the squeeze() method&#39;&#39;&#39;</span>
<span class="n">predictor_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">predictor_all</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">canonical_hrf</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The shape of the convolved predictor after convolution: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">predictor_conv</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>

<span class="c1"># After convolution, we also neem to &quot;trim&quot; off some excess values from</span>
<span class="c1"># the convolved signal (the reason for this is not important to understand)</span>
<span class="n">predictor_conv</span> <span class="o">=</span> <span class="n">predictor_conv</span><span class="p">[:</span><span class="n">predictor_all</span><span class="o">.</span><span class="n">size</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After trimming, the shape is: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">predictor_conv</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>

<span class="c1"># And we have to add a new axis again to go from shape (N,) to (N, 1),</span>
<span class="c1"># which is important for stacking the intercept, later</span>
<span class="n">predictor_conv</span> <span class="o">=</span> <span class="n">predictor_conv</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape after adding the new axis: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">predictor_conv</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The shape of the convolved predictor after convolution: (831,)
After trimming, the shape is: (800,)
Shape after adding the new axis: (800, 1)
</pre></div>
</div>
</div>
</div>
<p>It’s a bit of a hassle (squeezing out the singleton axis, trimming, adding the axis back …), but now we have a predictor which includes information about the expected HRF!</p>
<p>Let’s look at the predictor before and after convolution in the same plot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictor_all</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictor_conv</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">800</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Predictor before and after convolution&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (seconds!)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Activity (A.U.)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Before&#39;</span><span class="p">,</span> <span class="s1">&#39;After&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_113_0.png" src="../../_images/glm_part1_estimation_113_0.png" />
</div>
</div>
</div>
<div class="section" id="linear-scaling">
<h4>Linear scaling<a class="headerlink" href="#linear-scaling" title="Permalink to this headline">¶</a></h4>
<p>Great! Our predictor now includes the expected ‘lag’ and shape of the HRF, and we can start analyzing our signal with our new convolved predictor! But before we’ll do this, there is one more concept that we’ll demonstrate. Remember the concept of <strong>linear scaling</strong> of the BOLD-response? This property of the BOLD-response states that it will linearly scale with the input it is given.</p>
<p>Let’s see how that works:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">one_stim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">one_stim</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">one_stim_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">one_stim</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="p">)[:</span><span class="n">N</span><span class="p">]</span>
<span class="n">two_stim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">two_stim</span><span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">two_stim_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">two_stim</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="p">)[:</span><span class="n">N</span><span class="p">]</span>
<span class="n">three_stim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">three_stim</span><span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">three_stim_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">three_stim</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="p">)[:</span><span class="n">N</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ons</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">one_stim</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ons</span><span class="p">,</span> <span class="n">ons</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;One stimulus&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ons</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">two_stim</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ons</span><span class="p">,</span> <span class="n">ons</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Two stimuli&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ons</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">three_stim</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ons</span><span class="p">,</span> <span class="n">ons</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:green&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>    
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Three stimuli&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">colspan</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">one_stim_conv</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">two_stim_conv</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">three_stim_conv</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;One stim&#39;</span><span class="p">,</span> <span class="s1">&#39;Two stim&#39;</span><span class="p">,</span> <span class="s1">&#39;Three stim&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linear scaling of HRF&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (TR)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_115_0.png" src="../../_images/glm_part1_estimation_115_0.png" />
</div>
</div>
<p>Also, in our random stimulus-vector above (and also in the example we showed earlier) we assumed that each image was only showed briefly (i.e. we only modelled the onset) - but what if a stimulus (or task) may take longer, say, 15 seconds? Let’s see what happens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_stimulus_onsets2</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">))</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">55</span><span class="p">))</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">70</span><span class="p">,</span> <span class="mi">85</span><span class="p">))</span>
<span class="n">random_stim_vector2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">random_stim_vector2</span><span class="p">[</span><span class="n">random_stimulus_onsets2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">random_stim_vector2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Stimulus events&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="n">convolved_stim_vector2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">random_stim_vector2</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="p">)[:</span><span class="n">random_stim_vector2</span><span class="o">.</span><span class="n">size</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">convolved_stim_vector2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Convolved stimulus-vector&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (seconds)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_117_0.png" src="../../_images/glm_part1_estimation_117_0.png" />
</div>
</div>
<p>As you can see, convolution takes care to model the shape of the BOLD-response according to how long you specify the stimulus to take!</p>
<div class='alert alert-info'>
<b>ToThink</b> (1 point): Given the properties of the BOLD-response (and assuming linear-time invariance is not violated), would you expect the same or a different BOLD-response in response to 3 consecutive stimuli (of the same condition) of half a second second each (which follow each other immediately, i.e. without interstimulus interval) versus 1 stimulus of 1.5 seconds? Why? (Write your answer in the text-cell below)
</div><p>YOUR ANSWER HERE</p>
<p>Actually, convolution can model <em>any</em> sequence of stimulus events, even stimuli with random onsets - just look at the plot below!</p>
<p>(you can execute this cell below multiple times to see different random regressor shapes!)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_stimulus_onsets3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">random_stim_vector3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">random_stim_vector3</span><span class="p">[</span><span class="n">random_stimulus_onsets3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">event</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">random_stim_vector3</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">event</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Stimulus events&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">convolved_stim_vector3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">random_stim_vector3</span> <span class="o">*</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">)[:</span><span class="n">random_stim_vector3</span><span class="o">.</span><span class="n">size</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">convolved_stim_vector3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Convolved stimulus-vector&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (seconds)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_122_1.png" src="../../_images/glm_part1_estimation_122_1.png" />
</div>
</div>
<div class='alert alert-warning'>
    <b>ToDo</b> (1 point): Let's assume that we did an experiment in which participants were presented with a visual stimulus lasting 20 seconds six times over the course of an experiment of 3 minutes (starting at $t=0$, $t=30$, ... , $t=150$ sec). Create a stimulus predictor (on the timescale of seconds), store this in a variable named <tt>todo_pred</tt>, and then convolve this predictor with the <tt>canonical_hrf</tt> variable. Make sure to trim the convolved predictor, and store the result in a variable named <tt>todo_pred_conv</tt>.
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implement your ToDo here</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo&#39;&#39;&#39;</span>
<span class="k">try</span><span class="p">:</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">todo_pred</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">180</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The predictor is not the right length!&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">todo_pred</span><span class="p">)</span> <span class="o">==</span> <span class="mi">120</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Are you sure your stimulus events last 20 seconds?&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    
<span class="k">try</span><span class="p">:</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">todo_pred_conv</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">180</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Did you trim the convolved predictor?&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">todo_pred_conv</span><span class="p">),</span> <span class="mf">482.6069</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Did you convolve the predictor with the canonical HRF?&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Well done!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>So, in summary, convolving the stimulus-onsets (and their duration) with the HRF gives us (probably) a better predictor of the voxel signal than just the stimulus-onset, because (1) it models the lag of the BOLD-response and (2) models the shape of the BOLD-response (accounting for the linear scaling principle).</p>
</div>
<div class="section" id="resampling-revisited">
<h4>Resampling revisited<a class="headerlink" href="#resampling-revisited" title="Permalink to this headline">¶</a></h4>
<p>Now, we’re <em>almost</em> ready to start analyzing our signal with the convolved predictor! The problem, at this moment, however is that the convolved predictor and the signal are on different scales!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size convolved predictor: </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">predictor_conv</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size voxel signal: </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">voxel_signal</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Size convolved predictor: 800
Size voxel signal: 400
</pre></div>
</div>
</div>
</div>
<p>We can use resampling to find the data points of the convolved HRF predictor that correspond to the onset of the volumes of our voxel signal. Importantly, we have to “squeeze out” the singleton dimension before downsampling the predictor (otherwise it’ll give an error). Then, we can plot the convolved predictor and the signal in the same figure (because now they are defined on the same timescale!):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">original_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">)</span>
<span class="n">resampler</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">original_scale</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">predictor_conv</span><span class="p">))</span>

<span class="n">desired_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">predictor_conv_ds</span> <span class="o">=</span> <span class="n">resampler</span><span class="p">(</span><span class="n">desired_scale</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictor_conv_ds</span> <span class="o">+</span> <span class="n">voxel_signal</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Downsampled/convolved predictor + signal&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (seconds)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Predictor&#39;</span><span class="p">,</span> <span class="s1">&#39;Signal&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_129_0.png" src="../../_images/glm_part1_estimation_129_0.png" />
</div>
</div>
</div>
<div class="section" id="initial-upsampling-of-predictors">
<h4>Initial upsampling of predictors<a class="headerlink" href="#initial-upsampling-of-predictors" title="Permalink to this headline">¶</a></h4>
<p>In the previous examples, the resampling process came down to selecting every other datapoint (at <span class="math notranslate nohighlight">\(t=0\)</span>, <span class="math notranslate nohighlight">\(t=2\)</span>, <span class="math notranslate nohighlight">\(t=4\)</span>, …, <span class="math notranslate nohighlight">\(t=798\)</span>), because our onsets were all “locked” to “round” seconds (e.g., <span class="math notranslate nohighlight">\(t=10\)</span>, <span class="math notranslate nohighlight">\(t=60\)</span>, but never <span class="math notranslate nohighlight">\(t=10.29\)</span> or something). Very often, however, stimuli (or whatever you’re using to construct your predictors) are not locked to “round” seconds. How would you then initially create a predictor? One way is to do so is to create your stimulus predictor on a more precise timescale, such as on hundredths of seconds (i.e., with a precision of 0.01 seconds). In other words, we oversample the predictor with a factor 100. Then, we can set the indices in this oversampled predictor to our onsets with a higher precision (e.g., for a particular onset at 5.12 seconds, we can set the predictor at index 512 to 1).</p>
<p>Let’s do this below for some hypothetical onsets logged with millisecond precision for an experiment lasting 50 seconds:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">onsets2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.62</span><span class="p">,</span> <span class="mf">16.26</span><span class="p">,</span> <span class="mf">34.12</span><span class="p">,</span> <span class="mf">42.98</span><span class="p">])</span>  <span class="c1"># in seconds</span>
<span class="n">duration</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># duration experiment in seconds</span>
<span class="n">osf</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># osf = OverSampling Factor</span>

<span class="n">pred2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">duration</span> <span class="o">*</span> <span class="n">osf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of oversampled (factor: 1000) predictor: </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">pred2</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

<span class="n">onsets2_in_msec</span> <span class="o">=</span> <span class="p">(</span><span class="n">onsets2</span> <span class="o">*</span> <span class="n">osf</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># we convert it to int, because floats (even when it&#39;s 5.0) cannot be used as indices</span>
<span class="n">pred2</span><span class="p">[</span><span class="n">onsets2_in_msec</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Size of oversampled (factor: 1000) predictor: 5000
</pre></div>
</div>
</div>
</div>
<p>Now, to convolve this signal, we should also define our HRF with a higher temporal precision (i.e., hundredths of seconds). Notice that the HRF is also a lot smoother when defined it on a timescale of seconds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">osf</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">TR</span>
<span class="n">hrf_ms</span> <span class="o">=</span> <span class="n">glover_hrf</span><span class="p">(</span><span class="n">tr</span><span class="o">=</span><span class="n">TR</span><span class="p">,</span> <span class="n">oversampling</span><span class="o">=</span><span class="n">osf</span><span class="p">,</span> <span class="n">time_length</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">hrf_ms</span> <span class="o">/=</span> <span class="n">hrf_ms</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>  <span class="c1"># scale such that max = 1</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hrf_ms</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">hrf_ms</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (in tens of milliseconds!) after stimulus onset&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Double gamma HRF&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_133_0.png" src="../../_images/glm_part1_estimation_133_0.png" />
</div>
</div>
<p>Now, let’s convolve our predictor with the HRF again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred2_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">pred2</span><span class="p">,</span> <span class="n">hrf_ms</span><span class="p">)[:</span><span class="n">pred2</span><span class="o">.</span><span class="n">size</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pred2_conv</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (in hundreths of seconds!) after stimulus onset&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity (A.U.)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;High precision convolved predictor&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_135_0.png" src="../../_images/glm_part1_estimation_135_0.png" />
</div>
</div>
<p>Now the only thing that we have to do is to downsample the convolved predictor!</p>
<div class='alert alert-warning'>
    <b>ToDo</b> (1 point): Assuming that the described experiment had a TR of 1.25 seconds and 40 volumes, downsample the convolved predictor (<tt>pred2_conv</tt>) to the timescale of the hypothetical signal (i.e., resample the predictor to the timepoints $t=0, t=1.25, t=2.5, ... , t=48.75$). Call this downsampled predictor <tt>pred2_conv_ds</tt>.
<p>Tip: think about the original scale (msec) and the desired scale (steps of 1.25 sec) of your predictor.</p>
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implement your ToDo here</span>
<span class="n">original_scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">duration</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># in steps of 0.01 seconds, i.e., hundredths of seconds</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo &#39;&#39;&#39;</span>

<span class="k">try</span><span class="p">:</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">pred2_conv_ds</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">40</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downsampled predictor is not the right size!&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">pred2_conv_ds</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="mf">12.4886</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downsampling went wrong!&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Well done!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="fitting-an-hrf-informed-model">
<h4>Fitting an HRF-informed model<a class="headerlink" href="#fitting-an-hrf-informed-model" title="Permalink to this headline">¶</a></h4>
<p>Finally … we’re ready to see whether the HRF-based predictor <em>actually</em> models our original voxel signal (<code class="docutils literal notranslate"><span class="pre">voxel_signal</span></code>, from earlier in the tutorial) more accurately! Let’s create a proper design-matrix (<span class="math notranslate nohighlight">\(X\)</span>) by stacking an intercept with the stimulus-regressor, perform the regression analysis, and check out the results (by plotting the predicted signal against the true signal). For comparison, we’ll also plot the original (unconvolved) model as well!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">predictor_conv_ds</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># Add back a singleton axis (which was removed before downsampling)</span>
    <span class="c1"># otherwise stacking will give an error</span>
    <span class="n">predictor_conv_ds</span> <span class="o">=</span> <span class="n">predictor_conv_ds</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">predictor_conv_ds</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X_conv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">intercept</span><span class="p">,</span> <span class="n">predictor_conv_ds</span><span class="p">))</span>
<span class="n">betas_conv</span> <span class="o">=</span> <span class="n">inv</span><span class="p">(</span><span class="n">X_conv</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_conv</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_conv</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">voxel_signal</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_conv</span> <span class="o">@</span> <span class="n">betas_conv</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Activity (A.U.)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit with *convolved* regressor&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;True signal&#39;</span><span class="p">,</span> <span class="s1">&#39;Predicted signal&#39;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">)</span>
<span class="n">betas_simple</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1000.64701684</span><span class="p">,</span> <span class="mf">1.02307437</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_simple</span> <span class="o">@</span> <span class="n">betas_simple</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_lim</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Activity (A.U.)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit with original (*unconvolved*) regressor&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;True signal&#39;</span><span class="p">,</span> <span class="s1">&#39;Predicted signal&#39;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (volumes)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_141_0.png" src="../../_images/glm_part1_estimation_141_0.png" />
</div>
</div>
<p>Wow, that looks much better! First, let’s inspect the beta-parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The beta-parameter of our stimulus-predictor is now: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">betas_conv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;... which is </span><span class="si">%.3f</span><span class="s1"> times larger than the beta of our original &#39;</span>
      <span class="s1">&#39;beta (based on the unconvolved predictors)!&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">betas_conv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">betas_simple</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The beta-parameter of our stimulus-predictor is now: 6.511
... which is 6.364 times larger than the beta of our original beta (based on the unconvolved predictors)!
</pre></div>
</div>
</div>
</div>
<p>Like we did before, we’ll zoom in and show you how the estimated beta-parameters relate tho the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_signal_and_predicted_signal_zoom</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">,</span> <span class="n">predictor_conv_ds</span><span class="p">,</span> <span class="n">x_lim</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">65</span><span class="p">),</span> <span class="n">y_lim</span><span class="o">=</span><span class="p">(</span><span class="mi">995</span><span class="p">,</span> <span class="mi">1010</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_145_0.png" src="../../_images/glm_part1_estimation_145_0.png" />
</div>
</div>
<p>Alright, so we seem to measure a way larger effect of our stimulus on the voxel activity, but is the model fit actually also better? Let’s find out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">lstsq</span>  <span class="c1"># numpy implementation of OLS, because we&#39;re lazy</span>

<span class="n">y_hat_conv</span> <span class="o">=</span> <span class="n">X_conv</span> <span class="o">@</span> <span class="n">betas_conv</span>
<span class="n">y_hat_orig</span> <span class="o">=</span> <span class="n">X_simple</span> <span class="o">@</span> <span class="n">lstsq</span><span class="p">(</span><span class="n">X_simple</span><span class="p">,</span> <span class="n">voxel_signal</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">MSE_conv</span> <span class="o">=</span> <span class="p">((</span><span class="n">y_hat_conv</span> <span class="o">-</span> <span class="n">voxel_signal</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">MSE_orig</span> <span class="o">=</span> <span class="p">((</span><span class="n">y_hat_orig</span> <span class="o">-</span> <span class="n">voxel_signal</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE of model with convolution is </span><span class="si">%.3f</span><span class="s2"> while the MSE of the model without convolution is </span><span class="si">%.3f</span><span class="s2">.&quot;</span> <span class="o">%</span>
     <span class="p">(</span><span class="n">MSE_conv</span><span class="p">,</span> <span class="n">MSE_orig</span><span class="p">))</span>

<span class="n">R2_conv</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">voxel_signal</span> <span class="o">-</span> <span class="n">y_hat_conv</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">voxel_signal</span> <span class="o">-</span> <span class="n">voxel_signal</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">R2_orig</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">voxel_signal</span> <span class="o">-</span> <span class="n">y_hat_orig</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">voxel_signal</span> <span class="o">-</span> <span class="n">voxel_signal</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R-squared of model with convolution is </span><span class="si">%.5f</span><span class="s2"> and without convolution it is </span><span class="si">%.5f</span><span class="s2">.&quot;</span> <span class="o">%</span> 
     <span class="p">(</span><span class="n">R2_conv</span><span class="p">,</span> <span class="n">R2_orig</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE of model with convolution is 7.192 while the MSE of the model without convolution is 10.327.
R-squared of model with convolution is 0.30629 and without convolution it is 0.00388.
</pre></div>
</div>
</div>
</div>
<p>From the model fit metrics above, we can safely conclude that (at least for this voxel), a design (<span class="math notranslate nohighlight">\(X\)</span>) in which we include information about the expected lag/shape of the HRF is <em>way</em> better than a ‘HRF-naive’ design (i.e. an unconvolved design).</p>
<div class='alert alert-warning'>
<b>ToDo</b> (2 points):
<p>So far, our examples were based on the stimulus-onsets of the two conditions (circles and squares) lumped together. This tested the hypothesis of our voxel responded to <em>any kind</em> of stimulus – regardless of the condition (squares/circles) of the stimulus. Usually, however, you want to estimate the betas for each condition separately (i.e., how much each condition on average activates a voxel) and test the influence of each condition on the voxel separately (but estimated in the same model)! This is what you’re going to do in this ToDo.</p>
<p>We provide you with the predictors for circles (<tt>predictor_circles</tt>) and for squares (<tt>predictor_squares</tt>) below. You have to do the following:</p>
<ul class="simple">
<li><p>convolve each predictor with the double-gamma HRF (use <tt>canonical_hrf</tt>) separately (don’t forget to squeeze, trim, and add the axis back)</p></li>
<li><p>downsample the convolved predictors</p></li>
<li><p>stack an intercept and the two predictors <strong>in a single design-matrix</strong> (<span class="math notranslate nohighlight">\(X\)</span>) – use <tt>np.hstack((intercept, pred1, pred2))</tt> for this</p></li>
<li><p>calculate the beta-parameters (estimated in a single model!)</p></li>
<li><p>calculate MSE (store this in the variable <tt>mse_new</tt>) and <span class="math notranslate nohighlight">\(R^2\)</span> (store this in the variable <tt>r2_new</tt>)</p></li>
</ul>
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictor_circles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">800</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">predictor_circles</span><span class="p">[</span><span class="n">onsets_circles</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">predictor_squares</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">800</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">predictor_squares</span><span class="p">[</span><span class="n">onsets_squares</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Implement your solution below</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the ToDo above (but only if the variables are named correctly). &#39;&#39;&#39;</span>

<span class="k">if</span> <span class="s1">&#39;mse_new&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not find the variable &#39;mse_new&#39;! Did you spell it correctly?&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="s1">&#39;r2_new&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not find the variable &#39;r2_new&#39;! Did you spell it correctly?&quot;</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">mse_new</span><span class="p">,</span> <span class="mf">7.1633</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">r2_new</span><span class="p">,</span> <span class="mf">0.3091</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class='alert alert-info'>
<b>ToThink</b> (1 point): If you've done the above ToDo correctly, you should have found that the model fit of the design-matrix with the circles and squares predictors separately (as you did in the ToDo) leads to a (somewhat) better model fit (lower MSE/higher $R^2$) than the design-matrix with the conditions lumped together in a single predictor (as we did earlier).
<p>Argue why you think this may be the case here.</p>
</div><p>YOUR ANSWER HERE</p>
</div>
<div class="section" id="temporal-basis-functions">
<h4>Temporal basis functions<a class="headerlink" href="#temporal-basis-functions" title="Permalink to this headline">¶</a></h4>
<p>Most studies use a canonical HRF to convolve with their predictors. However, remember that using a canonical HRF assumes that the particular shape of that HRF will be appropriate for each voxel, each condition, and each subject in your study. This is quite a strong assumption. In fact, studies have shown that the exact shape of the HRF often differs between voxels, conditions, and subjects (as is explained in detail by the <a class="reference external" href="https://www.youtube.com/watch?v=YfeMIcDWwko&amp;index=21&amp;list=PLcvMDPDk-dSmTBejANv7kY2mFo1ni_gkA">video on basis sets</a> by Tor Wager).</p>
<p>In fact, this might also be the case in our data! If you’ve done the ToDo correctly, you might have seen that the predictions (<span class="math notranslate nohighlight">\(\hat{y}\)</span>) seem to “peak” too late for the circle-stimuli … In fact, let’s plot the data (<span class="math notranslate nohighlight">\(y\)</span>) and the prediction (based on the previously defined stimulus predictor, i.e., the circles and squares in a single predictor):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_signal_and_predicted_signal_zoom</span><span class="p">(</span><span class="n">voxel_signal</span><span class="p">,</span> <span class="n">X_conv</span><span class="p">,</span> <span class="n">x_lim</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">55</span><span class="p">),</span> <span class="n">y_lim</span><span class="o">=</span><span class="p">(</span><span class="mi">990</span><span class="p">,</span> <span class="mi">1015</span><span class="p">),</span> <span class="n">plot_params</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_155_0.png" src="../../_images/glm_part1_estimation_155_0.png" />
</div>
</div>
<p>So, what should be do about this? Well, one solution is to use <em>temporal basis functions</em> (also called <em>temporal basis sets</em>). Temporal basis functions model the HRF as <em>a combination of (hemodynamic response) functions</em>.</p>
<p>In practice, this amounts to convolving your predictor with not one, but multiple HRFs. This results in multiple predictors per stimulus-condition! Each HRF measures a “part” (or property) of the total HRF. Together, these predictors aim to estimate the complete HRF for a given stimulus-vector (condition).</p>
<p>We’re going to use <em>double-gamma basis functions</em> as an example of a temporal basis set (but there are other sets, like the (<em>single-gamme basis set</em>, <em>sine basis set</em> and <em>finite impulse response</em> set). In this particular basis set, the original double-gamma HRF is used in combination with its first derivative (often called the ‘temporal derivative’). (Sometimes, the second derivative, also called the “dispersion derivative”, is also added. We leave this out for simplicity here.)</p>
<p>Suppose we have only one stimulus condition. Then, the signal (<span class="math notranslate nohighlight">\(\mathbf{y}\)</span>) is not modelled by only one convolved predictor (<span class="math notranslate nohighlight">\(X_{\mathrm{stim}}\)</span>) but by two predictors: a predictor convolved with the original HRF (<span class="math notranslate nohighlight">\(X_{\mathrm{orig}}\)</span>) and a predictor convolved with the temporal derivative of the HRF (<span class="math notranslate nohighlight">\(X_{\mathrm{temp}}\)</span>). Formally:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ac3615d6-4824-4cef-889e-4453a99f904d">
<span class="eqno">(21)<a class="headerlink" href="#equation-ac3615d6-4824-4cef-889e-4453a99f904d" title="Permalink to this equation">¶</a></span>\[\begin{align}
y = \beta_{0} + \mathbf{X}_{\mathrm{orig}}\beta_{1} + \mathbf{X}_{\mathrm{temp}}\beta_{2} + \epsilon
\end{align}\]</div>
<p>Alright, but how do we compute this temporal derivative and how does this look like? Again, we can use a function from the <a class="reference external" href="https://nilearn.github.io">nilearn</a> package: <code class="docutils literal notranslate"><span class="pre">glover_time_derivative</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.glm.first_level.hemodynamic_models</span> <span class="kn">import</span> <span class="n">glover_time_derivative</span>

<span class="n">tderiv_hrf</span> <span class="o">=</span> <span class="n">glover_time_derivative</span><span class="p">(</span><span class="n">tr</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">oversampling</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tderiv_hrf</span> <span class="o">/=</span> <span class="n">tderiv_hrf</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">canonical_hrf</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Activity (A.U.)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (seconds)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Original HRF&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tderiv_hrf</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (seconds)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;First (temporal) derivative&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_157_0.png" src="../../_images/glm_part1_estimation_157_0.png" />
</div>
</div>
<p>The cool thing about this double-gamma basis set is that the derivatives can (to a certain extent) correct for slight deviations in the lag and shape of the HRF based on the data! Specifically, the first (temporal) derivative can correct for slight differences in lag (compared to the canonical single-gamma HRF) and the second (dispersion) derivative can correct for slight difference in the width (or “dispersion”) of the HRF (compared to the canonical single-gamma HRF).</p>
<p>“How does this ‘correction’ work, then?”, you might ask. Well, think about it this way: the original (canonical) HRF measures the increase/decrease — or amplitude — of the BOLD-response. In a similar way, the temporal derivative measures the <em>onset</em> — or lag — of the BOLD-response. (And if you use the dispersion derivative: this would measure the <em>width</em> of the BOLD-response.)</p>
<p>When we use our two predictors (one convolved with the canonical HRF, one with the temporal derivative) in a linear regression model, the model will assign each predictor (each part of the HRF) a beta-weight, as you know. These beta-weights are chosen such that model the data — some response of the voxel to a stimulus — as well as possible. Basically, assigning a (relatively) high (positive or negative) beta-weight to the predictor convolved with the temporal derivative will “shift” the HRF (increases/decreases the onset of the HRF).</p>
<p>Alright, let’s visualize this. Suppose we have a voxel that we know does not conform to the specific assumptions about lag (onset) of the canonical (double-gamma) HRF. Specifically, we see that the canonical HRF peaks too early.
We’ll show below that it suboptimally explains this voxel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">niedu.utils.nii</span> <span class="kn">import</span> <span class="n">simulate_signal</span>

<span class="c1"># tbs = temporal basis set</span>
<span class="n">y_tbs</span><span class="p">,</span> <span class="n">X_tbs</span> <span class="o">=</span> <span class="n">simulate_signal</span><span class="p">(</span>
    <span class="n">onsets</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span>
    <span class="n">conditions</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;stim&#39;</span><span class="p">],</span>
    <span class="n">duration</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">TR</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">icept</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">params_canon</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span>
    <span class="n">params_deriv1</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">8</span><span class="p">],</span>
    <span class="n">std_noise</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">rnd_seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">plot</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">X_canon_only</span> <span class="o">=</span> <span class="n">X_tbs</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># remove temporal derivative for now</span>

<span class="c1"># Do regression </span>
<span class="n">beta_canon</span> <span class="o">=</span> <span class="n">inv</span><span class="p">(</span><span class="n">X_canon_only</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_canon_only</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_canon_only</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_tbs</span>
<span class="n">yhat_canon</span> <span class="o">=</span> <span class="n">X_canon_only</span> <span class="o">@</span> <span class="n">beta_canon</span>

<span class="c1"># Plot the data and the prediction (y_hat)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_tbs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">yhat_canon</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">98</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (seconds)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Activation (A.U.)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
             <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;&lt;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="s1">&#39;Stim</span><span class="se">\n</span><span class="s1">onset&#39;</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;signal&#39;</span><span class="p">,</span> <span class="s1">&#39;predicted signal&#39;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prediction with canonical HRF only&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_159_0.png" src="../../_images/glm_part1_estimation_159_0.png" />
</div>
</div>
<p>As you can see, the predicted signal (orange line) misses the peak of the BOLD-response. Now, let’s see what happens if we add the temporal derivative to the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Do regression with HRF + temp deriv HRF</span>
<span class="n">beta_td</span> <span class="o">=</span> <span class="n">inv</span><span class="p">(</span><span class="n">X_tbs</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_tbs</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_tbs</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_tbs</span>
<span class="n">yhat_td</span> <span class="o">=</span> <span class="n">X_tbs</span> <span class="o">@</span> <span class="n">beta_td</span>

<span class="c1"># Plot model with temp deriv HRF</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_tbs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">yhat_td</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">98</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Activation (A.U.)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
             <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;&lt;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="s1">&#39;Stim</span><span class="se">\n</span><span class="s1">onset&#39;</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prediction with canonical HRF + temporal deriv&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time (seconds)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/glm_part1_estimation_161_0.png" src="../../_images/glm_part1_estimation_161_0.png" />
</div>
</div>
<p>As you can see, the prediction improves quite a bit when including the temporal derivative! But how should we interpret the beta-parameters? Well, usually people don’t really interpret the temporal (and dispersion derivative) HRFs (unless they’re interested in lag/width of the HRF), because most researchers are interesting in the activation/deactivation (the amplitude) of voxels in response to a stimulus, which corresponds to the beta-parameters associated with the canonical HRF. So, basically, the temporal (and dispersion) derivative(s) are only used to “correct” for deviations in terms of lag/shape from the canonical HRF!</p>
<p>So, should you then always use a (gamma) basis set? People are quite divided on the topic of whether to use basis sets or a canonical HRF. In our experience, derivatives (e.g. in the gamma basis sets) offer little improvement over a canonical HRF in simple group-level analyses (in which you average over many participants; the topic of week 5/6!), but it doesn’t hurt either (given that you have “enough” degrees of freedom).</p>
<div class='alert alert-warning'>
    <b>ToDo</b> (optional; 0 points): For gamma-based basis sets, the different components correlate with each other (especially the canonical HRF and its dispersion derivative). As such, these two predictors will "compete" for explained variance, which affects their parameter estimates and associated statistics. Because people are often only interested in the amplitude component of our basis set, they sometimes "orthogonalize" the HRF derivatives with respect to the canonical HRF predictor. Orthogonalization is a process in which a predictor is "decorrelated" from another (set of) predictors(s) (for more details on orthogonalization, see <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126255">this excellent article</a>). Orthogonalization amounts to regressing out the to-be-orthogalized predictor from the other predictor(s).
<p>In this ToDo, try orthogonalizing the canonical HRF predictor with respect to its temporal derivative. Using the variable <tt>Xc</tt> below (which contains just two predictors: the canonical HRF predictor and the temporal derivative predictor). Make sure to <em>replace</em> the temporal derivative predictor (i.e., second column) with the predictor with the variance related to the canonical HRF removed).</p>
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Implement your ToDo here. &#39;&#39;&#39;</span>
<span class="c1"># First column: canonical HRF predictor, second column: temp. deriv predictor</span>
<span class="n">Xc</span> <span class="o">=</span> <span class="n">X_tbs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo. &#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_orthogonalization</span>
<span class="n">test_orthogonalization</span><span class="p">(</span><span class="n">X_tbs</span><span class="p">,</span> <span class="n">Xc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class='alert alert-warning'>
<b>ToDo</b> (5 points):
<p>Reanalyze the voxel signal with the separate conditions (like the last ToDo), but this time with a both the canonical HRF (<tt>canonical_hrf</tt>) and the associated temporal derivative (you can use the variable <tt>tderiv_hrf</tt>)! Calculate the beta-parameters for the voxel signal (i.e., the variable <tt>voxel_signal</tt>), MSE, and <span class="math notranslate nohighlight">\(R^2\)</span>. Store the MSE in a variable named <tt>mse_gbf</tt> and <span class="math notranslate nohighlight">\(R^2\)</span> in a variable named <tt>r2_gbf</tt>.</p>
<p>Please implement this assignment in “steps”, such that we can test intermediate output:</p>
<ol class="simple">
<li><p>Convolve the circle predictor (<tt>predictor_circles</tt>) and the squares predictor (<tt>predictor_squares</tt>) with the two HRF basis functions (canonical, temporal deriv.) separately, giving you 4 predictors, stack them together and add an intercept (make sure the intercept is the first column). Then, downsample your design matrix to the scale of you voxel signal. Store your downsampled design matrix in a variable named <tt>X_gbf</tt>; Note: the order of the columns should be: intercept, circles (canonical), circles (temp. deriv.), squares (canonical), squares (temp. deriv.). If you don’t do this, automatic grading will fail and you won’t get any points; (2 points)</p></li>
<li><p>Run linear regression (your DV is the variable <tt>voxel_signal</tt>) and store your betas in a variable named <tt>betas_gbf</tt>; (1 point)</p></li>
<li><p>Calculate R-squared and store it in a variable named <tt>r2_gbf</tt>; (1 point)</p></li>
<li><p>Calculate MSE and store it in a variable named <tt>mse_gbf</tt>; (1 point)</p></li>
</ol>
<p>Some tips:</p>
<ul class="simple">
<li><p>you can use the definitions of the HRFs from earlier (<tt>canonical_hrf</tt> and <tt>tderiv_hrf</tt>)</p></li>
<li><p>make sure that your design-matrix has, eventually, 5 columns (intercept + 2 predictors x 2 conditions)</p></li>
<li><p>don’t forget to downsample your design matrix</p></li>
</ul>
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1: convolve the predictors (don&#39;t forget to trim and downsample)!</span>
<span class="c1"># Hint: print the shape of your predictors after convolving, trimming, and downsampling - </span>
<span class="c1"># does this shape correspond to the number of datapoints of the experiment?</span>

<span class="c1"># We have created the binary predictors for you already</span>
<span class="n">predictor_circles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">800</span><span class="p">)</span>
<span class="n">predictor_circles</span><span class="p">[</span><span class="n">onsets_circles</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">predictor_squares</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">800</span><span class="p">)</span>
<span class="n">predictor_squares</span><span class="p">[</span><span class="n">onsets_squares</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo. &#39;&#39;&#39;</span>

<span class="k">if</span> <span class="s1">&#39;X_gbf&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">():</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Could not find the variable &#39;X_gbf&#39;; did you name it correctly?&quot;</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span><span class="n">X_gbf</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X_gbf</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First column is not an intercept (vector of ones)!&quot;</span><span class="p">)</span>
    <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="k">if</span> <span class="n">X_gbf</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">400</span><span class="p">:</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;You probably forgot to trim and/or downsample ...&#39;</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_canon_and_tderiv_design</span>
<span class="n">test_canon_and_tderiv_design</span><span class="p">(</span><span class="n">predictor_circles</span><span class="p">,</span> <span class="n">predictor_squares</span><span class="p">,</span> <span class="n">canonical_hrf</span><span class="p">,</span> <span class="n">tderiv_hrf</span><span class="p">,</span> <span class="n">X_gbf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 2: run linear regression</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above steps. &#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_canon_and_tderiv_glm</span>
<span class="n">test_canon_and_tderiv_glm</span><span class="p">(</span><span class="n">X_gbf</span><span class="p">,</span> <span class="n">voxel_signal</span><span class="p">,</span> <span class="n">betas_gbf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 3: calculate R-squared (and store it in a variable named r2_gbf)</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo. &#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_canon_and_tderiv_r2</span>
<span class="n">test_canon_and_tderiv_r2</span><span class="p">(</span><span class="n">X_gbf</span><span class="p">,</span> <span class="n">betas_gbf</span><span class="p">,</span> <span class="n">voxel_signal</span><span class="p">,</span> <span class="n">r2_gbf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 4: calculate MSE (and store it in a variable named mse_gbf)</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo. &#39;&#39;&#39;</span>
<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_canon_and_tderiv_mse</span>
<span class="n">test_canon_and_tderiv_mse</span><span class="p">(</span><span class="n">X_gbf</span><span class="p">,</span> <span class="n">betas_gbf</span><span class="p">,</span> <span class="n">voxel_signal</span><span class="p">,</span> <span class="n">mse_gbf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>From what we’ve showed so far, hopefully, you noticed that how linear regression is applied to model a voxel signal is not that much different from ‘regular’ data, except for the convolution/HRF part. At this moment, you already know 95% of how univariate analysis works! There are, however, still a couple of concepts we need to address, which we’ll do next week: statistical inference of model parameters.</p>
<div class='alert alert-warning'>
<b>ToDo</b> (3 points)
<p>For the last ToDo, let’s do a short “puzzle”. Below, we’ll load a function, called <tt>black_box</tt>, that implements a linear regression model with the following functional form:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9ea39778-e03d-4d82-b653-d1c3ad349e9a">
<span class="eqno">(22)<a class="headerlink" href="#equation-9ea39778-e03d-4d82-b653-d1c3ad349e9a" title="Permalink to this equation">¶</a></span>\[\begin{align}
y = \beta_{0} + x_{1}\beta_{1} + x_{2}\beta_{2} + x_{3}\beta_{3}  
\end{align}\]</div>
<p>As you can see, it has four parameters (no error term!): <span class="math notranslate nohighlight">\(\beta_{0}\)</span> represents the intercept here and <span class="math notranslate nohighlight">\(\beta_{1}\)</span> up to <span class="math notranslate nohighlight">\(\beta_{4}\)</span> represent the parameters of the model’s single predictors (<span class="math notranslate nohighlight">\(x_{1}\)</span> up to <span class="math notranslate nohighlight">\(x_{3}\)</span>). The function <tt>black_box</tt> takes three inputs: <tt>x1</tt>, <tt>x2</tt>, and <tt>x3</tt>. The function outputs a (deterministic) value for <span class="math notranslate nohighlight">\(y\)</span>. So, you could use (“query”) the function like this:</p>
<p><tt>y_output = black_box(x1=10, x2=5, x3=2)</tt></p>
<p>Now, your assignment is to, experimentally, figure out (“reverse engineer”) the model’s parameters <span class="math notranslate nohighlight">\(\beta_{0}\)</span>, <span class="math notranslate nohighlight">\(\beta_{1}\)</span>, <span class="math notranslate nohighlight">\(\beta_{2}\)</span>, and <span class="math notranslate nohighlight">\(\beta_{3}\)</span>. In other words, run the function with some inputs values and check the corresponding outputs. From those combinations of inputs and outputs, you can reconstruct the four different parameters. Once you figure out these betas, store them in the variables <tt>beta_0</tt>, <tt>beta_1</tt>, <tt>beta_2</tt>, and <tt>beta_3</tt>.</p>
<p>Also, explain — concisely — how you figured out the answer in the text cell below the test-cell.</p>
</div><div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">black_box</span>
<span class="c1"># Implement your todo here!</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39; Tests the above ToDo. &#39;&#39;&#39;</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s1">&#39;3&#39;</span><span class="p">]:</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="s1">&#39;beta_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You didn&#39;t define the variable beta_</span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">b</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">niedu.tests.nii.week_2</span> <span class="kn">import</span> <span class="n">test_black_box</span>
<span class="n">test_black_box</span><span class="p">(</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span><span class="p">,</span> <span class="n">beta_2</span><span class="p">,</span> <span class="n">beta_3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>YOUR ANSWER HERE</p>
<div class='alert alert-success'>
    <b>Tip!</b>
    Before handing in your notebooks, we recommend restarting your kernel (<em>Kernel</em> &rarr; <em>Restart & Clear Ouput</em>) and running all your cells again (manually, or by <em>Cell</em> &rarr; <em>Run all</em>). By running all your cells one by one (from "top" to "bottom" of the notebook), you may spot potential errors that are caused by accidentally overwriting your variables or running your cells out of order (e.g., defining the variable 'x' in cell 28 which you then use in cell 15).
</div></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./fMRI-introduction/week_2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../../section_intros/2_glm.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Using the GLM to model fMRI data</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../week_3/glm_part2_inference.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The GLM, part 2: inference</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Lukas Snoek<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>