{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroimaging week 4: Preprocessing\n",
    "Preprocessing of (f)MRI data is quite a complex topic, involving techniques from signal processing (filtering), linear algebra/statistics (prewhitening, autocorrelation correction), and numerical optimization (image registration). Additionally, there is ongoing discussion about which preprocessing steps are necessary/sufficient and what preprocessing parameters are optimal. As always, this depends on your specific research question, the type and quality of your data, and the type of analysis.\n",
    "\n",
    "In this week, we'll discuss a couple (not all!) of preprocessing steps that are common in univariate fMRI analyses. \n",
    "\n",
    "**What you'll learn**: after this lab, you'll ...\n",
    "\n",
    "- be able to explain the influence of preprocessing on the measured effects using the t-value formula\n",
    "- understand (the advantage of) temporal filtering from both the time-domain and frequency-domain\n",
    "- understand the necessity of prewhitening given the assumptions of the GLM\n",
    "- understand the advantage of spatial filtering (smoothing)\n",
    "- understand how to handle outliers\n",
    "- explain the necessity of motion correction and the advantage of motion regression\n",
    "- know how to implement the concepts above in Python\n",
    "\n",
    "**Estimated time needed to complete**: 14-20 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Tip!</b>\n",
    "This lab contains several slightly more difficult ToDos (e.g., the first one) and is somewhat longer than the previous weeks. It's totally okay to skip a ToDo/ToThink/Assignment if you find it too hard or takes too much time! Almost always, the ToDos/ToThinks/assignments are independent of each other, so skipping one doesn't affect the others. The points you get for a single ToDo represents just a tiny fraction of all points, so skipping one does not affect your grade for this notebook a lot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "As we said before, preprocessing is a topic that almost warrants its own course. Nonetheless, we'll try to show you (and let you practice with) some of the most common and important preprocessing operations. Additionally, we'll introduce the concept of the fast fourier transform, which allows us to analyze our signal in the frequency domain, which helps to understand several preprocessing steps, such as temporal filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The *t*-value formula &mdash; yet again\n",
    "The previous two weeks, you have learned that, essentially, we want to find large effects (calculated as *t*-values) of our contrasts by optimizing various parts of the *t*-value formula. Conceptually, the *t*-value formula for a particular contrast ($\\mathbf{c}$) can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "t\\mathrm{-value} = \\frac{\\mathrm{effect}}{\\mathrm{uncertainty}} = \\frac{\\mathrm{effect}}{\\sqrt{\\mathrm{noise} \\cdot \\mathrm{design\\ variance}}} = \\frac{\\mathbf{c}\\hat{\\beta}}{\\sqrt{\\hat{\\sigma}^{2}\\mathbf{c}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{c}^{T}}}\n",
    "\\end{align}\n",
    "\n",
    "Last week you've learned that by ensuring low design variance (through *high* predictor variance and *low* predictor correlations) leads to larger normalized effects (higher *t*-values). This week, we will discuss the other term of the denominator of this formula: the noise (also called the residual variance or unexplained variance), which is defined in the *t*-value formula as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{noise} = \\frac{\\mathrm{SSE}}{\\mathrm{DF}} = \\frac{\\sum_{i=1}^{N}(y_{i} - \\hat{y}_{i})^{2}}{N - P}\n",
    "\\end{align}\n",
    "\n",
    "Through preprocessing, we aim to reduce the difference between our prediction ($\\hat{\\mathbf{y}}$) and our true signal ($\\mathbf{y}$), thus reducing the noise-term of the formula and thereby optimizing our (\"normalized\") effects.\n",
    "\n",
    "### On temporal vs. spatial preprocessing\n",
    "Roughly speaking, you can divide fMRI preprocessing into two types of operations: temporal and spatial. Temporal preprocessing involves operations that filter or otherwise affect the properties of your data across the time dimension (i.e., the time series). This is, of course, only applicable to *functional* MRI data (not structural or diffusion MRI). Examples of temporal operations are high-pass filtering and slice-time correction. Spatial preprocessing involves operations that filter or otherwise affect the spatial properties of your data (such as spatial orienatation, resolution, and shape). Examples of these spatial operations are spatial filtering (\"smoothing\"), distortion correction, motion correction (realignment), and spatial normalization/resampling.\n",
    "\n",
    "In this lab, we'll discuss these operations per theme: we'll start with temporal operations and then move to spatial operations. Note that this *is not* the order in which data is usually preprocessed (see page 35 of the \"Handbook of functional MRI Data Anslysis\" textbook).\n",
    "\n",
    "Let's start with temporal preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, some imports!\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from nilearn.glm.first_level.hemodynamic_models import glover_hrf\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal preprocessing\n",
    "In this section, we will discuss how temporal preprocessing may greatly reduce the error term of our statistics. We'll also delve into a variant of the GLM that deals with autocorrelation in our signals appropriately (\"generalized least squares\", or GLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice-time correction\n",
    "Slice-time correction is a temporal resampling technique that corrects for the fact that, in (most) BOLD-MRI scan sequences, volumes are acquired slice by slice. This means that each slices is acquired at a slightly different time. For example, suppose that we are acquiring an fMRI run with a TR of 2 seconds and our volumes consist of 40 slices (acquired axially, inferior &rarr; superior). In this case, the acquisition of each slice takes $\\frac{TR}{N_{slice}} = \\frac{2}{40} = 0.05$ seconds (let's call this `dt`). This means that, for the very first volume, the \"onset\" of the first slice is at 0 seconds (and lasts until 0.05 seconds), while the onset of the last slice is at 1.95 (and lasts until 2.00 seconds). In code, we can calculate these slice onsets **within a volume** using the `np.linspace(start, stop, n_steps)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = 2\n",
    "n_slices = 40\n",
    "slice_onsets_within_volume = np.linspace(0, TR, n_slices, endpoint=False)\n",
    "# Note, this would have worked as well: np.arange(0, TR, TR / 40)\n",
    "\n",
    "print(\"Length of slice onsets: %i\" % slice_onsets_within_volume.size, end='\\n\\n')\n",
    "print(\"Onsets of slices for first volume: %s\" % (slice_onsets_within_volume,), end='\\n\\n')\n",
    "print(\"Acquisition of first slice (of first volume) started at %.2f sec.\" % slice_onsets_within_volume[0])\n",
    "print(\"Acquisition of last slice (of first volume) started at %.2f sec.\" % slice_onsets_within_volume[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume we do a very simple experiment, lasting 32 seconds (i.e., 16 volumes with a TR of 2), in which we show a single stimulus at $t=0$. Furthermore, let's assume that the *entire brain* responds to this stimulus with an idealized (noiseless response). In other words, the response of each voxel will look exactly like the HRF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling = 100\n",
    "exp_length = 32\n",
    "n_vols = 32 // TR\n",
    "\n",
    "ideal_response = glover_hrf(tr=TR, time_length=exp_length, oversampling=TR * oversampling)\n",
    "ideal_response /= ideal_response.max()\n",
    "t = np.linspace(0, exp_length, ideal_response.size, endpoint=False)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(t, ideal_response)\n",
    "plt.grid()\n",
    "plt.title(\"Idealized response of all voxels\", fontsize=25)\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=20)\n",
    "plt.ylabel(\"Activation (A.U.)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must, however, take into accout that, even when our data would represent an idealized response, it would be sampled at different timepoints due to a difference in onsets for different slices. The first slice would for example be sampled at $[0, 2, 4, 6, ..., 30]$ seconds, while, for example, the middle slice would be sampled at $[1, 3, 5, ... 31]$ seconds. We can calculate the onset per slice **across volumes** using `np.linspace` again:\n",
    "\n",
    "```python\n",
    "# Note: this is the onset of slice number \"slice_num\" **across volumes**\n",
    "onsets_slice_across_volumes = np.linspace(dt * slice_num, length_exp + dt * slice_num, n_vols, endpoint=False)\n",
    "```\n",
    "\n",
    "where `dt` is the time it takes per slice (i.e., $dt = \\frac{TR}{N_{slice}}$). So, to get the slice times for, e.g. slice number 23, for the experiment we have so far (i.e., with a TR of 2, 40 slices, and 32 second duration), we would compute the onsets as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = TR / n_slices\n",
    "slice_num = 23\n",
    "\n",
    "# note the slice_num - 1, as Python uses 0-based indexing\n",
    "start = dt * (slice_num - 1)\n",
    "end = exp_length + dt * (slice_num - 1)\n",
    "onsets_slice23_across_volumes = np.linspace(start, end, n_vols, endpoint=False)\n",
    "print(onsets_slice23_across_volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, even with a noiseless response, the data will look different per slice. We'll plot this below for the first slice and the middle slice (on top of the idealized response):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onsets_s0_across_volumes = np.linspace(0, exp_length, n_vols, endpoint=False)\n",
    "onsets_s20_across_volumes = np.linspace(dt * 19, exp_length + dt * 19, n_vols, endpoint=False)\n",
    "\n",
    "print(\"Onset acquisition of slice 0 (first 10 volumes): %s\" % (onsets_s0_across_volumes[:10],))\n",
    "print(\"Onset acquisition of slice 20 (first 10 volumes): %s\" % (onsets_s20_across_volumes[:10],))\n",
    "\n",
    "# \"Fit\" the interpolation \n",
    "resampler = interp1d(t, ideal_response)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(t, ideal_response)\n",
    "plt.grid()\n",
    "plt.title(\"Resampled slicewise responses\", fontsize=25)\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=20)\n",
    "plt.ylabel(\"Activation (A.U.)\", fontsize=20)\n",
    "\n",
    "for onsets in [onsets_s0_across_volumes, onsets_s20_across_volumes]:\n",
    "    # Do not use this code as an example to solve the next ToDo!\n",
    "    # Because it does it the other way around.\n",
    "    slice_sig = resampler(onsets)  # resample to slice times! kind of like inverse slice-time correction\n",
    "    plt.plot(onsets, slice_sig, marker='*', ls='None', ms=10)\n",
    "\n",
    "plt.legend(['Idealized response', 'Actual response slice 0', 'Actual response slice 20'],\n",
    "           fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we would ignore the fact that these signals were acquired at different times (by assuming they were all acquired at $t=0, t=2, t=4$ etc.), our model will be suboptimal for all slices (except for slice 0). We'll show this below by plotting the resampled data from  slice 0, 20, and 39 on top of the idealized response (which represents our predictor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "slice_data = np.zeros((slice_sig.size, 3))\n",
    "slice_onsets_across_volumes = np.zeros((slice_sig.size, 3))\n",
    "for i, slc in enumerate([0, 19, 39]):\n",
    "    t_slice = np.linspace(dt * slc, exp_length + dt * slc, n_vols, endpoint=False)\n",
    "    slice_resp = resampler(t_slice)\n",
    "    slice_data[:, i] = slice_resp  # save for later\n",
    "    slice_onsets_across_volumes[:, i] = t_slice\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(t, ideal_response)\n",
    "    plt.plot(onsets_s0_across_volumes, slice_resp)\n",
    "    plt.title(\"Slice %i\" % (slc + 1), fontsize=25)\n",
    "    plt.grid()\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.ylabel(\"Activation (A.U.)\", fontsize=20)\n",
    "    \n",
    "    if i == 1:\n",
    "        plt.xlabel(\"Time (seconds)\", fontsize=20)\n",
    "        plt.legend(['Model', 'Actual response'], fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, \"later\" slices are quite misaligned relative to the idealized response/model (which assume that each slice is acquired at the start of the volume). We can fix this by *resampling the data from different slices to the onset times of a reference slice*. Basically, this amounts to saying: \"what would the signal from slice x look like if it was acquired at the onsets of a reference slice\". This process is called *slice-time correction*. \n",
    "\n",
    "Remember the temporal resampling of our \"high precision\" predictors that we did in week 2? We're going to do something similar to our data, here. Basically, we are going to resample our slice-wise timesieres corresponding to a particular set of onsets (let's call these the `original_onsets`) to the set of onsets of a particular reference slice (let's call these the `desired_onsets`). Using the same `interp1d` function from week 2, we can do the following:\n",
    "\n",
    "```python\n",
    "resampler = interp1d(original_onsets, original_signal)\n",
    "std_signal = resampler(desired_onsets)\n",
    "```\n",
    "\n",
    "where `original_onsets` is a numpy array with onsets of each slice **across volumes** and the `original_signal` refers to the actual signal of that voxel (here: slice). They should be the same size. The `desired_onsets` is another numpy array with, *in the case of slice-time correction*, the same size as `original_onsets`.\n",
    "\n",
    "Below, we'll pick the first slice as the reference slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "for i in range(slice_data.shape[1]):\n",
    "    plt.plot(onsets_s0_across_volumes, slice_data[:, i])\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"Activation (A.U.)\", fontsize=20)\n",
    "    plt.title(\"Original slicewise responses\", fontsize=25)\n",
    "\n",
    "# Define reference slice as the first one (with onsets [0, 2, 4, 6, etc])\n",
    "desired_onsets = onsets_s0_across_volumes\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(slice_data.shape[1]):\n",
    "    \n",
    "    # Resample slice-specific onsets to desired onsets\n",
    "    resampler = interp1d(slice_onsets_across_volumes[:, i], slice_data[:, i], fill_value='extrapolate')\n",
    "    stc_slice = resampler(desired_onsets)\n",
    "    \n",
    "    # Plot relative to the reference onsets, i.e., onsets_s0 (first slice)\n",
    "    plt.plot(onsets_s0_across_volumes, stc_slice)\n",
    "    plt.grid()\n",
    "    plt.title(\"Slice-time corrected data\", fontsize=25)\n",
    "    plt.xlabel(\"Time (seconds)\", fontsize=20)\n",
    "    plt.ylabel(\"Activation (A.U.)\", fontsize=20)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the resampled time series resemble each other much more after slice-time correction! It's showing some \"edge artifacts\" which are caused by extrapolation of the time series. In actual neuroimaging software packages, higher-order resampling (such as \"cubic\" or \"spline\" resampling) are used to somewhat mitigate this. Moreover, instead of the first slice, usually the middle slice is chosen as the reference slice, which reduces the amount of resampling (and extrapolation) that has to be done, further mitigating artifacts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (2 points):\n",
    "    \n",
    "*Note*: this is a difficult ToDo! \n",
    "\n",
    "Below, we load in new data (`slice_data`) from a very short experiment (60 seconds, TR = 3). The data are from 1 voxel from every of the 32 slices. Implement slice-time correction by resampling each slice to the onsets of the middle slice (slice no. 16). Use \"cubic\" interpolation (by setting `kind=\"cubic\"` when initializing your resampler). Please store the results in the pre-allocated array `stc_data`.\n",
    "\n",
    "Tip 1: define the TR, number of volumes, length of experiment, and `dt`.<br>\n",
    "Tip 2: before looping over the different slices, define the onsets of the reference slice (remember: Python is 0-indexed).<br>\n",
    "Tip 3: you have to initialize your resampler again for every iteration of your loop!<br>\n",
    "Tip 4: when initializing your resampler, set `fill_value='extrapolate'`.\n",
    "Tip 5: plot your data before and after slice-time correction to see whether it worked properly (it should show you more-or-less aligned timeseries after STC, which may still show some differences in amplitude). Not part of the ToDo, but a good way to check your implementation.\n",
    "\n",
    "Note: this is a ToDo which requires a little more code than usual!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b1db3082ee09d212624efb18d66c171",
     "grade": false,
     "grade_id": "cell-3cf95c7cc4ec7e18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "slice_data = np.load('stc_data.npy')\n",
    "print(\"Slice data is of shape (time x slices): %s\" % (slice_data.shape,))\n",
    "\n",
    "stc_data = np.zeros(slice_data.shape)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4e0c4c6a609e9a8cf07c3ccdd63380e",
     "grade": true,
     "grade_id": "cell-13dc95bb058ac9b1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Tests the above ToDo. \"\"\"\n",
    "from niedu.tests.nii.week_4 import test_stc_todo\n",
    "test_stc_todo(slice_data, stc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short primer on the frequency domain and the Fourier transform\n",
    "Now, before we'll delve into important temporal preprocessing operations, let's discuss how we can represent time series in the frequency domain using the Fourier transform.\n",
    "\n",
    "Thus far, we've always looked at our fMRI-signal as activity that varies across **time**. In other words, we're always looking at the signal in the *time domain*. However, there is also a way to look at a signal in the *frequency domain* (also called 'spectral domain') through transforming the signal using the *Fourier transform*. \n",
    "\n",
    "Basically, the fourier transform calculates to which degree sine waves of different frequencies are present in your signal. If a sine wave of a certain frequency (let's say 2 hertz) is (relatively) strongly present in your signal, it will have a (relatively) high *power* in the frequency domain. Thus, looking at the frequency domain of a signal can tell you something about the frequencies of the (different) sources underlying your signal.\n",
    "\n",
    "This may sound quite abstract, so let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with importing the python packages we'll need \n",
    "from niedu.utils.nii import create_sine_wave\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sine waves are oscillating signals that have (for our purposes) two important characteristics: their *frequency* and their *amplitude*. Frequency reflects how fast a signal is oscillating (how many cycles it completes in a given time period) and the amplitude is the (absolute) height of the peaks and troughs of the signal. To illustrate this, we generate a couple of sine-waves (with a sampling rate of 500 Hz, i.e., 500 samples per second) with different amplitudes and frequencies, which we plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 5\n",
    "sampling_rate = 500\n",
    "timepoints = np.arange(0, max_time, 1.0 / sampling_rate)\n",
    "\n",
    "amplitudes = np.arange(1, 4)\n",
    "frequencies = np.arange(1, 4)\n",
    "sines = []\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(13, 8))\n",
    "for i, amp in enumerate(amplitudes):\n",
    "    \n",
    "    for ii, freq in enumerate(frequencies):\n",
    "        this_ax = axes[i, ii]\n",
    "        \n",
    "        if ii == 0:\n",
    "            this_ax.set_ylabel('Activity (A.U.)', fontsize=14)\n",
    "        \n",
    "        if i == 2:\n",
    "            this_ax.set_xlabel('Time (seconds)', fontsize=14)\n",
    "        \n",
    "        sine = create_sine_wave(timepoints, frequency=freq, amplitude=amp)    \n",
    "        sines.append((sine, amp, freq))\n",
    "        this_ax.plot(timepoints, sine)\n",
    "        this_ax.set_xlim(0, 5)\n",
    "        this_ax.set_title('Amp = %i, freq = %i' % (amp, freq), fontsize=18)\n",
    "        this_ax.set_ylim(-3.5, 3.5)\n",
    "        this_ax.grid()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the signals vary in their amplitude (from 1 to 3) and their frequency (from 1 - 3). Make sure you understand these characteristics! Now, we are going to use the fast fourier transform to plot the same signals in the *frequency domain*. We're not going to use a function to compute the FFT-transformation, but we're going to use a function that computes the \"power spectrum density\" directly (which makes life a little bit easier): the `periodogram` function from `scipy.signal`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import periodogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `periodogram` function takes two arguments, the signal and the sampling frequency (the sampling rate in Hz with which you recorded the signal), and returns both the reconstructed frequencies and their associated power values. An example:\n",
    "\n",
    "```python\n",
    "freqs, power = periodogram(some_signal, 1000)  # sampling_rate = 1000 Hz\n",
    "```\n",
    "\n",
    "We'll use the `periodogram` function to plot the 9 sine-waves (from the previous plot) again, but this time in the frequency domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(13, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sine, amp, freq = sines[i]\n",
    "    title = 'Sine with amp = %i and freq = %i' % (amp, freq)\n",
    "    freq, power = periodogram(sine, sampling_rate)\n",
    "    ax.plot(freq, power)\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.set_ylim(0, 25)\n",
    "    \n",
    "    if i > 5:\n",
    "        ax.set_xlabel('Frequency (Hz)', fontsize=15)\n",
    "    \n",
    "    if i % 3 == 0:\n",
    "        ax.set_ylabel('Power', fontsize=15)\n",
    "\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.grid()\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the frequency domain correctly 'identifies' the amplitudes and frequencies from the signals. But the real 'power' from fourier transforms is that they can reconstruct a signal in *multiple underlying oscillatory sources*. Let's see how that works. We're going to load in a time-series recorded for 5 seconds of which we don't know the underlying oscillatory sources. First, we'll plot the signal in the time-domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_signal = np.load('mystery_signal.npy')\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.arange(0, 5, 0.001), mystery_signal)\n",
    "plt.title('Time domain', fontsize=25)\n",
    "plt.xlim(0, 5)\n",
    "plt.xlabel('Time (sec.)', fontsize=20)\n",
    "plt.ylabel('Activity (A.U.)', fontsize=20)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (2 points): It's hard to see which frequencies (and corresponding amplitudes) are present in this 'mystery signal'. Get the frequencies and power of the signal using the <tt>periodogram</tt> function (you have to deduce the sampling rate of the signal yourself! It is *not* the variable <tt>sampling_rate</tt> from before). Set the x-limit of the x-axis to (0, 8) (<tt>plt.xlim(0, 8)</tt>). Also, give the plot appropriate labels for the axes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "430da1112b529d08922661cb1167f6b2",
     "grade": true,
     "grade_id": "cell-031446c144829664",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know that you can use visualization of the signal in the frequency domain to help you understand from which underlying frequencies your signal is built up. Unfortunately, real fMRI data is not so 'clean' as the simulated sine waves we have used here, but the frequency representation of the fMRI signal can still tell us a lot about the nature and contributions of different (noise- and signal-related) sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency characteristics of fMRI data\n",
    "Now, we will load a (much noisier) voxel signal and the corresponding design-matrix (which has just one predictor apart from the intercept). The signal was measured with a TR of 2 seconds and contains 300 volumes (timepoints), so the duration was 600 seconds. The predictor reflects an experiment in which we showed 15 stimuli in intervals of 40 seconds (i.e., one stimulus every 40 seconds).\n",
    "\n",
    "We'll plot both the signal ($y$) and the design-matrix ($X$; without intercept):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niedu.utils.nii import simulate_signal\n",
    "\n",
    "onsets = np.arange(0, 600, 40)\n",
    "sig, X = simulate_signal(\n",
    "    onsets,\n",
    "    ['stim'] * onsets.size,\n",
    "    duration=600,\n",
    "    TR=2,\n",
    "    icept=0,\n",
    "    params_canon=[10],\n",
    "    std_noise=5,\n",
    "    rnd_seed=29,\n",
    "    phi=0.95,\n",
    "    plot=False\n",
    ")\n",
    "X = X[:, :-1]  # trim off the temporal derivative\n",
    "\n",
    "print(\"Shape of X: %s\" % (X.shape,))\n",
    "print(\"Shape of y (sig): %s\" % (sig.shape,))\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(sig)\n",
    "plt.xlim(0, sig.size)\n",
    "plt.title('Signal in time domain', fontsize=25)\n",
    "plt.ylabel('Activity (a.u.)', fontsize=20)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.arange(sig.size), X[:, 1], c='tab:orange')\n",
    "plt.title('Predictor in time domain', fontsize=25)\n",
    "plt.xlabel('Time (TR)', fontsize=15)\n",
    "plt.xlim(0, sig.size)\n",
    "plt.ylabel('Activity (a.u.)', fontsize=20)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (2 points)\n",
    "</div>\n",
    "\n",
    "Run linear regression using the variable `X` (which already contains an intercept) to explain the variable `sig`. Calculate the model's MSE, and store this in a variable named `mse_no_filtering`. Then, in the next code-cell, plot the signal and the predicted signal ($\\hat{y}$) in a single figure. Give the axes sensible labels and add a legend.\n",
    "\n",
    "**Tip** (feel free to ignore): This tutorial, you'll be asked to compute t-values, R-squared, and MSE of several models quite some times. To make your life easier, you could (but certainly don't have to!) write a function that runs, for example, linear regression and returns the R-squared, given a design (X) and signal (y). For example, this function could look like:\n",
    "\n",
    "```python\n",
    "def compute_mse(X, y):\n",
    "    # you implement the code here (run lstsq, calculate yhat, etc.)\n",
    "    # ...\n",
    "    # and finally, after you've computed the model's MSE, return it\n",
    "    return r_squared\n",
    "```\n",
    "\n",
    "If you're ambitious, you can even write a single function that calculates t-values, MSE, and R-squared. This could look something like this:\n",
    "\n",
    "```python\n",
    "def compute_all_statistics(X, y, cvec):\n",
    "    # Implement everything you want to know and return it\n",
    "    # ...\n",
    "    return t_value, MSE, r_squared # and whatever else you've computed!\n",
    "```\n",
    "\n",
    "Doing this will save you a lot of time and may prevent you from making unneccesary mistakes (like overwriting variables, typos, etc.). Lazy programmers are the best programmers!\n",
    "\n",
    "(Note: writing these functions is *optional*!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a35cdfede995e21a70a9c28e52af2d6",
     "grade": false,
     "grade_id": "cell-dadea5c142374e31",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement the linear regression part of the ToDo here:\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d58580f2d92156d354b3822cc1087d19",
     "grade": true,
     "grade_id": "cell-20561dbb73c2f8ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above linear regression ToDo. '''\n",
    "from niedu.tests.nii.week_4 import test_mse_no_filtering\n",
    "test_mse_no_filtering(X, sig, mse_no_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c24d8fc5e579579c8924a29d2b44e1fa",
     "grade": true,
     "grade_id": "cell-117a97afa6bbd6ac",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement your y/yhat plot here (this is manually graded)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (1 point): In your plot above, you should see that the fit of your model is \"off\" due to some low frequency drift. Name two potential causes of drift.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0def768ee55a4bbda525a8cf5e4bc311",
     "grade": true,
     "grade_id": "cell-4e3ee9534090d6cb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The two approaches to temporal preprocessing\n",
    "\n",
    "Basically, there are *two* ways to preprocess your data:\n",
    "1. Manipulating the signal ($\\mathbf{y}$) **directly** *before* fitting your GLM-model;\n",
    "2. Including \"noise predictors\" in your design ($\\mathbf{X}$) when fitting your model;\n",
    "\n",
    "Often, preprocessing steps can be done both by method 1 (manipulating the signal directly) and by method 2 (including noise predictors). For example, one of the videos showed that you could apply a high-pass filter by applying a \"gaussian weighted running line smoother\" (the method FSL employs) *directly* on the signal (method 1) **or** you could add \"low-frequency (drift) predictors\" to the design matrix (method 2; in the video they used a 'discrete cosine basis set'; the SPM method). In practice, both methods often yield very similar resuls. The most important thing to understand is that both methods are trying to accomplish the same goal: reduce the noise term of the model.\n",
    "\n",
    "First, we will discuss how temporal and spatial filtering can *directly* filter the signal (method 1) to reduce error. Later in the tutorial, we will discuss including adding outlier-predictors and motion-predictors to the design to reduce noise (method 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-pass filtering of fMRI data (option 1)\n",
    "From the previous ToDo, you probably noticed that the fit of the predictor to the model was not very good. The cause for this is the slow 'drift' - a low-frequency signal - that prevents the model from a good fit. Using a high-pass filter - meaning that you *remove* the low-frequency signals and thus *pass only the high frequencies* - can, for this reason, improve the model fit. But before we go on with actually high-pass filtering the signal, let's take a look at the frequency domain representation of our voxel signal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 5))\n",
    "TR = 2\n",
    "sampling_frequency = 1 / TR  # our sampling rate is 0.5, because our TR is 2 sec!\n",
    "freq, power = periodogram(sig, fs=0.5)\n",
    "plt.plot(freq, power)\n",
    "plt.xlim(0, freq.max())\n",
    "plt.xlabel('Frequency (Hz)', fontsize=15)\n",
    "plt.ylabel('Power (dB)', fontsize=15)\n",
    "plt.axvline(x=0.01,color='r',ls='dashed', lw=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the frequency-domain plot above, you can clearly see a low-frequency drift component at frequencies approximately below 0.01 Hz (i.e., left of the dashed red line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (1 point): Apart from the low frequency drift component around 0.01 Hz, there is also a component visible at 0.025 Hz (and its <a href=\"https://en.wikipedia.org/wiki/Harmonic\">harmonics</a> at 0.05, 0.075, 0.1, etc.). What does this component represent? Please explain (concisely).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03631a5da5728b09ddd15c324a25fc50",
     "grade": true,
     "grade_id": "cell-e983db4008407af2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get rid of that pesky low frequency drift that messes up our model! There is no guideline on how to choose the cutoff of your high-pass filter, but most recommend to use a cutoff of around 100 seconds (i.e., of 0.01 Hz). This means that any oscillation slower than 100 seconds (one cycle in 100 seconds) is removed from your signal.\n",
    "\n",
    "Anyway, as you've seen in the videos, there are many different ways to high-pass your signal (e.g., frequency-based filtering methods vs. time-based filtering methods). Here, we demonstrate a time-based 'gaussian running line smoother', which is used in FSL. As you've seen in the videos, this high-pass filter is estimated by convolving a gaussian \"kernel\" with the signal (taking the element-wise product and summing the values) across time, which is schematically visualized in the image below:\n",
    "\n",
    "![](https://docs.google.com/drawings/d/e/2PACX-1vRZTMvXJDBj3HGhrMZxQy1_6T1yF7bVinpBpeIQBgVUPAM_igGXrMonQskFP_Mymy-NVvGJnsvbDhiv/pub?w=934&h=649)\n",
    "\n",
    "One implementation of this filter is included in the scipy \"ndimage\" subpackage. Let's import it\\*:\n",
    "\n",
    "---\n",
    "\\***Note**: if you're going to restart your kernel during the lab for whatever reason, make sure to re-import this package to avoid `NameErrors` (i.e., the error that you get when you call a function that isn't imported)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gaussian_filter` function takes two mandatory input: some kind of (n-dimensional) signal and a cutoff, \"sigma\", that refers to the width of the gaussian filter in standard deviations. \"What? We decided to define our cutoff in seconds (or, equivalently, Hz), right?\", you might think. For some reason neuroimaging packages seem to define cutoff for their temporal filters in **seconds** while more 'low-level' filter implementations (such as in scipy) define cutoffs (of gaussian filters) in **the width of the gaussial filter**, i.e., **sigma**. Fortunately, there is an easy way to (approximately) convert a cutoff in seconds to a cutoff in sigma, given a particular TR (in seconds):\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma = \\frac{\\mathrm{cutoff}_{sec}}{\\sqrt{8\\ln{2}} \\cdot \\mathrm{TR}_{sec}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\ln{2}$ refers to the natural logarithm (i.e., log with base $e$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1 point): Suppose I acquire some fMRI data (200 volumes) with a sampling frequency of 0.25 Hz and I would like to apply a high-pass filter of 80 seconds. What sigma should I choose? Calculate sigma and store it in a variable named <tt>sigma_todo</tt>. You can use the function <tt>np.log(some_number)</tt> to evaluate the natural logarithm of a number.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07547af9d3ceed4c05cafebc4c135b87",
     "grade": false,
     "grade_id": "cell-cc9371327535b16b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9d799f0948a7c41e69429fe56e13718",
     "grade": true,
     "grade_id": "cell-bb07c42af9a395ef",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo.'''\n",
    "from niedu.tests.nii.week_4 import test_sec2sigma\n",
    "test_sec2sigma(sec=80, ans=sigma_todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, the gaussian filter does not return the filtered signal itself, but the estimated low-frequency component of the data. As such, to filter the signal, we have to subtract this low-frequency component from the original signal to get the filtered signal! \n",
    "\n",
    "Below, we estimate the low-frequency component using the high-pass filter first and plot it together with the original signal, which shows that it accurately captures the low-frequency drift (upper plot). Then, we subtract the low-frequency component from the original signal to create the filtered signal, and plot it together with the original signal to highlight the effect of filtering (lower plot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = gaussian_filter(sig, 8.5)\n",
    "\n",
    "plt.figure(figsize=(17, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(sig, lw=2)\n",
    "plt.plot(filt, lw=4)\n",
    "plt.xlim(0, sig.size)\n",
    "plt.legend(['Original signal', 'Low-freq component'], fontsize=20)\n",
    "plt.title(\"Estimated low-frequency component using HP-filter\", fontsize=25)\n",
    "plt.ylabel(\"Activation (A.U.)\", fontsize=20)\n",
    "plt.grid()\n",
    "\n",
    "# IMPORTANT: subtract filter from signal\n",
    "filt_sig = sig - filt\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(sig, lw=2)\n",
    "plt.plot(filt_sig, lw=2, c='tab:green')\n",
    "plt.xlim(0, sig.size)\n",
    "plt.legend(['Original signal', 'Filtered signal'], fontsize=20)\n",
    "plt.title(\"Effect of high-pass filtering\", fontsize=25)\n",
    "plt.xlabel(\"Time (TR)\", fontsize=20)\n",
    "plt.ylabel(\"Activation (A.U.)\", fontsize=20)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal looks much better, i.e., it doesn't display much drift anymore. But let's check this by plotting the original and filtered signal in the frequency domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 5))\n",
    "\n",
    "freq, power = periodogram(sig, fs=0.5)\n",
    "plt.plot(freq, power, lw=2)\n",
    "\n",
    "freq, power = periodogram(filt_sig, fs=0.5)\n",
    "plt.plot(freq, power, lw=2)\n",
    "\n",
    "plt.xlim(0, freq.max())\n",
    "plt.ylabel('Power (dB)', fontsize=15)\n",
    "plt.xlabel('Frequency (Hz)', fontsize=15)\n",
    "plt.title(\"The effect of high-pass filtering in the frequency domain\", fontsize=20)\n",
    "plt.legend([\"Original signal\", \"Filtered signal\"], fontsize=15)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! It seems that the high-pass filtering worked as expected! But does it really improve our model fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1 point): We've claimed several times that high-pass filtering improves model fit, but is that really the case in our case? To find out, fit the same design (variable <tt>X</tt>) on the filtered signal (variable <tt>filt_sig</tt>) using linear regression. Calculate MSE and store it in the variable <tt>mse_with_filter</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54297a09edca887487d560636bace95b",
     "grade": false,
     "grade_id": "cell-623fa8547acd57c2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement linear regression of X on filt_sig here!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd46e6e5d50aa08131933941de8d3665",
     "grade": true,
     "grade_id": "cell-cea5cd70b866a178",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nii.week_4 import test_mse_with_filter\n",
    "test_mse_with_filter(X, filt_sig, mse_with_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "<b>Assignment</b> (2 points)\n",
    "    \n",
    "So far, we've filtered only a single (simulated) voxel timeseries. Normally, you want to temporally filter *all* your voxels in your 4D fMRI data, of course. Below, we load in such a 4D fMRI file (`data_4d`), which has $50$ timepoints and ($80 \\cdot 80 \\cdot 44 = $) $281600$ voxels.\n",
    "\n",
    "For this assignment, you need to apply the high-pass filter (i.e., the `gaussian_filter` function; use `sigma=25`) on each and every voxel separately, which means that you need to loop through all voxels (which amounts to three nested for-loops across all three spatial dimensions). Below, we've already loaded in the data. Now it's up to you write the loops (across spatial dimensions) to filter the signal in the inner-most loop and store it in the pre-allocated `data_4d_filt` variable (the loop may, if implemented correctly, take about 20 seconds!).\n",
    "\n",
    "There is no test-cell (only hidden tests).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a820568dc8790a4432234e52d21dabb1",
     "grade": false,
     "grade_id": "cell-a0c2db1cc9a5d1fb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import nibabel as nib\n",
    "\n",
    "f = op.join(op.dirname(op.abspath('')), 'week_1', 'func.nii.gz')\n",
    "print(\"Loading data from %s ...\" % f)\n",
    "data_4d = nib.load(f).get_fdata()\n",
    "\n",
    "print(\"Shape of the original 4D fMRI scan: %s\" % (data_4d.shape,))\n",
    "\n",
    "# Here, we pre-allocate a matrix of the same shape as data_4d, in which\n",
    "# you need to store the filtered timeseries\n",
    "data_4d_filt = np.zeros(data_4d.shape)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70cd1367420cfcd14d329ccedd0a45d9",
     "grade": true,
     "grade_id": "cell-de2da1b458164588",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above assignment (hidden tests only). '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Note</b>: when temporally filtering your fMRI data ($\\mathbf{y}$), it is important to apply the <em>same filter</em> to your design matrix ($\\mathbf{X}$)! This makes sure that your design matrix does not contain any \"information\" that is removed from the fMRI time series by the filter, anyway. Note that most neuroimaging software packages (including FSL) do this automatically.    \n",
    "    \n",
    "Technically, by filtering your design matrix as well, you're orthogonalizing your design matrix with respect to your temporal high-pass filter. If you want to know more about this, check out <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.24528\">this excellent paper</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-pass filtering of fMRI data (option 2)\n",
    "As we've seen, high-pass filtering using a \"running line smoother\" is an operation that is applied to the signal directly (before model fitting) to reduce the noise term. Another way to reduce the noise term is to include *noise regressors* (also called 'nuisance variables/regressors') in the design matrix. As such, we can subdivide our design matrix into \"predictors of interest\" (which are included to model the task/stimuli) and \"noise predictors\" (which aim to model the thus-far unmodelled variance). These \"noise predictors\" are also sometimes called \"nuisance\" predictors/regressors/covariates. We can now slightly reformulate our linear regression equation by dividing our design into two components, $\\mathbf{X}_{\\mathrm{interest}}$ and $\\mathbf{X}_{\\mathrm{noise}}$:\n",
    "\n",
    "\\begin{align}\n",
    "y = \\mathbf{X}_{\\mathrm{interest}}\\beta_{\\mathrm{interest}} + \\mathbf{X}_{\\mathrm{noise}}\\beta_{\\mathrm{noise}} + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "Importantly, the difference between $\\mathbf{X}_{\\mathrm{noise}}$ and $\\epsilon$ is that the $\\mathbf{X}_{\\mathrm{noise}}$ term refers to noise-related activity that you *are able to model* while the $\\epsilon$ term refers to the noise that you *can't model* (this is often called the \"irreducible noise/error\" term). \n",
    "\n",
    "We can use this technique, which we'll call \"nuisance regression\" (which we'll discuss in more detail later), as an alternative to directly high-pass filtering the signal ($\\mathbf{y}$). One example of this (with respect to high-pass filtering) is including a series of cosines with varying frequencies in your design, which have the same effect as a high-pass filter. This type of filter is called a \"discrete cosine (basis) set\". Basically, for any given high-pass cutoff (in hertz), the \"discrete cosine transform\" (DTC) will yield a set of cosine regressors that is sufficient to filter out any frequency slower than your cutoff.\n",
    "\n",
    "Fortunately, the `nilearn` package contains a function to calculate discrete cosine sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level.design_matrix import _cosine_drift as discrete_cosine_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes two arguments: `high_pass` (in hertz) and `frame_times` (an array with volume onsets). The signal from the previous examples (`sig`) was from an experiment lasting 600 seconds and with a TR of 2, so the volume onsets can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_times = np.linspace(0, 600, 300, endpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (0 points): Here, we assumed our volume onset at the start of the TR. But technically, the exact definition/computation of our \"frame times\" depends on a preprocessing step that we discussed previously. Which one?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute a discrete cosine set for a highpass cutoff of 100 seconds (i.e., 0.01 hertz):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_set = discrete_cosine_transform(high_pass=0.01, frame_times=frame_times)\n",
    "dc_set = dc_set[:, :-1]  # remove the (extra) intercept\n",
    "print(sig.shape)\n",
    "print(dc_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the function returns a numpy array with the same number of timepoints as our signal (300) and 12 predictors (we removed the last one, because that's an intercept). Note that it's not super important to know how, mathematically, a discrete cosine set is created; it's more important to understand the idea of adding these low-frequency cosine predictors to your design matrix in order to account for (\"explain\") the low-frequency parts of your data.\n",
    "\n",
    "Let's plot the discrete cosine set that we created (note: we only plot the first 6 for clarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(dc_set[:, :6], lw=3)\n",
    "plt.xlim(0, sig.size)\n",
    "plt.grid()\n",
    "plt.title(\"Discrete cosine set for a high-pass filter of 0.01 Hz\", fontsize=25)\n",
    "plt.xlabel(\"Time (TRs)\", fontsize=20)\n",
    "plt.ylabel(\"Activation (A.U)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2  points): Add the discrete cosine set as predictors to the design (<tt>X</tt>) and store it in a new variable named <tt>X_dct</tt> (do *not* overwrite the <tt>X</tt> variable). Make sure the first two columns of <tt>X_dct</tt> are the original predictors from <tt>X</tt> followed by the DCT set. Then, run linear regression. Save the parameters (\"betas\") in a variable named <tt>betas_dct</tt>. Plot the predicted signal ($\\hat{\\mathbf{y}}$) and the signal (<tt>sig</tt>) in the same plot. Name the axis labels appropriately.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dfe98bc7456d1fb80440fa031077ef1",
     "grade": true,
     "grade_id": "cell-fe0f074f077b93fc",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Implement your ToDo here. \"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27d29e22695f6fbdc51ae1ef41f2ffb3",
     "grade": true,
     "grade_id": "cell-5a0c55935e0371e4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from niedu.tests.nii.week_4 import test_dct_betas\n",
    "test_dct_betas(X, dc_set, sig, betas_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen in the previous ToDos, it doesn't really matter which strategy you choose, filtering the signal directly or adding nuisance regressors to the design: both (usually) work equally well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and prewhitening \n",
    "As you (should) have seen in the previous ToDos, the model fit increases tremendously after high-pass filtering! This surely is the most important reason why you should apply a high-pass filter. But there is another important reason: high-pass filters reduce the signal's autocorrelation! \n",
    "\n",
    "\"Sure, but why should we care about autocorrelation?\", you might think? Well this has to with the estimation of the standard error of our model, i.e., $\\hat{\\sigma}^{2}\\mathbf{c}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{c}^{T}$. As you've seen in the videos, the Gauss-Markov theorem states that in order for OLS to yield valid estimates (including estimates of the parameters' standard errors) *the errors (residuals) have a mean of 0, have 0 covariance (i.e., are uncorrelated), and have equal variance*. \n",
    "\n",
    "Let's go through these three assumptions step by step. We'll use the previously filtered signal for this.\n",
    "\n",
    "### Assumption of zero-mean of the residuals\n",
    "First, let's check whether the mean of the residuals is zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = inv(X.T @ X) @ X.T @ filt_sig\n",
    "y_hat = X @ b\n",
    "resids = filt_sig - y_hat\n",
    "mean_resids = resids.mean()\n",
    "print(\"Mean of residuals: %3.f\" % mean_resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (1 point): What component of the design-matrix ($\\mathbf{X}$) ensures that the mean of the residuals is zero? Explain (concisely) why.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c70670653a497b0332076553b813e6aa",
     "grade": true,
     "grade_id": "cell-398fc4a5c6e31dfc",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal variance of the residuals\n",
    "Alright, sweet &mdash; the first assumption seems valid for our data. Now, the next two assumptions &mdash; about equal variance of the residuals and no covariance between residuals &mdash; are trickier to understand and deal with. In the book (and videos), these assumptions are summarized in a single mathemtical statement: the covariance-matrix of the residuals not differ substantially from the identity-matrix ($\\mathbf{I}$) scaled by the noise-term ($\\hat{\\sigma}^{2}$). Or, put in a formula:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{cov}[\\epsilon] = \\hat{\\sigma}^{2}\\mathbf{I}\n",
    "\\end{align}\n",
    "\n",
    "This sounds difficult, so let's break it down. First off all, the covariance matrix of the residuals is always a symmetric matrix of shape $N \\times N$, in which the *diagonal represents the variances* and the *off-diagonal represents the covariances*. For example, at index $[i, i]$, the value represents the variance of the residual at timepoint $i$. At index $[i, j]$, the value represents the covariance between the residuals at timepoints $i$ and $j$. \n",
    "\n",
    "In OLS, we assume that the covariance matrix of the residuals ($\\mathrm{cov}[\\epsilon]$) matches the \n",
    "identity-matrix ($\\mathbf{I}$) times the noise-term ($\\hat{\\sigma}^{2}$). The identity-matrix is simply a matrix with all zeros except for the diagonal, which contains ones. For example, the identity-matrix for a residual-array of length $8$ looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_mat = np.eye(8)  # makes an \"eye\"dentity matrix\n",
    "print(identity_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also represent this visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(identity_mat, cmap='gray', aspect='auto')\n",
    "plt.xlabel(\"Time\", fontsize=15)\n",
    "plt.ylabel(\"Time\", fontsize=15)\n",
    "plt.title(\"Assumed covariance matrix of residuals\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we calculated that the noise-term of a model explaining this hypothetical signal of length $8$ equals 2.58 ($\\hat{\\sigma}^{2} = 2.58$). Then, OLS *assumes* that the noise stems from a covariance matrix estimated from the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_term = 2.58\n",
    "assumed_cov_resid = noise_term * identity_mat\n",
    "print(assumed_cov_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, this assumption about the covariance matrix of the residuals states that the *variance across residuals (the diagonal of the matrix) should be equal* and the *covariance between residuals (the off-diagonal values of the matrix) should be 0* (in the population).\n",
    "\n",
    "Now, we won't explicitly estimate the covariance matrix of the residuals (which is usually estimated using techniques that fall beyond the scope of this course); however, we *do* want you to understand *conceptually* how fMRI data might invalidate the assumptions about the covariance matrix of the residuals and how fMRI analyses deal with this (i.e., using prewhitening, which is explained later). \n",
    "\n",
    "So, let's check *visually* whether the assumption of equal variance of our residuals roughly holds for our (simulated) fMRI data. Now, when we consider this assumption in the context of our fMRI data, the assumption of \"equal variance of the residuals\" (also called homoskedasticity) means that we assume that the \"error\" in the model is equally big across our timeseries data. In other words, the mis-modelling (error) should be constant over time.\n",
    "\n",
    "Let's check this for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(resids, marker='.')\n",
    "plt.xlim(0, resids.size)\n",
    "plt.xlabel(\"Time (TR)\", fontsize=15)\n",
    "plt.ylabel(\"Activation (A.U.)\", fontsize=15)\n",
    "plt.title(\"Residuals\", fontsize=20)\n",
    "plt.axhline(0, ls='--', c='black')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks quite alright! Sure, there is some variation here and there, but given that our estimates (including the residuals and their variance!) are imperfect, this suffices. \n",
    "\n",
    "Just to give you some intuition about serious issues with homoskedasticity, check out the (hypothetical) timeseries residuals below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfactor = np.linspace(0, 2, sig.size)\n",
    "example_resids = resids * mfactor\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.xlim(0, sig.size)\n",
    "plt.xlabel(\"Time (TR)\", fontsize=15)\n",
    "plt.ylabel(\"Activation (A.U.)\", fontsize=15)\n",
    "plt.title(\"An example of residuals with (problematic) unequal variance\", fontsize=20)\n",
    "plt.axhline(0, ls='--', c='black')\n",
    "plt.plot(example_resids, marker='.')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'> \n",
    "<b>ToThink</b> (1 point): What could cause unequal variance in the residuals of an fMRI signal, *given that autocorrelation (i.e. low-frequency components) is filtered out appropriately*? In other words, can you think of something that might cause larger (or smaller) errors across the duration of an fMRI run?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ea0844f548cf9c9641c8869acdc2dbe",
     "grade": true,
     "grade_id": "cell-b7a49704abd2c6ec",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero covariance between residuals\n",
    "The last assumption of zero covariance between residuals (corresponding to the assumption of all zeros on the off-diagonal elements of the covariance-matrix of the residuals) basically refers to the assumption that *there is no autocorrelation (correlation in time) in the residuals*. In other words, knowing the residual at timepoint $i$ does not tell you anything about the residual at timepoint $i+\\tau$, where $\\tau$ reflects a particular \"lag\" and can be any positive number (up to $N$). For example, the \"lag 1\" autocorrelation ($\\tau = 1$) is the correlation between the data at timepoints $i$ and $i+1$. In OLS, we assume that there is no autocorrelation across all possible lags.\n",
    "\n",
    "Take for example the residuals of our unfiltered signal from before, which looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = inv(X.T @ X) @ X.T @ sig\n",
    "resids_new = sig - X @ b\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(resids_new, marker='.')\n",
    "plt.axhline(0, ls='--', c='black')\n",
    "plt.xlim(0, 200)\n",
    "plt.xlabel(\"Time (TR)\", fontsize=15)\n",
    "plt.title('Residuals (containing unmodelled drift!)', fontsize=20)\n",
    "plt.ylabel('Activity (a.u.)', fontsize=15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, there is clear and strong autocorrelation in the residuals. For example, the residuals are overall getting larger across time (\"drift\") and it contains other low-frequency (oscillatory) patterns. As such, we *do know something about the residual at timepoint $i+1$ (and other lags) given the residual at timepoint $i$, namely that it is likely that the residual at timpoint $i+1$ is likely __lower__ than the residual at timepoint $i$*! Therefore, drift is a perfect example of something that (if not modelled) causes autocorrelation in the residuals (i.e. covariance between residuals)! In other words, autocorrelation (e.g. caused by drift) will cause the values of the covariance matrix of the residuals at the indices $[i, i+1]$ to be non-zero, violating the third assumption of Gauss-Markov's theorem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1  point)\n",
    "    \n",
    "We stated that autocorrelation captures the information that you have of the residual at timepoint $i+\\tau$ given that you know the residual at timepoint $i$. Practically, you can compute the autocorrelation (or actually, autocovariance) for a particular lag $\\tau$ by computing the covariance of the residuals with the lag-$\\tau$ shifted version of itself. In general, the autocovariance for the residuals $\\epsilon$ with lag $\\tau$ is calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{cov}[\\epsilon_{i}, \\epsilon_{i+\\tau}] = \\frac{1}{N-\\tau-1}\\sum_{i=1}^{N-\\tau}(\\epsilon_{i}\\cdot\\epsilon_{i+\\tau})\n",
    "\\end{align}\n",
    "\n",
    "Jeanette Mumford explains how to do this quite clearly in her [video on prewhitening](https://www.youtube.com/watch?v=4VSzZKO0k_w) (around minute 10). For this ToDo, calculate the covariance (with $\\tau = 1$) between the residuals (i.e., using the variable `resids_new`) and store this in a variable named `lag1_cov`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84a90d401ac762182cdbdd0f7f59d551",
     "grade": false,
     "grade_id": "cell-d0a50982f817bdd4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e1dae5dfad437f168a9307114ab47a2",
     "grade": true,
     "grade_id": "cell-650ab4e9ce9a3385",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nii.week_4 import test_lag1_cov\n",
    "test_lag1_cov(resids_new, lag1_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting for unequal variance and autocorrelation: prewhitening\n",
    "So, in summary, if the covariance matrix of your residuals appear to significantly deviate from the identity-matrix scaled by the noise-term ($\\mathrm{cov}[\\epsilon] = \\hat{\\sigma}^{2}\\mathbf{I}$) &mdash; either due to unequal variance or non-zero covariance &mdash; your estimate of the variance term of your effects ($\\mathrm{var}[c\\hat{\\beta]$) will be incorrect. \n",
    "\n",
    "Unfortunately, even after high-pass filtering (which corrects for *most* but not *all* autocovariance), the covariance matrix of the residuals of fMRI timeseries usually do no conform to the Markov-Gauss assumptions of equal variance and zero covariance. Fortunately, some methods have been developed by statisticians that transform the data such that the OLS assumptions hold again. One such technique is called *prewhitening*. \n",
    "\n",
    "Prewhitening uses an estimate of the error covariance matrix &mdash; usually denoted by $\\mathbf{V}$ &mdash; to account for possible unequal variance and/or autocovariance of the residuals. The matrix $\\mathbf{V}$ is an $N \\times N$ matrix ($N$ referring to the number of timepoints of your signal), and  may be estimated using different techniques, with names such as \"ARMA\", \"AR(1)\", and \"REML\". In this course, we won't discuss these techniques and instead assume that your software of choice (FSL, AFNI, SPM) has computed an accurate estimate of $\\mathbf{V}$ for you already. But if you're up for a (programming) challenge, you can do the *optional* ToDo below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (optional, ungraded)\n",
    "    \n",
    "The \"AR(1)\" method is a relatively \"easy\" way to estimate $\\mathbf{V}$. It computes only a single parameter, the lag-1 correlation (not covariance!). Then, it assumes that the correlation decreases exponentially as a function of lag:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{autocor}_{\\tau} = \\phi^{\\tau + 1}\n",
    "\\end{align}\n",
    "\n",
    "where $\\phi$ is the estimated lag-1 correlation. For example, for $\\phi = 0.9$, $\\mathbf{V}$ would look like:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{V} = \\begin{bmatrix}\n",
    "                 1.0 & 0.9 & 0.9^{2} & 0.9^{3} & \\dots & 0.9^{N-1} \\\\\n",
    "                 0.9 & 1.0 & 0.9 & 0.9^{2} & \\dots & 0.9^{N-2} \\\\\n",
    "                 0.9^{2} & 0.9 & 1.0 & 0.9 & \\dots & 0.9^{N-3} \\\\\n",
    "                 0.9^{3} & 0.9^{2} & 0.9 & 1.0 & \\dots & 0.9^{N-4} \\\\\n",
    "                 \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                 0.9^{N-1} & 0.9^{N-2} & 0.9^{N-3} & 0.9^{N-4} & \\dots & 1.0\n",
    "             \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Compute the lag-1 correlation below and create the corresponding AR(1) matrix of the residuals (`resids_new`) and store this in a variable named `V_ar1` (do not use the `toeplitz` function for this).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9df6869dc6c9c223de14be75c8606e91",
     "grade": false,
     "grade_id": "cell-28c312e7a92208f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement the (optional) ToDo here.'''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bef8c54c29c5917f09a3ade1466332a",
     "grade": true,
     "grade_id": "cell-5e0eeaee778dd227",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the optional ToDo above. '''\n",
    "from scipy.linalg import toeplitz\n",
    "phi = (t0 - t0.mean()) @ (t1 - t1.mean()) / np.sqrt(np.sum((t0 - t0.mean()) ** 2) * np.sum((t1 - t1.mean()) ** 2))\n",
    "V_ans = phi ** toeplitz(np.arange(resids_new.size))\n",
    "np.testing.assert_array_almost_equal(V_ans, V_ar1)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have a signal of 20 timepoints (an irrealistically low number, but just ignore that for now) and that you already estimated the covariance matrix of the residuals of this signal. Now, suppose you take a look at it and you notice that it looks faaaaar from the identity-matrix ($\\mathbf{I}$) that we need for OLS.\n",
    "\n",
    "For example, you might see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "phi = 0.7\n",
    "V = phi ** toeplitz(np.arange(N))\n",
    "# This will increase variance over time\n",
    "V[np.diag_indices_from(V)] += np.linspace(0, 1, V.shape[0])\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(15, 8))\n",
    "axes[0].imshow(V, vmax=2, cmap='gray', aspect='auto')\n",
    "axes[0].set_title(\"V (actual covariance matrix)\", fontsize=25)\n",
    "axes[0].set_xlabel('Time (volumes)', fontsize=20)\n",
    "axes[0].set_ylabel('Time (volumes)', fontsize=20)\n",
    "\n",
    "axes[1].imshow(np.eye(N), vmax=2, cmap='gray', aspect='auto')\n",
    "axes[1].set_title(\"Identity-matrix (assumed matrix)\", fontsize=25)\n",
    "#axes[1].colorbar()\n",
    "axes[1].set_xlabel('Time (volumes)', fontsize=20)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'> \n",
    "<b>ToThink</b> (0 points): In the above cell, the `phi` variable controls the amount of autcorrelation (technically, it is the $\\phi$ parameter of an AR(1) autocorrelation model). Try changing the value of this variable. Do you understand the way the plotted $V$ matrix is changing as a function of $\\phi$? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, shit. We have both unequal variance (different values on the diagonal) *and* non-zero covariance (some non-zero values on the off-diagonal). So, what to do now? Well, we can use the technique of prewhitening to make sure our observed covariance matrix ($\\mathbf{V}$) will be \"converted\" to the identity matrix! Basically, this amounts to plugging in some extra terms to formula for ordinary least squares. As you might have seen in the book/videos, the *original* OLS solution (i.e., how OLS finds the beta-parameters is as follows):\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}y\n",
    "\\end{align}\n",
    "\n",
    "Now, given that we've estimated our covariance matrix of the residuals, $\\mathbf{V}$, we can rewrite the OLS solution such that it prewhitens the data (and thus the covariance matrix of the residuals will approximate $\\hat{\\sigma}^{2}\\mathbf{I}$) as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta} = (\\mathbf{X}^{T}\\mathbf{V}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{V}^{-1}y\n",
    "\\end{align}\n",
    "\n",
    "Then, accordingly, the standard-error of any contrast of the estimated beta-parameters becomes:\n",
    "\n",
    "\\begin{align}\n",
    "SE_{\\mathbf{c}\\hat{\\beta}} = \\sqrt{\\hat{\\sigma}^{2} \\cdot \\mathbf{c}(\\mathbf{X}^{T}\\mathbf{V}^{-1}\\mathbf{X})^{-1}\\mathbf{c}^{T}}\n",
    "\\end{align}\n",
    "\n",
    "This \"modification\" of OLS is also called \"generalized least squares\" (GLS) and is central to univariate fMRI analyses! You *don't* have to understand how this works mathematically; again, you should only understand *why* prewhitening makes sure that our data behaves according to the assumptions of the Gauss-Markov theorem.\n",
    "\n",
    "(Fortunately for us, there is usually an option to 'turn on' prewhitening in existing software packages, so we don't have to do it ourselves. But it is important to actually turn it on whenever you want to meaningfully and in an unbiased way interpret your statistics in fMRI analyses!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Given the target signal (<tt>some_sig</tt>), design-matrix (<tt>some_X</tt>), and the (hypothetical) covariance-matrix of the residuals from before (the variable <tt>V</tt>), calculate the beta-parameters using the prewhitened version of OLS (i.e., 'generalized least squares'; the formula above). Also, calculate the $t$-value of the contrast <tt>[0, 1]</tt> given the appropriate (GLS) computation of the standard-error. Store your results in the variable <tt>betas_gls</tt> and <tt>tval_gls</tt>, respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e922bd77c51acf6f2391959d2062b898",
     "grade": false,
     "grade_id": "cell-88f856f11c0a35a6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here!\n",
    "some_sig = sig[:20]  # y\n",
    "some_X = X[:20, :]   # X\n",
    "c_vec = np.array([0, 1])  # the contrast you should use\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fe3912140fd6b631556ee652463bc83",
     "grade": true,
     "grade_id": "cell-30eabb21fe70e080",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the optional ToDo above '''\n",
    "from niedu.tests.nii.week_4 import test_gls_todo    \n",
    "test_gls_todo(some_sig, some_X, V, c_vec, betas_gls, tval_gls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (<em>optional!</em> 0 points)\n",
    "\n",
    "When it comes to estimating parameters from data with unequal (co)variance, OLS actually still gives you unbiased parameters: on average, they will be correct. However, OLS is not the estimator with least variance anymore, meaning that it is less \"precise\" (it is not the \"Best Linear Unbiased Estimator\" anymore; still unbiased, but not the \"best\"). In fact, with unequal (co)variance, GLS is the best unbiased linear estimator. A good way to build intuition about this is to iteratively generate data with known parameters (the \"true betas\", $\\beta$, \"sigma squared\", $\\sigma^{2}$, and $V$) and to estimate the parameters back from the generated data. Then, you can plot the histograms of the parameters and you'll see that, on average, the estimated parameters are the same as the true parameters.\n",
    "\n",
    "Below, we set up such a \"simulation\" loop for you. We define the true parameters (`true_betas`, `siqsq`, `V`). Now, if you're up to the challenge, complete the loop by:\n",
    "1. Generating some random design matrix (e.g., using `np.random.normal` of size (N, 1));\n",
    "2. Stack an intercept\n",
    "3. Generate correlated noise using the `np.random.multivariate_normal` function (with `cov=V`);\n",
    "4. Generate the data using the formula $X\\beta + \\mathrm{noise}$;\n",
    "5. Estimate the OLS parameters, and store in `betas_ols`;\n",
    "6. Estimate the GLS parameters, and store in `betas_gls`;\n",
    "7. In the next cell, plot both parameters (`betas_ols[:, 1]` and `betas_gls[:, 1]`) as histograms\n",
    "\n",
    "Do the histograms look like you expected? Also, try changing the `phi` parameter, which controls the amount of autocorrelation in the data (it's the AR1 parameter which is used to create `V`). What happens to the difference between OLS and GLS?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df49910967176b41e544926994e94022",
     "grade": false,
     "grade_id": "cell-7c2a62f024a1a631",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "true_betas = np.array([0, 1])\n",
    "iters = 100\n",
    "\n",
    "N = 50\n",
    "phi = 0.8  # AR1 parameter\n",
    "sigsq = 2\n",
    "V = sigsq * phi ** toeplitz(np.arange(N))\n",
    "\n",
    "betas_ols = np.zeros((iters, 2))\n",
    "betas_gls = np.zeros((iters, 2))\n",
    "\n",
    "for i in range(iters):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c9f409feb7afa95dc104b40ae945539",
     "grade": true,
     "grade_id": "cell-4eef0788eaaeee62",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (More on) nuisance regression\n",
    "Let's go back to the technique of nuisance regression. We have seen before that this technique can be used to model low-frequency components in our data (effectively functioning as a high-pass filter), but it can, in general, be used to model *any* thus-far unmodelled variance in the signal that would otherwise end up in the noise term. For example, people use this technique to model variance due to physiological processes (such cardiac and respiratory related signals; see e.g. [Glover et al., 2000](https://www.ncbi.nlm.nih.gov/pubmed/10893535)), motion-related variance (which we'll discuss later), and high-intensity \"spikes\". To get a better feel for nuisance regression and its consequences, let's look at this process of removing high-intensity spikes (which is sometimes called \"despiking\").\n",
    "\n",
    "### Using nuisance regression for despiking\n",
    "This technique of adding noise-predictors to the design matrix is sometimes used to model 'gradient artifacts', which are also called 'spikes' (which you've heard about in one of the videos for this week). This technique is also sometimes called \"despiking\". These spikes reflect sudden large intensity increases in the signal across the entire brain that likely reflect scanner instabilities. One way to deal with these artifacts is to \"censor\" bad timepoints (containing the spike) in your signal using a noise predictor.\n",
    "\n",
    "But what defines a 'spike'/bad timepoint? One way is to compute the normalized \"root mean square successive differences\" (RMSSD), normalizing this, and imposing some threshold above which a timepoint is marked as a spike (technically, it's a little bit more complex, but we'll ignore that for now).\n",
    "\n",
    "We'll delve into the details of this computation later. For now, let's take a look at some example data that we're going to use for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('spike_data.npz') as spike_data:\n",
    "    all_sig = spike_data['all_sig']\n",
    "    pred = spike_data['pred']\n",
    "\n",
    "print(\"Shape of all_sig: %s\" % (all_sig.shape,))\n",
    "print(\"Shape of pred: %s\" % (pred.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example data `all_sig` is a (simulated) 4D fMRI scan with $10 \\times 10 \\times 10$ voxels and 500 timepoints (assuming a TR of 2, this amounts to a duration of 1000 seconds). The predictor reflects a design in which the participants was shown a stimulus every 100 seconds (50 TRs). Let's plot the predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(pred)\n",
    "plt.grid()\n",
    "plt.xlabel('Time (TRs)', fontsize=20)\n",
    "plt.ylabel('Activation (A.U.)', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now let's take a look at how you would calculate the \"root mean square successive differences\" (RMSSD). This quantity reflects the difference between every timepoint $t$ of your signal and the signal at timepoints $t-1$, which is then squared, averaged (across all voxels, $1 \\dots K$), after which the square root is taken:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{RMSSD}_{t} = \\sqrt{\\frac{1}{K}\\sum_{K}(s_{t, k} - s_{t-1, k})^2}\n",
    "\\end{align}\n",
    "\n",
    "First, let's focus on the \"successive differences\". Suppose we have only one \"signal\" of length 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sig = np.array([1, 3, -2, 0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"successive differences\" are the difference between 3 and 1, -2 and 3, 0 and -2, and 5 and 0. Note that the successive difference for the first timepoint ($t=0$) is not defined! In code, we can compute this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succ_diff = ex_sig[1:] - ex_sig[:-1]\n",
    "print(succ_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1 point)\n",
    "</div>\n",
    "\n",
    "Let's try the first step in computing spikes from our data: computing the \"successive differences\". Our data (`all_sig`) has 4 dimensions, with the fourth dimension representing time. Calculate the successive differences and store the result in a variable named `all_sig_sd`. Hint: your output should have a shape of $10 \\times 10 \\times 10 \\times 499$. Do not use a (for) loop! This can be done in one concise statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb4a007271c20f134e43346934b16959",
     "grade": false,
     "grade_id": "cell-e9aa5fb38ea53895",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the sucessive differences here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c803bfd9996df987af9a8f4e63729226",
     "grade": true,
     "grade_id": "cell-39cad92120209b1c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo'''\n",
    "from niedu.tests.nii.week_4 import test_successive_diff\n",
    "test_successive_diff(all_sig, all_sig_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1 point): Let's try the next step. Compute the squares ($^2$) of the successive differences, average those across voxels, and finally take the square root. Store the result in a variable named <tt>all_sig_rmssd</tt>, which should be of length 499. Also, no for loops! (Check out the <tt>axis</tt> argument of the <tt>np.mean</tt> function.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0fa5c5e8f77d0b14bdba89eaa18af09",
     "grade": false,
     "grade_id": "cell-1f1e9f9020d66e31",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "febd8c5e12a73b7aff8464f9d7f62e50",
     "grade": true,
     "grade_id": "cell-b3293b66da18bed8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo'''\n",
    "from niedu.tests.nii.week_4 import test_rmssd\n",
    "test_rmssd(all_sig_sd, all_sig_rmssd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on to the normalizing operation, there is one little thing we need to do. As we noted earlier, RMSSD is not defined for $t=0$ (because there is no $t-1$ for the first timepoint). We can insert a duplicate of the first value here for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sig_rmssd_fixed = np.insert(all_sig_rmssd, 0, all_sig_rmssd[0])\n",
    "print(all_sig_rmssd_fixed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (2 points)\n",
    "    \n",
    "Now, in order to identify spikes, we need to identify timepoints that have a RMSSD-values that differ more than (let's say) 7 standard deviations from the mean RMSSD value. As such, you need to 'z-score' the RMSSD values: subtract the mean value from each individual value and divide each of the resulting 'demeaned' values by the standard deviation ($\\mathrm{std}$) of the values. In other words, the z-transform of any signal $s$ with mean $\\bar{s}$ is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "z(s) = \\frac{(s - \\bar{s})}{\\mathrm{std}(s)}\n",
    "\\end{align}\n",
    "\n",
    "Implement this z-score transform for the variable `all_sig_rmssd_fixed` and store it in the variable `z_rmssd` (1 point). Then, plot the z-scored RMSSD signal (with appropriate axis labels; 1 point). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe3b3b0528824f31e25077e2da16bdd2",
     "grade": false,
     "grade_id": "cell-5a1c56ebd42c2704",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First, z-score the RMSSD signal\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e22abf22a376183b86b5f00546456e3",
     "grade": true,
     "grade_id": "cell-52789944709f295b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests part 1 of ToDo. '''\n",
    "from niedu.tests.nii.week_4 import test_zscore_rmssd\n",
    "test_zscore_rmssd(all_sig_rmssd_fixed, z_rmssd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02528b61c6e5362cceb11cf51347942f",
     "grade": true,
     "grade_id": "cell-26abd9ec9a7cf085",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now, plot the zscored rmssd signal!\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can set a threshold above which we define timepoints as \"spikes\". Let's say we do this for $z > 7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_spikes = z_rmssd > 7  # creates array with True/False\n",
    "n_spike = identified_spikes.sum()\n",
    "print(\"There are %i spikes in the data!\" % n_spike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to remove this influence, we can simply add a nuisance predictor for each spike, in which the predictor contains zeros at timepoints without the spike and 1 at the timepoint with a spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_pred = np.zeros((pred.size, n_spike))\n",
    "t_spikes = np.where(identified_spikes)[0]\n",
    "for i, t in enumerate(t_spikes):\n",
    "    print(\"Creating spike predictor for t = %i\" % t)\n",
    "    spike_pred[t, i] = 1\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(spike_pred)\n",
    "plt.xlabel(\"Time (TRs)\", fontsize=20)\n",
    "plt.ylabel(\"Activation (A.U.)\", fontsize=20)\n",
    "plt.grid()\n",
    "plt.xlim(0, pred.size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point):\n",
    "    \n",
    "Why do you think we do not convolve the spike regressors with an HRF (or basis set)? Write your answer in the text-cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "309f70d6d532c935d826ac9b0c25b501",
     "grade": true,
     "grade_id": "cell-a89dbfe92f7f7298",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "<b>Assignment</b> (2 points)\n",
    "    \n",
    "\n",
    "Calculate the t-value of the stimulus-predictor-against-baseline contrast in a model with both the stimulus predictor (`pred`) and the spike predictors (`spike_pred`). Also, stack an intercept. Store the t-value in the variable `tval_spike_model`. Use `spike_sig` (defined below) as your target, i.e., $y$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2340fce74dc8024de2a7155ba363680a",
     "grade": false,
     "grade_id": "cell-9f652d7f8d3de436",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the t-value of the model with only an intercept + stim predictor\n",
    "spike_sig = all_sig[5, 5, 5, :]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd467b365ffa962eac9a16cb708184a6",
     "grade": true,
     "grade_id": "cell-3f022b3782558bc9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the assignment above. '''\n",
    "\n",
    "print(\"Only hidden tests\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): An eager researcher might think that adding more and more (nuisance) predictors will always improve the amount of variance explained and thus will improve his/her chances of finding significant effects (i.e., $t$-values). Argue why this is not the case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26fb5f35f8d1cc0136b5d46cf1da8c15",
     "grade": true,
     "grade_id": "cell-1682e0bc202bae31",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial preprocessing\n",
    "Alright, so in the previoussections we've looked at how to filter our signal in the time-domain through high-pass filtering, prewhitening, and nuisance regression. Importantly, these operations are *performed on the timeseries of each voxel separately* (like you did in a previous ToDo)! Essentially, given our 4D fMRI data ($X \\times Y \\times Z \\times T$), temporal filtering as we discussed here is only applied to the fourth dimension (the time-dimension, $T$).\n",
    "\n",
    "In addition to these temporal operations, there are several spatial operations performed on (f)MRI data, including spatial smoothing (a form of filtering), distortion correction, and spatial registration/resampling.\n",
    "\n",
    "### Spatial filtering (smoothing)\n",
    "Spatial filtering refers to operations applied to the *spatial* dimensions ($X$, $Y$, and $Z$) of our 4D data. The most common spatial filtering operation &mdash; spatial smoothing &mdash; is usually implemented using a \"3D gaussian (low-pass) smoothing kernel\". Sounds familiar? Well, it should, because it's essentially the same type of filter as we used for our temporal high-pass filter! Only this time, it's not a 1-dimensional gaussian (\"kernel\") that does the high-pass filtering, but it's a 3-dimensional gaussian that does *low-pass* filtering. Just like the temporal gaussian-weighted running line smoother is applied across time, we apply the 3D gaussian across space (i.e., the 3 spatial dimensions, $X$, $Y$, and $Z$). The figure below schematically visualized the process\\*:\n",
    "\n",
    "![](https://docs.google.com/drawings/d/e/2PACX-1vQZ8Eb8uNU9F0ZLLD1rRNwEE6zab8kVNFsUUZpDcPyDFWrfaPTSNB5otplVAuTV4lR0PhYWM-60_pMD/pub?w=850&h=421)\n",
    "\n",
    "(Note that we show the spatial data in 2D, simply because it's easier to visualize, but in reality this is always 3D!)\n",
    "\n",
    "In fact, because both (high-pass) temporal filtering and (low-pass) spatial filtering in most fMRI applications depend on the same \"gaussian filtering\" principle, we can even use the same Python function: `gaussian_filter`! However, as we mentioned before, spatial smoothing in fMRI is used as a *low-pass filter*, which means that the (spatial!) frequencies *higher* than a certain cutoff are filtered out, while the (spatial!) frequencies *lower* than this cut-off are *passed*. Therefore, we don't need to subtract the output from the `gaussian_filter` function from the (spatial) data! (If we'd would that, than we're effectively high-pass filtering the data!)\n",
    "\n",
    "---\n",
    "\\* 3D gaussian figure from Philipp Klaus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on and demonstrate smoothing on fMRI data, we need to determine the sigma of our (3D) gaussian kernel that we'll use to smooth the data. Annoyingly, smoothing kernels in the fMRI literature (and software packages!) are usually not reported in terms of sigma, but as \"full-width half maximum\" (FWHM), which refers to the width of the gaussian at half the maximum height:\n",
    "\n",
    "![fwhm](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/FWHM.svg/1200px-FWHM.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you might read in papers something like \"We smoothed our data with a gaussian kernel with a FWHM of 3 millimeter\". So, we need to convert our desired FWHM (in millimeters) to sigma. In fact, this is the same conversion that we needed to do for our temporal Gaussian running line high-pass filter, but this time applied to the spatial domain (i.e., FWHM in mm) instead of the temporal domain (i.e., TR in seconds). We can use the same formula to convert FWHM (in mm.) to sigma: \n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_{kernel} = \\frac{\\mathrm{FWHM}_{mm}}{\\sqrt{8 \\ln{2}} \\cdot \\mathrm{voxel\\ size}_{mm}}\n",
    "\\end{align}\n",
    "\n",
    "So, for example, if I want to smooth at FWHM = 6 mm and my voxels are 3 mm in size (assuming equal size in the three spatial dimensions, which is common), my sigma becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_{kernel} = \\frac{6}{\\sqrt{8 \\ln{2}} \\cdot 3} \\approx 0.85\n",
    "\\end{align}\n",
    "\n",
    "The entire conversion process is a bit annoying, but it's simply necessary due to conventions in the fMRI literature/software packages.\n",
    "\n",
    "Anyway, having dealt with the conversion issue, let's look at an example. We'll use the 4D fMRI data from before (the `data_4d` variable) and we'll extract a single 3D volume which we'll smooth using the `gaussian_filter` function. This particular data has a voxel-size of $6\\ mm^{3}$; given that we want to smooth at an FWHM of, let's say, 10 millimeter, we need a sigma of $\\frac{10}{\\sqrt{8 \\ln{2}} \\cdot 6} \\approx 0.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = data_4d[:, :, :, 20] # We'll pick the 21st volume (Python is 0-indexed, remember?)\n",
    "\n",
    "fwhm = 10\n",
    "voxelsize = 6\n",
    "\n",
    "sigma = fwhm / (np.sqrt(8 * np.log(2)) * voxelsize)\n",
    "smoothed_vol = gaussian_filter(vol, sigma=sigma)\n",
    "\n",
    "# Let's plot both the unsmoothed and smoothed volume\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(vol[:, :, 30], cmap='gray') # And we'll pick the 11th axial slice to visualize\n",
    "plt.axis('off')\n",
    "plt.title(\"Unsmoothed volume\\n\", fontsize=15)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(smoothed_vol[:, :, 30], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Smoothed volume\\n($\\sigma = %.1f; FWHM = %s_{mm}$)' % (sigma, fwhm), fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (2 points)\n",
    "</div>\n",
    "\n",
    "Now, in the above example we only smoothed a single volume, but in your analyses you would of course smooth all volumes in your fMRI run! (Just like with temporal filtering you need to filter the timeseries of all voxels separately.) In this ToDo, you need to loop through all (50) volumes from the `data_4d` variable and smooth them separately. Store the smoothed data in the already pre-allocated variable `data_4d_smoothed`. Use a sigma of 0.7 for the gaussian filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdf3ecb0b9be9c6c17388206df4fef17",
     "grade": false,
     "grade_id": "cell-6c6a67416765405d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here\n",
    "\n",
    "data_4d_smoothed = np.zeros(data_4d.shape)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db57d1585966afd821097b436550a1c0",
     "grade": true,
     "grade_id": "cell-0592292dc79b7722",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the ToDo above '''\n",
    "from niedu.tests.nii.week_4 import test_smoothing_todo    \n",
    "test_smoothing_todo(data_4d, data_4d_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (1 point): Since the `gaussian_filter` works for any $N$-dimensional array, one could argue that you don't have to loop through all volumes and apply a 3D filter, but you could equally well skip the loop and use a 4D filter straightaway. Explain (concisely) why this is a bad idea (for fMRI data).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28d1cf717215a33678ec31025646258a",
     "grade": true,
     "grade_id": "cell-8d04961478f5fde2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Susceptibility distortion correction (SDC)\n",
    "Most functional MRI sequences use a technique called \"gradient-echo echo planar imaging\" (GE-EPI), which is a relatively sensitive and well-established technique for BOLD-MRI. For reasons beyond the scope of this course, GE-EPI scans unfortunately show signs of **\"geometric distortion\"** (sometimes called susceptibility distortion) and **signal loss** in some areas of the brain. Signal loss, which is most apparent in areas with B0 (static magnetic field) inhomogeneities caused by sudden air-tissue transitions (like around the ears, earduct &rarr; brain, and the ventromedial cortex, frontal sinus &rarr; brain). Geometric distortion is most visible in the phase-encoding direction of the scan (usually posterior &rarr; anterior or left &rarr; right). These geometric distorions are usually visible as tissue \"folding\" inwards or outwards (depending on the exact phase-encoding direction, e.g., posterior &rarr; anterior or anterior &rarr; posterior). \n",
    "\n",
    "In the gif below, you can see where signal loss (in ventromedial cortex and around the ears, best visible in slice `z=12`) and geometric distortion is most apparent. The \"before\" image is the one *with* geometric distortion (most apparent as folding inwards of the cerebellum and frontal cortex). Hover your cursor above the image to see it changing from uncorrected (\"before\") to corrected (\"after\").\n",
    "\n",
    "![SegmentLocal](sdc.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is (currently) no way to correct for signal loss, there *are* ways to correct for geometric distortion (as you can see in the gif above). Most distortion correction techniques require you to acquire additional scans that can be used to correct (or \"unwarp\") your functional MRI scans. One technique is to acquire a \"fieldmap\" scan to image the homogeneity of the magnetic field (based on two scans with slightly different TEs) which can then be used for geometric correction. Another technique, often called \"topup unwarping\" (or PEPOLAR unwarping), uses one or more volumes of a separate functional MRI scan with the opposite phase-encoding direction as your to-be-correction functional MRI scan. In the \"topup\" scan, the distortion is expected to be in the complete opposite direction as in your to-be-correction fMRI scan (e.g., inward folding of tissue where the fMRI scan would show outward folding and vice versa), which can then be used to estimate an undistorted shape (which is \"in the middle\" of the fMRI and topup scan). \n",
    "\n",
    "The mathematics and implementation of distortion correction is beyond the scope of this course, but we just wanted it to show you so that you're familiar with the term!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion correction / realignment\n",
    "Participant movement during scanning arguably affects your data quality most, and should be taken care of accordingly. As such, *motion correction* (or sometimes called *motion realignment*) is an important step in any preprocessing pipeline. Motion correction makes sure that all volumes (i.e., 3D fMRI images) are spatially aligned. Before discussing motion correction in more detail, let's take a look at how a motion corrupted scan looks like. Actually, the data that we've been using so far (`data_4d`) actually has very little motion, so we're going to add some motion to it (with a custom function `add_motion_to_vols`). Then, we'll plot it volume by volume as a short movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niedu.utils.nii import add_motion_to_vols, animate_volumes\n",
    "\n",
    "vols = add_motion_to_vols(data_4d)\n",
    "animate_volumes(vols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rigid body registration\n",
    "Motion correction aligns all volumes by picking a reference volume (usually the first or middle one) and subsequently \"moving\" all the other volumes such that they are spatially aligned with the reference volume. Specifically, motion correction uses *translation* (movement along the primary axes: left/right, up/down, front/back) and *rotation* (around the center of the image).\n",
    "\n",
    "![pitchrollyaw](https://www.researchgate.net/profile/Tsang_Ing_Ren/publication/279291928/figure/fig1/AS:292533185462272@1446756754388/Orientation-of-the-head-in-terms-of-pitch-roll-and-yaw-movements-describing-the-three.png)\n",
    "*Image from [Arcoverde Neto et al. (2014)](https://www.researchgate.net/figure/Orientation-of-the-head-in-terms-of-pitch-roll-and-yaw-movements-describing-the-three_fig1_279291928).*\n",
    "\n",
    "This specific transformation of the image has six parameters (translation in the X, Y, and Z direction, and rotation in the X, Y, and Z direction) and is often called a \"rigid body registration\". Given these parameters, the to-be-moved volume can be spatially resampled at the location of the reference volume.\n",
    "\n",
    "A rigid body registration is an example of a more general *affine registration*: a linear transformation of an image that may involve translation, rotation, scaling, and shearing. For motion correction, scaling and shearing is *not* applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (0 points): Why do you think motion correction not use scaling and shearing transformations?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine registrations\n",
    "For affine registrations, the way the image should be moved is defined in the affine matrix, a $4 \\times 4$ matrix. This matrix is similar to the one we discussed in week 1, but instead of describing how each image coordinate ($i, j, k$) relates to world coordinates ($x, y, z$), the affine matrix, here, describes how the coordinates from the original (non-corrected) image related to the corrected image:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} i_{corr}, j_{corr}, k_{corr}, 1 \\end{bmatrix} = A\\begin{bmatrix}\n",
    "           i \\\\\n",
    "           j \\\\\n",
    "           k \\\\\n",
    "           1\n",
    "         \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "For example, suppose I have a particular affine matrix (`A_example`, below) that encodes a translation of 3 voxels downwards (superior &rarr; inferior). Then, for any coordinate $(x, y, z, 1)$, I can compute the location of the coordinate in the corrected image as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_example = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 3],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# Note that we always have to append a 1, just like in week 1\n",
    "coord = np.array([0, 0, 0, 1])  # the middle of the image!\n",
    "corr_coord = A_example @ coord\n",
    "print(corr_coord[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the affine transformation above shows us that the middle coordinate $(0, 0, 0)$ refers to the corrected coordinate $(0, 0, 3)$ after translating the image 3 voxels downwards. But how is this particular translation encoded in the affine? For translation, this is relatively straightforward. For any translation $(x, y, z)$, the affine matrix looks like:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} i_{trans} & j_{trans} & k_{trans} & 1 \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "           1 & 0 & 0 & x \\\\\n",
    "           0 & 1 & 0 & y \\\\\n",
    "           0 & 0 & 1 & z \\\\\n",
    "           0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "           i \\\\\n",
    "           j \\\\\n",
    "           k \\\\\n",
    "           1\n",
    "         \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "For rotation, however, it is slightly more complicated! For rotation transformations (with rotations $x, y, z$ in radians), the affine is computed by the matrix \n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} i_{rot} & j_{rot} & k_{rot} & 1 \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "           1 & 0 & 0 & 0 \\\\\n",
    "           0 & \\cos(x) & -\\sin(x) & 0 \\\\\n",
    "           0 & \\sin(x) & \\cos(x) & 0 \\\\\n",
    "           0 & 0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "           \\cos(y) & 0 & \\sin(y) & 0 \\\\\n",
    "           0 & 1 & 0 & 0 \\\\\n",
    "           -\\sin(y) & 0 & \\cos(y) & 0 \\\\\n",
    "           0 & 0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "           \\cos(z) & -\\sin(z) & 0 & 0 \\\\\n",
    "           \\sin(z) & \\cos(z) & 0 & 0 \\\\\n",
    "           0 & 0 & 1 & 0 \\\\\n",
    "           0 & 0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    i \\\\\n",
    "    j \\\\\n",
    "    k \\\\\n",
    "    1\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "As you can see, rotations are actually encoded in the affine matrix as the result of matrix multiplication of the the separate x, y, and z rotation matrices. Don't worry, you don't have to program this rotation-to-affine operation. Using a custom function (`get_rotation_matrix`), let's take a look at what the affine matrix would look like for a rotation of 45 degrees in every direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_matrix(x=0, y=0, z=0):\n",
    "    \"\"\" Computes the rotation matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        Rotation in the x (first) dimension in degrees\n",
    "    y : float\n",
    "        Rotation in the y (second) dimension in degrees\n",
    "    z : float\n",
    "        Rotation in the z (third) dimension in degrees\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rot_mat : numpy ndarray\n",
    "        Numpy array of shape 4 x 4\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.deg2rad(x)\n",
    "    y = np.deg2rad(y)\n",
    "    z = np.deg2rad(z)\n",
    "    \n",
    "    rot_roll = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, np.cos(x), -np.sin(x), 0],\n",
    "        [0, np.sin(x), np.cos(x), 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    rot_pitch = np.array([\n",
    "        [np.cos(y), 0, np.sin(y), 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-np.sin(y), 0, np.cos(y), 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    rot_yaw = np.array([\n",
    "        [np.cos(z), -np.sin(z), 0, 0],\n",
    "        [np.sin(z), np.cos(z), 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    rot_mat = rot_roll @ rot_pitch @ rot_yaw\n",
    "    return rot_mat\n",
    "    \n",
    "    \n",
    "A_rot = get_rotation_matrix(x=45, y=45, z=45)\n",
    "print(A_rot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to combine both rotations and translations, we multiply the translation matrix ($T$) with our rotation matrix ($R = R_{x}R_{y}R_{z}$):\n",
    "\n",
    "\\begin{align}\n",
    "A = TR\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Below, we also define a function to get the translation matrix. Now, suppose I want to rotate my image with 45 degrees in the z-direction and translate my image with 10 voxels in the x-direction and 5 voxels in the y-direction. Calculate the corresponding affine matrix, and store this in a variable named <tt>affine_todo</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c07100da2e9de8f9cb37785879bc48d",
     "grade": false,
     "grade_id": "cell-ae387d34dc0502cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_translation_matrix(x=0, y=0, z=0):\n",
    "    \"\"\" Computes the translation matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        Translation in the x (first) dimension in voxels\n",
    "    y : float\n",
    "        Rotation in the y (second) dimension in voxels\n",
    "    z : float\n",
    "        Rotation in the z (third) dimension in voxels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trans_mat : numpy ndarray\n",
    "        Numpy array of shape 4 x 4\n",
    "    \"\"\"\n",
    "    \n",
    "    trans_mat = np.eye(4)\n",
    "    trans_mat[:, -1] = [x, y, z, 1]\n",
    "    return trans_mat\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "750344e241b739c94342ad07f96c6f78",
     "grade": true,
     "grade_id": "cell-2fe012448a3f390d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nii.week_4 import test_affine_todo\n",
    "test_affine_todo(affine_todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're not going to discuss the intricacies of how to actually resample an image given a particular affine. Instead, we defined a function called `resample_image` that resamples an image given a particular translation and rotation matrix. It's a bit more complicated than we discussed so far, but you get the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "def resample_image(image, trans_mat, rot_mat):\n",
    "    \"\"\" Resamples an image given a translation and rotation matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : numpy array\n",
    "        A 3D numpy array with image data\n",
    "    trans_mat : numpy array\n",
    "        A numpy array of shape 4 x 4\n",
    "    rot_mat : numpy array\n",
    "        A numpy array of shape 4 x 4\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image_reg : numpy array\n",
    "        A transformed 3D numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # We need to rotate around the origin, not (0, 0), so\n",
    "    # add a \"center\" translation\n",
    "    center = np.eye(4)\n",
    "    center[:3, -1] = np.array(image.shape) // 2 - 0.5\n",
    "    A = center @ trans_mat @ rot_mat @ inv(center)\n",
    "    \n",
    "    # affine_transform does \"pull\" resampling by default, so\n",
    "    # we need the inverse of A\n",
    "    image_corr = affine_transform(image, matrix=np.linalg.inv(A))\n",
    "    \n",
    "    return image_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (0 points): Try out different transformations of an actual brain image by changing the arguments given to <tt>get_translation_matrix</tt> and <tt>get_rotation_matrix</tt>. You can also change the axis (<tt>PLOT_AXIS</tt>) and slice number (<tt>SLICE_IDX</tt>) that is plotted.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat = get_translation_matrix(5, 5, 0)\n",
    "rot_mat = get_rotation_matrix(0, 0, 40)\n",
    "\n",
    "vols = data_4d\n",
    "VOL = vols[:, :, :, 0]\n",
    "PLOT_AXIS = 2\n",
    "SLICE_IDX = 20\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.take(VOL, SLICE_IDX, PLOT_AXIS).T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Original image', fontsize=20)\n",
    "\n",
    "# Resample\n",
    "vol_res = resample_image(VOL, trans_mat, rot_mat)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.take(vol_res, SLICE_IDX, PLOT_AXIS).T, origin='lower', cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Transformed image', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose that we want to align two volumes with each other. How would we go about it? Visually, it's quite hard to assess whether a particular affine matrix works well. In practice, most neuroimaging software packages use automatic algorithms that try to find a affine matrix that *minimizes some cost function*. In other words, it defines a function to evaluate the mismatch (or cost) of the alignment of two volumes given an affine matrix, and then it tries to minimize this function by (in a smart way) trying out different rotation and translation parameters.\n",
    "\n",
    "One cost function that is often used for rigid body registration is \"least squares\", which is defined as the sum over the squared (voxel-wise, $v$) differences between the reference image (sometimes called the \"fixed\" image) and the realigned image (sometimes called the \"moving\" image):\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{cost}_{LS} = \\sum_{v}(\\mathrm{fixed}_{v} - \\mathrm{moving}_{v})^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Define a function named <tt>least_squares_cost</tt> below, that takes two input parameters &mdash; <tt>fixed</tt> and <tt>moving</tt> (both 3D numpy array) &mdash; and outputs the least-squares cost. Note: you don't need a for loop!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "236d6f2a40befad8c3b7ed90e0cce5af",
     "grade": false,
     "grade_id": "cell-a1efe673673187ef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68bbd35a5c099121da0bd4f98a158c9c",
     "grade": true,
     "grade_id": "cell-9db862292e2766d3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Tests the above ToDo \"\"\"\n",
    "fixed = np.ones((10, 10, 10))\n",
    "moving = np.zeros((10, 10, 10))\n",
    "\n",
    "np.testing.assert_almost_equal(least_squares_cost(fixed, moving), 1000)\n",
    "\n",
    "fixed = np.arange(1000).reshape((10, 10, 10))\n",
    "moving = fixed[::-1]\n",
    "np.testing.assert_almost_equal(least_squares_cost(fixed, moving), 330000000)\n",
    "\n",
    "moving = np.roll(fixed.ravel(), 1).reshape((10, 10, 10))\n",
    "np.testing.assert_almost_equal(least_squares_cost(fixed, moving), 999000)\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): For this ToDo, *you* are going to be the optimizer! Below, we have loaded two (brain) images: <tt>fixed</tt> and <tt>moving</tt>. Use the <tt>resample_image</tt> function to translate/rotate the <tt>moving</tt> image. Make sure you name the output (i.e., the transformed volume) <tt>moved</tt>. This is then plotted on top of the reference volume to give you an idea of how well your transformation worked. Keep doing this until you're satisfied. Hint 1: you need rotation in the z dimension and translation in the x and y dimension. Hint 2: rotation is counter clockwise (if you define negative rotation, e.g. -20, it will be rotated clockwise).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b46fab32c20ce50a99d1360dc0d660bd",
     "grade": true,
     "grade_id": "cell-367a4f3bd6f89c4a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with np.load('moco_todo.npz') as moco_data:\n",
    "    moving = moco_data['moving']\n",
    "    fixed = moco_data['fixed']\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "### Plotting starts here\n",
    "# Only works if you've defined a variable named 'moved'\n",
    "# Do not change the code below\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(fixed[:, :, 25].T, origin='lower', cmap='gray')\n",
    "moved4plot = np.ma.masked_where(moved < 2, moved)\n",
    "plt.imshow(moved4plot[:, :, 25].T, origin='lower', alpha=0.8)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomical realignment/registration and normalization\n",
    "So far, we discussed how to register volumes to a reference volume using rigid body registrations (with a six parameter affine) in the context of motion correction. This is, however, only the first step in the complete registration protocol of (most) studies. If your goal is, in the end, to perform some kind of group analysis (in which you pool together data from multiple subjects), you want to make sure that each voxel (at location $(X, Y, Z)$) is in (roughly) the same location in the brain. The problem is that people's brains differ quite a bit in terms of anatomy! Therefore, you need to register (transform) your subject-level (functional) data to some kind of common template. This process is often done in two stages: first, you compute a registration from (a reference volume of) your functional data (\"moving\") to your anatomical data (your high-resolution T1-weighted scan; \"fixed\"), and second you compute a registration from your anatomical data (\"moving\") to a common template (\"fixed\"). Then, you can register your functional data to the common template by sequentially applying the first and second stage registration parameters. This is visualized in the figure below.\n",
    "\n",
    "![reg](https://docs.google.com/drawings/d/e/2PACX-1vQEpo53zns0VHAgEBkEU9lqQ58RnqINqdUpdLDX-5X9pjLk_BNghfuPBp9zu2nRNzgVQBJZ4GuX5ejz/pub?w=1698&h=516)\n",
    "\n",
    "In the figure above, you can see that the first stage (BOLD &rarr; T1w) uses the same registration protocol as we used with motion-correction: an affine transformation with 6 parameters (translation and rotation in 3 directions; a \"rigid body registration\"). The second stage (T1w &rarr; template) also uses an affine transformation, but this time with 12 parameters. In addition to the 6 rigid body registration parameters, a 12 parameter affine (sometimes reffered to as an affine with 12 degrees-of-freedom, DOF) also includes global scaling in 3 directions (i.e., making the brain bigger of smaller) and shearing in 3 directions (i.e., \"stretching\" in 3 directions). These extra parameters attempt to overcome the anatomical differences between the individual subjects' brains and the template. \n",
    "\n",
    "However, only a 12 DOF affine is usually not sufficient for proper alignment. Therfore, most neuroimaging software packages also offer the option to apply a non-linear registration step (\"warping\") *on top of the 12 DOF affine registration*. Instead of globally transforming your \"moving\" volume (i.e., the transformation applies to each voxel equally), non-linear registrations allow for *local* transformations (i.e., some voxels might be transformed \"more\" than others). Usually, the combination of an initial 12 DOF affine and a non-linear (\"warping\") registration operation lead to a proper alignment of the subject's anatomical volume and the template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (2 points): When we discussed motion correction, we used the \"least squares\" cost function to optimize our rigid body registration parameters. Is it a good idea to use this cost function for the BOLD &rarr; T1w registration procedure? And for our initial T1w &rarr; template registration procedure? Argue for both cases why (not).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "226571d4d9116e0ef8c14c1217c4977d",
     "grade": true,
     "grade_id": "cell-10b71c1f875e6d4d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion filtering\n",
    "Even after motion realignment, your data is still 'contaminated' by motion. This is because movement itself influences the measured activity. For example, suppose that you measure a single voxel in someone's brain; then, this person moves his/her head 2 centimeters. Now, we can do motion realigment to make sure we measure the same voxel before and after the movement, but *this does not change the fact that this particular voxel was originally measured at two different locations*. It could be that after the movement, the voxel was actually a little bit closer to the headcoil, which results in a (slight) increase in signal compared to before the movement (this is also known as 'spin history effects').\n",
    "\n",
    "Ideally, you want to account for these interactions between motion and the measured activity. One way to do this is through \"motion filtering\", of which one popular approach is to simply add the 6 realignment parameters (rotation and translation in 3 directions) to the design-matrix ($X$)! In other words, we treat the motion realignment parameters as \"nuisance regressors\" that are aimed to explain activity that is related to motion.\n",
    "\n",
    "Alright, let's load some realigment parameters (6 in total) from an fMRI run of 200 volumes. We'll plot them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This data has been motion-corrected using the FSL tool 'MCFLIRT', which outputs a file\n",
    "ending in *.par that contains the 6 motion parameters (rotation/translation in 3 directions each).\n",
    "We'll load in this file and plot these motion parameters. \"\"\"\n",
    "\n",
    "motion_params = np.loadtxt('func_motion_pars_new.txt')\n",
    "rotation_params = motion_params[:, :3]\n",
    "translation_params = motion_params[:, 3:]\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Rotation', fontsize=25)\n",
    "plt.plot(rotation_params)\n",
    "plt.xlim(0, motion_params.shape[0])\n",
    "plt.legend(['x', 'y', 'z'], fontsize=15)\n",
    "plt.ylabel('Rotation in radians', fontsize=15)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Translation', fontsize=25)\n",
    "plt.plot(translation_params)\n",
    "plt.legend(['x', 'y', 'z'], fontsize=15)\n",
    "plt.ylabel('Translation in mm', fontsize=15)\n",
    "plt.xlim(0, motion_params.shape[0])\n",
    "plt.xlabel('Time (TR)', fontsize=15)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, is this good data or not? In general, you could say that higher values (in either translation or rotation) are worse. That said, from the translation and rotation parameters alone it is hard to judge whether the participant moved *too* much, partly because we're dealing with 6 parameter traces here. One way to make this decision process a little easier is to convert these parameters to a single \"framewise displacement\" (FD) trace. FD aims to represent \"bulk motion\" and is a summary statistic of motion that combines all motion parameters into a single metric. It is calculated as follows:\n",
    "\n",
    "\\begin{align}\n",
    "FD_{t} = |r(Rot_{tx} - Rot_{(t-1)x})| + |r(Rot_{ty} - Rot_{(t-1)y})| + |r(Rot_{tz} - Rot_{(t-1)z})| + |Trans_{xt} - Trans_{x(t-1)}| + |Trans_{yt} - Trans_{y(t-1)}| + |Trans_{zt} - Trans_{z(t-1)}|\n",
    "\\end{align}\n",
    "\n",
    "where $|x|$ means the absolute value of $x$ and $r$ stands for a predefined radius in millimeters (which is usually taken to be 50). So, basically it is a sum across the absolute successive differences ($x_{t} - x_{t-1}$) of parameter traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (0 points; optional): Calculate the FD trace corresponding to the motion parameters in the <tt>motion_params</tt> variable. Assume a radius of 50. Store the result in a variable named <tt>fd_trace</tt>. Note that, just like with the RMSSD, the first value is undefined (because at $t=0$, there is no $t-1$). Therefore, prepend a 0 to your FD trace.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02724321e95398bcd307079c29d0e8da",
     "grade": false,
     "grade_id": "cell-851337eba26429e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement the optional ToDo here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0653e006b851f7afc941a5fa3508564d",
     "grade": true,
     "grade_id": "cell-df4b286bad8da0ad",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nii.week_4 import test_fd_calc\n",
    "test_fd_calc(motion_params, fd_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (0 points): Looking at the plots above (before the optional ToDo), can you deduce which volume was used as a reference volume? You can also use the <tt>motion_params</tt> variable to find out.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said in the beginning of this section, motion parameters (translation/rotation in 3 dimensions) are used as nuisance regressors in subject-level analyses. The reason for this is that, even after motion correction, the actual movement may have influenced the signal intensity through \"spin-history effects\", which occurs when a voxel is excited by an RF pulse slightly earlier/later due to movement (i.e., it may have moved up or down one slice), which changes how much T1 relaxation occurs (which affect the signal intensity; read more [here](http://imaging.mrc-cbu.cam.ac.uk/imaging/CommonArtefacts)).\n",
    "\n",
    "There are two ways to go about this, which are not mutually exclusive. The most common way (in task-based fMRI) is to add the six motion parameters (rotation/translation in three directions) to your design. Any variance of your signal that covaries with (one or more of) these motion parameters will be accounted for (and will thus *not* end up in your noise term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (0 points; optional): Sometimes, in addition to the original six rigid body registration parameters, people add the lag-1 shiften parameters (i.e., $t-1$) to their design, as well as the squares (i.e., $x^{2}$) of those twelve regressors, yielding a set of 24 nuisance regressors (sometimes called the \"Friston24\" set). Calculate this extended set of motion parameters and store it in a variable named <tt>friston24</tt>. Hint: because (again) the timepoint $t-1$ is not defined at $t=0$, prepend a row of zeros to the lag-1 shifted parameters. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e79b924fe4247d7a4301e8060010fac7",
     "grade": false,
     "grade_id": "cell-7cefa2c24e24556a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31a1ff1b281d2cf53a602e5438f476a7",
     "grade": true,
     "grade_id": "cell-dfeca80192f08c70",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nii.week_4 import test_friston24    \n",
    "test_friston24(motion_params, friston24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from directly adding the (extended) motion parameters to your design, you can also perform \"motion scrubbing\". Motion scrubbing is usually performed in resting-state analyses and is conceptually very similar to the \"despiking\" we did earlier. With motion scrubbing, you usually set a predefined threshold for your FD trace (for example 1 mm) and any timepoint above this threshold will be \"scrubbed out\". This is done by defining a separate regressor for each timepoint above the threshold containing all zeros except for the above-threshold timepoint, which contains a zero (just like we did with despiking). Sometimes, even the timepoint before and/or after the \"bad\" timepoint are additionally added to the design matrix. As this course focuses mostly on task-based fMRI analyses, we won't discuss motion scrubbing any further (but if you want to know more about the different motion correction strategies, we recommend the following article: [Power et al., 2015](https://www.sciencedirect.com/science/article/pii/S1053811914008702))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "<b>Assignment</b> (2 points)\n",
    "    \n",
    "For this ToDo, you have to compare two models and the resulting (normalized) effects (t-values): a model *without* the six motion parameters and a model *with* the six motion parameters. You should use the `motion_params` variable which we defined earlier as your motion parameters.\n",
    "\n",
    "We provide you with a design-matrix (`X`) with an intercept and a single stimulus-predictor and the signal of a single voxel (`sig`). Calculate the t-value for the contrast of the predictor-of-interest against baseline for both the original design-matrix (only intercept + predictor-of-interest; store this in a variable named `tval_simple`)\n",
    "and the design-matrix extended with the six motion parameters (which thus has 8 predictors; store this in a variable named `tval_ext`).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll load the data\n",
    "with np.load('data_motion_filtering.npz') as data_last_todo:\n",
    "    X = data_last_todo['X']\n",
    "    sig = data_last_todo['sig']\n",
    "\n",
    "print(\"Shape of original X: %s\" % (X.shape,))\n",
    "print(\"Shape of signal: %s\" % (sig.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b85b6cf1408d630de5104a05b5b38ebf",
     "grade": false,
     "grade_id": "cell-bc72faf7a5efe723",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Implement your ToDO here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "022945715f828f224b19085116279ee5",
     "grade": true,
     "grade_id": "cell-b96ab79b7be0d9b5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "\n",
    "print(\"Only hidden tests!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (2 points): If you did the above ToDo correctly, you should have found that the $t$-value of the extended model (with motion parameters) is actually <em>smaller</em> than the simple model (without motion parameters) ... What caused the t-value of *this predictor* to become smaller when the motion-parameters were included in your design-matrix? Inspect the different elements of the $t$-value formula carefully.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "496ef198454b0c40d1226a9ff13f39dd",
     "grade": true,
     "grade_id": "cell-a0a6ef6b1921850b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fmriprep\n",
    "*This is an optional section!*\n",
    "\n",
    "Today, many excellent general-purpose, open-source neuroimaging software packages exist: [SPM](https://www.fil.ion.ucl.ac.uk/spm/) (Matlab-based), [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki), [AFNI](https://afni.nimh.nih.gov/), and [Freesurfer](https://surfer.nmr.mgh.harvard.edu/) (with a shell interface). We argue that there is not one single package that is always the best choice for every step in your preprocessing pipeline. Fortunately, people from the [Poldrack lab](https://poldracklab.stanford.edu/) created [fmriprep](https://fmriprep.readthedocs.io/en/stable/), a software package that offers a preprocessing pipeline which \"glues together\" functionality from different neuroimaging software packages (such as Freesurfer and FSL), such that each step in the pipeline is executed by the software package that (arguably) does it best.\n",
    "\n",
    "We have been using *Fmriprep* for preprocessing of our own data and we strongly recommend it. It is relatively simple to use, requires minimal user intervention, and creates extensive visual reports for users to do visual quality control (to check whether each step in the pipeline worked as expected). The *only* requirement to use Fmriprep is that your data is formatted as specified in the Brain Imaging Data Structure (BIDS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The BIDS-format\n",
    "[BIDS](https://bids.neuroimaging.io/) is a specification on how to format, name, and organize your MRI dataset. It specifies the file format of MRI files (i.e., compressed Nifti: `.nii.gz` files), lays out rules for how you should name your files (i.e., with \"key-value\" pairs, such as: `sub-01_ses-1_task-1back_run-1_bold.nii.gz`), and outlines the file/folder structure of your dataset (where each subject has its own directory with separate subdirectories for different MRI modalities, including fieldmaps, functional, diffusion, and anatomical MRI). Additionally, it specifies a way to include \"metadata\" about the (MRI) files in your dataset with [JSON](https://en.wikipedia.org/wiki/JSON) files: plain-text files with key-value pairs (in the form \"parameter: value\"). Given that your dataset is BIDS-formatted and contains the necessary metadata, you can use `fmriprep` on your dataset. (You can use the awesome [bids-validator](https://bids-standard.github.io/bids-validator/) to see whether your dataset is completely valid according to BIDS.)\n",
    "\n",
    "There are different tools to convert your \"raw\" scanner data (e.g., in DICOM or PAR/REC format) to BIDS, including [heudiconv](https://heudiconv.readthedocs.io/en/latest/), [bidscoin](https://github.com/Donders-Institute/bidscoin), and [bidsify](https://github.com/NILAB-UvA/bidsify) (created by Lukas). We'll skip over this step and assume that you'll be able to convert your data to BIDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Fmriprep\n",
    "Now, having your data in BIDS is an important step in getting started with Fmriprep. The next step is installing the package. Technically, Fmriprep is a Python package, so it can be installed as such (using `pip install fmriprep`), but we do not recommend this \"bare metal\" installation, because it depends on a host of neuroimaging software packages (including FSL, Freesurfer, AFNI, and ANTs). So if you'd want to directly install Fmriprep, you'd need to install those extra neuroimaging software packages as well (which is not worth your time, trust us).\n",
    "\n",
    "Fortunately, Fmriprep also offers a \"Docker container\" in which Fmriprep and all the associated dependencies are already installed. [Docker](https://www.docker.com/) is software that allows you to create \"containers\", which are like lightweight \"virtual machines\" ([VM](https://en.wikipedia.org/wiki/Virtual_machine)) that are like a separate (Linux-based) operating system with a specific software configuration. You can download the Fmriprep-specific docker \"image\", which is like a \"recipe\", build the Fmriprep-specific \"container\" according to this \"recipe\" on your computer, and finally use this container to run Fmriprep on your computer as if all dependencies were actually installed on your computer! Docker is available on Linux, Mac, and Windows. To install Docker, google something like \"install docker for {Windows,Mac,Linux}\" to find a google walkthrough.\n",
    "\n",
    "Note that you need administrator (\"root\") privilege on your computer (which is likely the case for your own computer, but not on shared analysis servers) to run Docker. If you don't have root access on your computer/server, ask you administrator/sysadmin to install [singularity](https://fmriprep.readthedocs.io/en/stable/installation.html#singularity-container), which allows you to convert Docker images to Singularity images, which you can run without administrator privileges.\n",
    "\n",
    "Assuming you have installed Docker, you can run the \"containerized\" Fmriprep from your command line directly, which involves a fairly long and complicated command (i.e., `docker run -it --rm -v bids_dir /data ... etc`), or using the `fmriprep-docker` Python package. This `fmriprep-docker` package is just a simple wrapper around the appropriate Docker command to run the complicated \"containerized\" Fmriprep command. We strongly recommend this method.\n",
    "\n",
    "To install `fmriprep-docker`, you can use `pip` (from your command line):\n",
    "\n",
    "```\n",
    "pip install fmriprep-docker\n",
    "```\n",
    "\n",
    "Now, you should have access to the `fmriprep-docker` command on your command line and you're ready to start preprocessing your dataset. For more detailed information about installing Fmriprep, check out their [website](https://fmriprep.readthedocs.io/en/stable/installation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Fmriprep\n",
    "Assuming you have Docker and `fmriprep-docker` installed, you're ready to run Fmriprep. The basic format of the `fmriprep-docker` command is as follows:\n",
    "\n",
    "```\n",
    "fmriprep-docker <your bids-folder> <your output-folder> \n",
    "```\n",
    "\n",
    "This means that `fmriprep-docker` has two mandatory positional arguments: the first one being your BIDS-folder (i.e., the path to your folder with BIDS-formattefd data), and the second one being the output-folder (i.e., where you want Fmriprep to output the preprocessed data). We recommend setting your output-folder to a subfolder of your BIDS-folder named \"derivatives\": `<your bids-folder>/derivatives`.\n",
    "\n",
    "Then, you can add a bunch of extra \"flags\" (parameters) to the command to specify the preprocessing pipeline as you like it. We highlight a couple of important ones here, but for the full list of parameters, check out the [Fmriprep](https://fmriprep.readthedocs.io/en/stable/usage.html) website.\n",
    "\n",
    "#### Freesurfer\n",
    "When running Fmriprep from Docker, you don't need to have Freesurfer installed, but you *do* need a Freesurfer license. You can download this here: https://surfer.nmr.mgh.harvard.edu/fswiki/License. Then, you need to supply the `--fs-license-file <path to license file>` parameter to your `fmriprep-docker` command:\n",
    "\n",
    "```\n",
    "fmriprep-docker <your bids-folder> <your output-folder> --fs-license-file /home/lukas/license.txt\n",
    "```\n",
    "\n",
    "#### Configuring what is preprocessed\n",
    "If you just run Fmriprep with the mandatory BIDS-folder and output-folder arguments, it will preprocess everything it finds in the BIDS-folder. Sometimes, however, you may just want to run one (or several) specific participants, or one (or more) specific tasks (e.g., only the MRI files associated with the localizer runs, but not the working memory runs). You can do this by adding the `--participant` and `--task` flags to the command:\n",
    "\n",
    "```\n",
    "fmriprep-docker <your bids-folder> <your output-folder> --participant sub-01 --task localizer\n",
    "```\n",
    "\n",
    "You can also specify some things to be ignored during preprocessing using the `--ignore` parameters (like `fieldmaps`):\n",
    "\n",
    "```\n",
    "fmriprep-docker <your bids-folder> <your output-folder> --ignore fieldmaps\n",
    "```\n",
    "\n",
    "#### Handling performance\n",
    "It's very easy to parallelize the preprocessing pipeline by setting the `--nthreads` and `--omp-nthreads` parameters, which refer to the number of threads that should be used to run Fmriprep on. Note that laptops usually have 4 threads available (but analysis servers usually have more!). You can also specify the maximum of RAM that Fmriprep is allowed to use by the `--mem_mb` parameters. So, if you for example want to run Fmriprep with 3 threads and a maximum of 3GB of RAM, you can run:\n",
    "\n",
    "```\n",
    "fmriprep-docker <your bids-folder> <your output-folder> --nthreads 3 --omp-nthreads 3 --mem_mb 3000\n",
    "```\n",
    "\n",
    "In our experience, however, specifying the `--mem_mb` parameter is rarely necessary if you don't parallelize too much. \n",
    "\n",
    "#### Output spaces\n",
    "Specifying your \"output spaces\" (with the `--output-spaces` flag) tells Fmriprep to what \"space(s)\" you want your preprocessed data registered to. For example, you can specify `T1w` to have your functional data registered to the participant's T1 scan. You can, instead or in addition to, also specify some standard template, like the MNI template (`MNI152NLin2009cAsym` or `MNI152NLin6Asym`). You can even specify surface templates if you want (like `fsaverage`), which will sample your volumetric functional data onto the surface (as computed by freesurfer). In addition to the specific output space(s), you can add a resolution \"modifier\" to the parameter to specify in what spatial resolution you want your resampled data to be. Without any resolution modifier, the native resolution of your functional files (e.g., $3\\times3\\times3$ mm.) will be kept intact. But if you want to upsample your resampled files to 2mm, you can add `YourTemplate:2mm`. For example, if you want to use the FSL-style MNI template (`MNI152NLin6Asym`) resampled at 2 mm, you'd use:\n",
    "\n",
    "```\n",
    "fmriprep-docker <your bids-folder> <your output-folder> --output-spaces MNI152NLin6Asym:2mm\n",
    "```\n",
    "\n",
    "You can of course specify multiple output-spaces:\n",
    "\n",
    "```\n",
    "fmriprep-docker <your bids-folder> <your output-folder> --output-spaces MNI152NLin6Asym:2mm T1w fsaverage\n",
    "```\n",
    "\n",
    "#### Other parameters\n",
    "There are many options that you can set when running Fmriprep. Check out the [Fmriprep website](https://fmriprep.readthedocs.io/) (under \"Usage\") for a list of all options!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues, errors, and troubleshooting\n",
    "While Fmriprep often works out-of-the-box (assuming your data are properly BIDS-formatted), it may happen that it crashes or otherwise gives unexpected results. A great place to start looking for help is [neurostars.org](https://neurostars.org). This website is dedicated to helping neuroscientists with neuroimaging/neuroscience-related questions. Make sure to check whether your question has been asked here already and, if not, pose it here!\n",
    "\n",
    "If you encounter Fmriprep-specific bugs, you can also submit and issue at the [Github repository](https://github.com/poldracklab/fmriprep) of Fmriprep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fmriprep output/reports\n",
    "After Fmriprep has run, it outputs, for each participants separately, a directory with results (i.e., preprocessed files) and an HTML-file with a summary and figures of the different steps in the preprocessing pipeline.\n",
    "\n",
    "We ran Fmriprep on a single run/task (`flocBLOCKED`) from a single subject (`sub-03`) some data with the following command:\n",
    "\n",
    "```\n",
    "fmriprep-docker /home/lsnoek1/ni-edu/bids /home/lsnoek1/ni-edu/bids/derivatives --participant-label sub-03 --output-spaces T1w MNI152NLin2009cAsym\n",
    "```\n",
    "\n",
    "We've copied the Fmriprep output for this subject (`sub-03`) in the `fmriprep` subdirectory of the `week_4` directory. Let's check its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir('bids/derivatives/fmriprep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said, Fmriprep outputs a directory with results (`sub-03`) and an associated HTML-file with a summary of the (intermediate and final) results. Let's check the directory with results first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint  # pprint stands for \"pretty print\", \n",
    "\n",
    "sub_path = os.path.join('bids/derivatives/fmriprep', 'sub-03')\n",
    "pprint(sorted(os.listdir(sub_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `figures` directory contains several figures with the result of different preprocessing stages (like functional &rarr; high-res anatomical registration), but these figures are also included in the HTML-file, so we'll leave that for now. The other two directories, `anat` and `func`, contain the preprocessed anatomical and functional files, respectively. Let's inspect the `anat` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_path = os.path.join(sub_path, 'anat')\n",
    "pprint(os.listdir(anat_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see a couple of different files. There are both (preprocessed) nifti images (`*.nii.gz`) and associated meta-data (plain-text files in JSON format: `*.json`).\n",
    "\n",
    "Importantly, the nifti outputs are in two different spaces: one set of files are in the original \"T1 space\", so without any resampling to another space (these files have the same resolution and orientation as the original T1 anatomical scan). For example, the `sub_03_desc-preproc_T1w.nii.gz` scan is the preprocessed (i.e., bias-corrected) T1 scan. In addition, most files are also available in `MNI152NLin2009cAsym` space, a standard template. For example, the `sub-03_space-MNI152NLin2009cAsym_desc-preproc_T1w.nii.gz` is the same file as `sub_03_desc-preproc_T1w.nii.gz`, but resampled to the `MNI152NLin2009cAsym` template. In addition, there are subject-specific brain parcellations (the `*aparcaseg_dseg.nii.gz `and `*aseg_dseg.nii.gz` files), files with registration parameters (`*from- ... -to ...` files), probabilistic tissue segmentation files (`*label-{CSF,GM,WM}_probseg.nii.gz`) files, and brain masks (to outline what is brain and not skull/dura/etc; `*brain_mask.nii.gz`).\n",
    "\n",
    "Again, on the [Fmriprep website](https://fmriprep.readthedocs.io/), you can find more information about the specific outputs.\n",
    "\n",
    "Now, let's check out the `func` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_path = os.path.join(sub_path, 'func')\n",
    "pprint(os.listdir(func_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, like the files in the `anat` folder, the functional outputs are available in two spaces: `T1w` and `MNI152NLin2009cAsym`. In terms of actual images, there are preprocessed BOLD files (ending in `preproc_bold.nii.gz`), the functional volume used for \"functional &rarr; anatomical\" registration (ending in `boldref.nii.gz`), brain parcellations in functional space (ending in `dseg.nii.gz`), and brain masks (ending in `brain_mask.nii.gz`). In addition, there are files with \"confounds\" (ending in `confounds_regressors.tsv`) which contain variables that you might want to include as nuisance regressors in your first-level analysis. These confound files are speadsheet-like files (like `csv` files, but instead of being comma-delimited, they are tab-delimited) and can be easily loaded in Python using the [pandas](https://pandas.pydata.org/) package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conf_path = os.path.join(func_path, 'sub-03_task-flocBLOCKED_acq-Mb4Mm27Tr700_desc-confounds_regressors.tsv')\n",
    "conf = pd.read_csv(conf_path, sep='\\t')\n",
    "conf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confound files from Fmriprep contain a large set of confounds, ranging from motion parameters (`rot_x`, `rot_y`, `rot_z`, `trans_x`, `trans_y`, and `trans_z`) and their derivatives (`*derivative1`) and squares (`*_power2`) to the average signal from the brain's white matter and cerebrospinal fluid (CSF), which should contain sources of noise such as respiratory, cardiac, or motion related signals (but not signal from neural sources, which should be largely constrained to gray matter). For a full list and explanation of Fmriprep's estimated confounds, check their website. Also, check [this thread](https://neurostars.org/t/confounds-from-fmriprep-which-one-would-you-use-for-glm/326) on Neurostars for a discussion on which confounds to include in your analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the actual preprocessed outputs, Fmriprep also provides you with a nice (visual) summary of the different (major) preprocessing steps in an HTML-file, which you'd normally open in any standard browser to view. Here. we load this file for our example participants (`sub-03`) inside the notebook below. Scroll through it to see which preprocessing steps are highlighted. Note that the images from the HTML-file are not properly rendered in Jupyter notebooks, but you can right-click the image links (e.g., `sub-03/figures/sub-03_dseg.svg`) and click \"Open link in new tab\" to view the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='./bids/derivatives/fmriprep/sub-03.html', width=700, height=600)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
