{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine learning (\"decoding\") analyses\n",
    "This week's tutorial is about how to implement \"decoding\" analyses in Python! \n",
    "\n",
    "The term \"decoding\" is often used to denote analyses that aim to predict a single experimental feature (which can be either within-subject or between-subject) based on patterns of neuroimaging data. Some more advanced techniques make it possible to predict more than one experimental feature at once (with [Bayesian \"reconstruction\" techniques](https://www.sciencedirect.com/science/article/pii/S0896627309006850) and [\"inverted encoding models\"](https://www.sciencedirect.com/science/article/pii/S0896627315006352)), but these are beyond the scope of this course. Here, we'll focus on machine learning/statistical models and techniques that allow you to predict a single experimental feature. \n",
    "\n",
    "We'll make heavy use of the awesome [scikit-learn](http://scikit-learn.org/stable/) library &mdash; the go-to library for machine learning in Python. \n",
    "\n",
    "**What you'll learn**: at the end of this tutorial, you will be able to:\n",
    "\n",
    "* use and implement feature-selection/extraction methods;\n",
    "* fit machine learning models and predict (new) samples;\n",
    "* implement cross-validation routines;\n",
    "* statistically evaluate model performance estimates;\n",
    "\n",
    "**Estimated time needed to complete**: 8-12 hours<br>\n",
    "**Credits**: if you use scikit-learn in your research, please cite the corresponding [paper](http://www.jmlr.org/papers/v12/pedregosa11a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data representation\n",
    "Decoding analyses always need two sets of data: the brain patterns, which we'll refer to as $\\mathbf{R}$ (for **R**esponse), and a single experimental feature that we want to predict, which we'll refer to as $\\mathbf{S}$ (which traditionally refers to **S**timulus). Note that $\\mathbf{s}$ could be a within-subject experimental factor (such as stimulus or response-related factors) or a between-subject variable (such as age or depressed vs. healthy control). Moreover, the to-be-predicted variable\\* can be either continuous (e.g., reaction time) or categorical (e.g., object category), which are associated with different types of models, regression and classification models respectively (more about this later). Note that \"direction\" of analysis is the exact opposite of what is done in enconding analyses. In encoding analyses, we try to predict the brain data (dependent variable) using experimental features (independent variables), while in decoding analyses we try to predict an experimental feature (dependent variable) using a a set of brain patterns (independent variables)! But essentially, encoding and decoding models are mathematically the same (they just use different inputs). \n",
    "\n",
    "In the first part of this lab, we'll work with simulated data. For now, we'll assume that our data is from a simple face perception experiment in which participants viewed images with either male (condition: \"M\") or female faces (condition: \"F\") across four different fMRI runs. So, our experimental feature of interest is a categorical variable with two levels (\"M\" and \"F\"), making this a classification analysis (which is more common than regression in the context of cognitive neuroscience). Each run, participants saw fourty images (twenty for each condition) presented in a random order. \n",
    "\n",
    "We'll simulate the patterns ($\\mathbf{R}$) and experimental feature ($\\mathbf{S}$, \"M\" vs. \"F\") below. For now, we'll generate random data (with some autocorrelation), for reasons that will become clear later. We'll assume that we are restricting our analysis to a single region-of-interest containing a 1000 voxels.\n",
    "\n",
    "---\n",
    "\\* Often, the to-be-predicted variable is called the \"target\" or \"dependent variable\". Here, we'll use the term \"target\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import toeplitz\n",
    "from niedu.utils.nipa import generate_labels\n",
    "\n",
    "N_per_run = 40\n",
    "M = 4  # nr of runs\n",
    "K = 1000  # nr of voxels\n",
    "\n",
    "# Generate random data drawn for a multivariate normal\n",
    "# distribution with AR1 noise (with phi = 0.85) to\n",
    "# simulate autocorrelated noise in the estimated patterns,\n",
    "# which is plausible for designs with relatively short ISIs\n",
    "mu = np.zeros(N_per_run)\n",
    "V = 0.85 ** toeplitz(np.arange(N_per_run))\n",
    "\n",
    "# R_runs is a list of M arrays of shape N_per_run x K\n",
    "R_runs = [np.random.multivariate_normal(mu, V, size=K).T for _ in range(M)]\n",
    "\n",
    "# S_runs is a list of M arrays of shape N_per_run\n",
    "# The custom generate_label function creates slightly correlated\n",
    "# labels\n",
    "S_runs = [generate_labels(['M', 'F'], N_per_run / 2, [0.7, 0.3]) for _ in range(M)]\n",
    "\n",
    "print(\"Example of patterns for run 1:\\n\", R_runs[0])\n",
    "print(\"\\nExample of target for run 1:\\n\", S_runs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, technically, we have everything we need for a decoding analysis. However, machine learning (ML) and statistical models often require all data to be represented numerically, so we need to convert our target (containing the values \"M\" and \"F\") into a numeric format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (optional; 0 points): If you want to practice your Python skills, try converting the string labels of run 1 (the <tt>S_run1</tt> variable below) to numeric labels. Specifically, use the integer 0 for trials of condition \"F\" and the integer 1 for trials of condition \"M\". Store the result in a new variable called <tt>S_run1_num</tt>; make sure it's a numpy array. (To convert a list to a numpy array, you can do: <tt>np.array(your_list)</tt> ).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6fbc253687d441a6389d7e913bc915e",
     "grade": false,
     "grade_id": "cell-763391e8a76ad943",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "S_run1 = S_runs[0]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a896bc06aa4c5246e7ca560c9f829f6",
     "grade": true,
     "grade_id": "cell-a55c734166e7288f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_2 import test_lab2num\n",
    "test_lab2num(S_run1, S_run1_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While converting labels to numeric values can be done quite easily using standard Python, it gives us with a nice excuse to introduce some scikit-learn functionality. Specifically, the `LabelEncoder` class, which allows you to *encode* your target *labels* into a numeric format. Let's start with importing it (from the `preprocessing` module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way `LabelEncoder` is used is similar to some of the Nilearn and Nistats functionality you've seen. In short, you first need to initialize the object (with, optionally, some parameters), after which you can give it data to `fit` and `transform`. This pattern is something we'll encounter a lot in this lab when working with scikit-learn. \n",
    "\n",
    "Let's initialize a `LabelEncoder` object below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder objects are not initialized with any parameters\n",
    "lab_enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's \"fit\" it on the labels of our first run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc.fit(S_runs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `fit` function doesn't *return* anything useful (well, technically, it returns it*self*). Instead, when calling `fit`, it stores some parameters in the object itself as *attributes* which are used when calling the transform. For the `LabelEncoder` specifically, it stores the unique conditions (often called *classes* in machine learning) in an attribute called `classes_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lab_enc.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, most \"things\" that are inferred or computed in the `fit` method of scikit-learn objects (and are needed later when calling `transform`) are stored in attributes with a trailing underscore (like `classes_`). We know, this all sounds incredibly trivial, but explaning these things in detail will give you a better understanding of how scikit-learn works (which is going to help a lot when dealing with more complicated functionality).\n",
    "\n",
    "Finally, after fitting the `LabelEncoder` object, we can call the `transform` method to actually transform the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_run1_num = lab_enc.transform(S_runs[0])\n",
    "print(S_run1_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, unlike the `fit` method, the `transform` method actually returns something, i.e., the transformed labels. Also note that the numeric labels are assigned alphabetically (i.e., \"F\" gets assigned 0, \"M\" gets assigned 1). \n",
    "\n",
    "Like you've seen in the Nilearn notebook, we can often call `fit` and `transform` at once using the `fit_transform` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_run1_num = lab_enc.fit_transform(S_runs[0])\n",
    "print(S_run1_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, after the `LabelEncoder` has been fit, it can be reused on other data, i.e., you can call the `transform` method on new arrays. This is how many classes in scikit-learn are actually used (i.e., fit on a particular subset of data and then apply to another subset), as it allows for efficient *cross-validation* &mdash a topic that will discuss in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): We just converted the labels from the first run only. For this ToDo, convert the labels from <em>all</em> runs (i.e., the <tt>S_runs</tt> variable) using the <tt>LabelEncoder</tt>. Store the results in a new variable named <tt>S_runs_num</tt>, which should be a list (of length four, i.e., four runs) of arrays with ones and zeros (instead of \"M\" and \"F\"). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1d0d3d3dce247c7ef19dfec6547a444",
     "grade": false,
     "grade_id": "cell-90aae09304eae8be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here . '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef8b43398bb1a0011a309ca6d872e656",
     "grade": true,
     "grade_id": "cell-5ce7ae8cc2e6fe0e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_2 import test_lab2num_all_runs   \n",
    "test_lab2num_all_runs(S_runs, lab_enc, S_runs_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we have everything we need to start building our decoding pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "In addition to the \"preprocessing\" steps for pattern analyses discussed in last week's lab, when doing decoding, you additionally need to standardize your brain features (i.e., the columns in your pattern matrix) on which you fit your model. With \"standardization\", we mean making sure each feature ($\\mathbf{R}_{j}$ for column $j$) in your pattern matrix has 0 zero mean and unit (1) standard deviation, which can be achieved as follows for each feature $j$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{R}_{j, norm} = \\frac{\\mathbf{R}_{j} - \\bar{\\mathbf{R}}_{j}}{\\hat{\\sigma}(\\mathbf{R}_{j})}\n",
    "\\end{align}\n",
    "\n",
    "where $\\bar{\\mathbf{R}_{j}}$ represents the *mean* of $\\mathbf{R}_{j}$ and $\\hat{\\sigma}(\\mathbf{R}_{j})$ represents the standard deviation of $\\mathbf{R}_{j}$. In other words, for each value in $\\mathbf{R}$, you subtract the mean from the column it belongs to and subsequently you divide the result by the standard deviation of the column it belongs to. This process is also known as *z-scoring*.\n",
    "\n",
    "This standardization process is done for each brain feature (column) separately. Standardization is important for most ML/statistical models because it makes sure that each brain feature has the same *scale*, which often helps in efficiently estimating model parameters. \n",
    "\n",
    "Importantly, when you have patterns from multiple runs (as is often the case in fMRI decoding), these patterns should also be independently standardized, even if you want to pool these patterns later on (see [Lee & Kable, 2018](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0207083)). This is because some runs may yield patterns with a relatively higher mean or standard deviation across samples (for example, because participants start moving more towards the end of the experiment, leading to more noisy pattern estimates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): In this ToDo, you have to standardize the data from run 1 (the <tt>R_run1</tt> variable below) using Numpy (afterwards, we'll show you how to do this using scikit-learn). Store the results (which should have columns with mean zero and unit standard deviation) in a new variable called <tt>R_run1_norm</tt>. Importantly, this can be done efficiently (without a for-loop) using broadcasting!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4a6d945f4c59c3ccc40eba32c37fdc8",
     "grade": false,
     "grade_id": "cell-6441d565a6145e81",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "R_run1 = R_runs[0]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e62b5ad3081ea687a7c018b3f14204d5",
     "grade": true,
     "grade_id": "cell-1c415219d875b252",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the ToDo above. '''\n",
    "# ToThink: do you understand how we're testing your answer here?\n",
    "np.testing.assert_array_almost_equal(R_run1_norm.mean(axis=0), np.zeros(R_run1.shape[1]))\n",
    "np.testing.assert_array_almost_equal(R_run1_norm.std(axis=0), np.ones(R_run1.shape[1]))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this similarly using the `StandardScaler` class from scikit-learn, which has the same `fit`/`transform` interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "R_run1_norm = scaler.fit_transform(R_run1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fitting process, the `StandardScaler` computed the feature-wise mean and standard deviation, which it stores in the `mean_` and `_scale` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.mean_.shape)\n",
    "print(scaler.scale_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial run-wise standardization, we usually don't want to cross-validate our standardization process (i.e., to for example fit the `StandardScaler` on run 1 and subsequently transform the other runs) for reasons discussed before. As such, we need to call fit and transform on each run separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we use a \"list comprehension\" to loop across our runs\n",
    "# to standardize each run separately!\n",
    "R_runs_norm = [scaler.fit_transform(this_R) for this_R in R_runs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to start fitting models. Before we do so, we are going to concatenate our data ($\\mathbf{R}$ and $S$) across runs because we want to give our model as much data as possible! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load S_runs_num if you didn't manage to do the last ToDo\n",
    "S_runs_num = np.load('S_runs_num.npy')\n",
    "\n",
    "R_all = np.vstack(R_runs_norm)  # stack vertically\n",
    "S_all = np.concatenate(S_runs_num)\n",
    "\n",
    "print(\"Shape R_all:\", R_all.shape)\n",
    "print(\"Shape S_all:\", S_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting models\n",
    "When fitting decoding models, we assume that we can approximate our target variable ($\\mathbf{S}$) as a function of the input data ($\\mathbf{R}$):\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{S}} \\approx f(\\mathbf{R})\n",
    "\\end{align}\n",
    "\n",
    "Usually, the models used in pattern analyses assume linear functions (especially with relatively little data), i.e., functions that approximate the target using a linear combination of input variables ($\\mathbf{R}_{j}$) weighted by parameters ($\\beta$). Note that the univariate GLM often used in encoding models is such a linear model. The process of model fitting is estimating parameters that minize the discrepancy (error) between the predicted values ($\\hat{\\mathbf{S}}$) and the actual values ($\\mathbf{S}$) of the target variable, both for regression ($\\mathbf{S}$ is continuous) and classification ($\\mathbf{S}$ is categorical) models. Different models differ in how they exactly estimate their parameters, but the general process is the same (minimizing error between predictions and target). In this course, we won't go much into the differences across models (partly because in practice, we found that performance doesn't differ that much between models).\n",
    "\n",
    "Alright, let's get to it. Below, we import the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class, a particular linear *classification* model (unlike the name suggests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are many \"options\" (often called \"hyperparameters\") we can set upon initialization of a `LogisticRegression` object, but for now, we will only set the \"solver\" (for no other reason that this will get rid of a warning during the fitting process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = CLassiFier\n",
    "clf = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the fitting process using this model (or actually, *any* model in scikit-learn) is as simple as, guess what, calling the `fit` method! Unlike the `LabelEncoder` and `StandardScaler` that we discussed before, models in scikit-learn (including `LogisticRegression`) require two arguments when calling their `fit` method: `X` and `y`, which represent the input data (in our case: $\\mathbf{R}$) and the target variable (in our case: $\\mathbf{S}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text in the output cell is there because the\n",
    "# fit model returns \"itself\" (you can just ignore this)\n",
    "clf.fit(R_all, S_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, the model stores the estimated parameters ($\\beta$) as an araay in the `coef_` (\"coefficients\", another term for parameters) attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of coef_:\", clf.coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model estimated one parameter for each brain feature (column in $\\mathbf{R}$). Now, unlike the previously discussed `LabelEncoder` and `StandardScaler`, scikit-learn models do not have a `transform` method; instead, they have a `predict` method, which takes a single input (a 2D array with observations) and generates discrete\\* predictions for this input. For now, we'll call `predict` on the same data we've fit the model on:\n",
    "\n",
    "---\n",
    "\\* Some models, including the `LogisticRegression` model, have an additional method called `predict_proba` which returns probabilistic instead of discrete predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(R_all)\n",
    "print(\"Predictions for samples in R_all:\\n\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Alright, we have some preditions for our data! But how do we evaluate these predictions? This actually depends on whether you have a classification model (with categorical predictions) or a regression model (with continuous predictions). Because classification analyses are most popular in cognitive neuroscience (and our example data is categorical) and you are already familiar with some regression metrics such as $R^2$ (discussed in the previous course), we'll focus on model evaluation metrics for classification here.\n",
    "\n",
    "### Metrics for discrete predictions\n",
    "There are many different metrics to evaluate the predictions of classification models. The most well known (but not necessarily always appropriate) metric for *discrete* predictions is *accuracy*, which is defined as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{accuracy} = \\frac{\\mathrm{number\\ of\\ correct\\ predictions}}{\\mathrm{total\\ number\\ of\\ predictions}}\n",
    "\\end{align}\n",
    "\n",
    "For accuracy, the best possible score is 1 (predict all samples correctly) and \"chance level\" performance (i.e., the expected score when randomly guessing) is, in general, $\\frac{1}{\\mathrm{Number\\ of\\ classes}}$, so for our example with two classes (\"M\" and \"F\"), it is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Using the predictions (the <tt>preds</tt> variable) and the true labels (the <tt>S_all</tt> variable), compute the associated accuracy and store this in a variable <tt>acc</tt>. Note: there is no need for a for-loop! You can use the fact that boolean values, <tt>True</tt> and <tt>False</tt>, evaluate to 1 and 0, respectively, in Python... \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "373dfe02556199bac272d782cf3c279b",
     "grade": false,
     "grade_id": "cell-ecc18f08c4f7800e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26aa8032b0dc42469085e6a4231994e2",
     "grade": true,
     "grade_id": "cell-85ce2cd9b3b961d5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_2 import test_acc\n",
    "test_acc(preds, S_all, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for probabilistic predictions\n",
    "Some classifiers allow for *probabilistic* (instead of discrete) predictions. For those classifiers, an additional method called `predict_proba` exists, which outputs a probability for each class. So, in a two-class classification setting, the `predict_proba` method will not give a single discrete prediction (i.e., either `0` or `1`) but a probability distribution over classes (e.g., 0.92 for class `0` and 0.08 for class `1`).\n",
    "\n",
    "The `LogisticRegression` model from scikit-learn actually allows for probabilistic prediction. In general, if we'll give it all $N$ trials for a target variable with $M$ classes, it will output a $N \\times M$ array with probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(R_all)\n",
    "# let's print the first five trials\n",
    "print(np.round(probas[:5, :], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One often-used performance metric for probabilistic predictions is the \"Area Under the ROC curve\" (often abbreviated as *AUROC* or just *AUC*). Fortunately, the scikit-learn library contains implementations of many performance metrics, including AUROC (called `roc_auc_score` in scikit-learn), which can be imported from the `metrics` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC is an excellent metric to use for probabilistic predictions, but there's one caveat: when evaluating probabilistic predictions (formatted as a $N \\times M$ matrxi), it needs the *true* target values (dependent variable) in a *one-hot-encoded* format. One-hot-encoding (OHE) is a technique that transforms a $N \\times 1$ vector with $M$ classes into a $N \\times M$ binary matrix:\n",
    "\n",
    "\\begin{align}\n",
    "S = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "2 \\\\\n",
    "4\n",
    "\\end{bmatrix}\n",
    "\\underset{\\Longrightarrow}{\\mathrm{OHE}} \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "You might know this technique under the name \"dummy (en)coding\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (optional): Complete the <tt>one_hot_encode</tt> function below that takes in a 1D vector representing a target variable with any number of classes and observations and outputs a one-hot-encoded version of that target variable.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03ea1bc0dd383cde558a690099c6e7b3",
     "grade": false,
     "grade_id": "cell-87a96c9d529cbb57",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement the optional ToDo here. '''\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    ''' One-hot-encodes a 1D target vector. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy array\n",
    "        1D target vector with N observations and M classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An NxM numpy array\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7735cda1760d04dcd441b6136dd1fe4",
     "grade": true,
     "grade_id": "cell-a84ba55c68963bc6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "# Test 1\n",
    "y = np.array([0, 1])\n",
    "out = one_hot_encode(y)\n",
    "ans = np.eye(2)\n",
    "np.testing.assert_array_equal(ans, out)\n",
    "\n",
    "# Test 2\n",
    "y = np.array([1, 2, 3, 2, 2, 1])\n",
    "out = one_hot_encode(y)\n",
    "ans = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0]])\n",
    "np.testing.assert_array_equal(ans, out)\n",
    "\n",
    "# Test 3\n",
    "y = np.array([3, 2, 1])\n",
    "out = one_hot_encode(y)\n",
    "ans = np.rot90(np.eye(3))\n",
    "np.testing.assert_array_equal(ans, out)\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above ToDo was a nice way to practice your Python skills, we nonethless recommend using the `OneHotEncoder` class from scikit-learn to one-hot-encode your target vector. It uses the `fit`-`transform` syntax you are familiar with by now. Importantly, as the `OneHotEncoder` is, in practice, often used to one-hot-encoded predictors (independent variables), it expects a 2D array (not a 1D vector). So, when one-hot-encoding a 1D target variable, you can add a singleton axis (with `np.newaxis`) to make it work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)  # we don't want a \"sparse\" output\n",
    "S_all_ohe = ohe.fit_transform(S_all[:, np.newaxis])\n",
    "print(S_all_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the `roc_auc_score` function to compute our model performance. Like any metric implementation in scikit-learn, it is called as follows:\n",
    "\n",
    "```Python\n",
    "score = metric(true_labels, predicted_labels)\n",
    "```\n",
    "\n",
    "where `true_labels` and `predicted_labels` are either 1D vectors of length $N$ (in case of discrete predictions) or 2D $N \\times M$ arrays (in case of probabilistic predictions). Note that by default these metrics output a single score (often the average of the class-specific scores); some (but not all) metrics (including the `roc_auc_score`) allow the function to return a class-specific score by setting the optional argument `average` to `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omit average=None to get a single (average) score\n",
    "scores = roc_auc_score(S_all_ohe, probas, average=None)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (optional, quite difficult!): Another class of classification metrics are \"pseudo $R^2$\" scores. These metrics are similar to $R^2$ in regression models in the sense that they are bounded between 0 (chance performance) and 1 (perfect performance). They often assume probabilistic predictions. \n",
    "    \n",
    "One of these pseudo $R^2$ metrics is \"Tjur's pseudo $R^2$\" which is defined as the difference between the average (across observations) probability of a particular class and the average probability of the other class(es) for a given label. So, suppose we're dealing with a two-class classification problem (with class 0 and class 1), and the average probability for class 1 of trials belonging to class 1 is 0.9, while the average probability for class 1 of trials belonging to class 0 is 0.3, then the Tjur's pseudo $R^2$ score for class 1 is $0.9-0.3=0.6$. Formally, the Tjur's pseudo $R^2$ score for class $m$ is defined as:\n",
    "    \n",
    "\\begin{align}\n",
    "R^2_{m} = \\frac{1}{N}\\sum_{i=1}^{N}p(\\hat{s}^{m}_{i}) - \\frac{1}{N}\\sum_{i=1}^{N}p(\\hat{s}^{\\neg m}_{i})\n",
    "\\end{align}\n",
    "    \n",
    "for any set of trials belonging to class 1 ($p^{m}_{i}$) and complementary set of trials *not* beloning to class 1 ($p^{\\neg m}_{i}$).\n",
    "    \n",
    "Complete the function <tt>tjur_r2</tt> below that takes two arguments &mdash; <tt>target</tt> (a 1D array with target labels) and <tt>probas</tt> (a 2D array with probabilities) &mdash; and should output an array of length $M$ with Tjur's pseudo $R^2$ scores for the $M$ classes in the target array.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "081c737f8b83bcde8f5a4873cbc5f31f",
     "grade": false,
     "grade_id": "cell-a7ce0f176f737457",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "\n",
    "def tjur_r2(target, probas):\n",
    "    ''' Computes Tjur's R2 score for all classes in `target`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target : numpy array\n",
    "        A 1D array with numerical targets\n",
    "    probas : numpy array\n",
    "        A 2D array with target probabilities\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    A numpy array of length M with R2 scores for all M classes\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2939cea8449286fb012fec874ca5233",
     "grade": true,
     "grade_id": "cell-be49190fb247b809",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the ToDo above. '''\n",
    "y = np.array([0, 0, 1, 1])\n",
    "probas = np.array([[0.9, 0.1], [0.95, 0.05], [0.3, 0.7], [0.4, 0.6]])\n",
    "ans = tjur_r2(y, probas)\n",
    "np.testing.assert_array_almost_equal(ans, np.array([0.575, 0.575]))\n",
    "\n",
    "y = np.array([1, 1, 3, 3, 2, 2])\n",
    "probas = np.array([[0.8, 0.1, 0.1], [0.7, 0.2, 0.1], [0.4, 0.0, 0.6], [0.3, 0.1, 0.6], [0.1, 0.5, 0.4], [0.2, 0.6, 0.2]])\n",
    "ans = tjur_r2(y, probas)\n",
    "np.testing.assert_array_almost_equal(ans, np.array([0.5, 0.45, 0.4]))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success'>\n",
    "    <b>Tip</b>: <a href=\"https://www.biorxiv.org/content/10.1101/743138v1.abstract\">This article</a> by Dinga and colleagues (2019) argues against \"accuracy\" as a performance metric for classification models and reviews several (often probabilistic) alternatives; a highly recommended read!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "If you did the previous ToDos correctly, you should have found that the accuracy was 1 (the maximum possible score)! Amazing! But wait, how is this possible? We generated *random* data, right? \n",
    "\n",
    "So, what is the issue here? Well, we fitted the model on the same data that we want to generate predictions for! While this is common practice in many statistical models in psychology and neuroscience (including standard univariate \"activation-based\" fMRI models), this is not advisable for decoding models. The reason for this is that our decoding models often have many more predictors (i.e., brain features) than observations (i.e., trials). The consequence is that the model has a hard time figuring out what is \"signal\" and what is \"noise\", which will often cause your model to capitalize on spurious correlations between your data ($\\mathbf{R}$) and the target ($\\mathbf{S}$). The result is that your model will be *overfitted* and your model performance estimate will be overly optimistic estimate of generalization performance.\n",
    "\n",
    "To obtain an unbiased (that is, not overly optimistic) estimate of generalization performance, machine-learning based analyses often perform *cross-validation*. Cross-validation is, in it's broadest definition, the process of estimating analysis parameters on a different subset of your data than the data you want to generate predictions for (and thus base generalization performance on). With \"analysis parameters\", we do not only mean the parameters of your statistical model ($\\hat{\\beta}$), but this may also involve parameters estimated during preprocessing and feature transformations (which we'll talk about later). Importantly, cross-validation (if done properly) allows you to derive an unbiased estimate of generalization performance, i.e., how well your analysis would generalize to a new dataset. Importantly, cross-validation _does not solve overfitting_; it only _detects_ overfitting. To prevent overfitting, you can employ regularization or feature selection/extraction techniques, of which the latter is briefly discussed in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train vs. test set partitioning\n",
    "Usually, the subset of data you use to fit your analysis parameters on is called the *train* set and the subset you evaluate your model predictions on is often called the *test* set. Assuming that each observation (i.e., row in $\\mathbf{R}$) is independent from all other observations, any spurious correlation that is capitalized upon in the train set will not generalize to the test set!  \n",
    "\n",
    "There are different cross-validation schemes (i.e., how you partition your data in a train and set set). For the example in the next cell, we'll use a simple *hold-out* scheme, in which we'll reserve 50% of the data for the test set (note that this could have been a different percentage). (We'll discuss more intricate cross-validation schemes such as K-fold in the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train = R_all[0::2, :]  # all even samples\n",
    "S_train = S_all[0::2]\n",
    "\n",
    "R_test = R_all[1::2, :]  # all odd samples\n",
    "S_test = S_all[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data into a train and test set, we have introduced a \"problem\" however: the features within the train and test set may not have 0 mean and unit (1) variance anymore! Given that the features were properly standardized across *all* samples in our simulated fMRI dataset, this is unlikely to be a problem for our classifier. It is still good practice to make sure your *train set* is properly standardized. So, before fitting our classifier, let's standardize the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train_norm = scaler.fit_transform(R_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit our model on the standardized train set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(R_train_norm, S_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before predicting the test set, however, we need to decide whether we want to independently standardize the test set or whether to *cross-validate* our previously estimated standardized parameters (the feature-wise mean and standard deviation). Although opinions differ on this topic (see e.g. [this excellent article](https://jamanetwork.com/journals/jamapsychiatry/article-abstract/2756204)), if we want a truly unbiased estimate of generalization, we should also cross-validate our standardization procedure in addition to cross-validation of our model. So, to cross-validate our standardization procedure, we do the following for each feature $j$ of our test set:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{R}_{j, norm}^{\\mathrm{test}} = \\frac{\\mathbf{R}_{j}^{\\mathrm{test}} - \\bar{\\mathbf{R}}_{j}^{\\mathrm{train}}}{\\hat{\\sigma}(\\mathbf{R}_{j}^{\\mathrm{train}})}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we're *not* calling fit on the test set, i.e.,\n",
    "# we're cross-validating our standardization procedure!\n",
    "R_test_norm = scaler.transform(R_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can cross-validate our model and derive predictions for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(R_test_norm)\n",
    "print(\"Predictions for our test set samples:\\n\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions (`preds`) are made independently from the fitting process. Now, let's evaluate the model performance on these predictions, this time we're going to be lazy and use the `accuracy_score` from the `metrics` module in scikit-learn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc_cv = accuracy_score(S_test, preds)\n",
    "print(\"Cross-validated accuracy:\", acc_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, accuracy is not perfect (1.0) anymore, but it is still much higher than you'd expect on random data (for which chance-level performance would be 0.5).\n",
    "\n",
    "In hold-out cross-validation (which we demonstrated previously), you use your train set *only* for fitting and your test set *only* to evaluate model predictions. In other words, you only fit and predict once, but on different subsets of your data. If you have a big dataset (i.e., many samples), your test set can be relatively large, and thus cross-validated accuracy on the test set will probably be a good estimate of how well our model will generalize to future/other data. However, if you have a relatively small dataset, you will probably have a relatively small test-set. If you then estimate cross-validated accuracy on this test-set, the chance of just getting a relatively good (or bad) score by coincidence is quite high (see e.g. [Varoquaux et al., 2017](https://www.sciencedirect.com/science/article/pii/S105381191630595X#s0055))! In other words, the estimate of cross-validation accuracy is not really robust. Fortunately, there are ways to increase robustness of cross-validation accuracy estimates; one of them is by using K-fold cross-validation instead of hold-out cross-validation, in which you divide your dataset into $K$ folds, which you iteratively use as train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (1 point)<br>\n",
    "Can you think of a (practical) reason to prefer hold-out cross-validation over K-fold cross-validation?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dabde50be0ddef5424645e3b022bf497",
     "grade": true,
     "grade_id": "cell-eb1ce32f7f2423d1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As fMRI data-sets often contain few samples (trials/subjects), K-fold cross-validation is often used. Instead of writing our own K-fold cross-validation scheme, we'll use some of scikit-learn's functionality. Specifically, we are going to use the [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) class from scikit-learn's `model_selection` module. Click the highlighted link above and read through the manual to see how it works.\n",
    "\n",
    "Importantly, if you're dealing with a classification analysis, always use **Stratified**KFold (instead of the regular [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)), because this version makes sure that each fold contains the same proportion of the different classes (here: 0 and 1). \n",
    "\n",
    "Anyway, enough talking. Let's initialize a `StratifiedKFold` object with, let's say, 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# They call folds 'splits' in scikit-learn\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have a `StratifiedKFold` object now, but not yet any indices for our folds (i.e. indices to split our $\\mathbf{R}$ and $S$ into different subsets). To do that, we need to call the `split` method, which takes two inputs: the data ($\\mathbf{R}$) and the target ($S$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = skf.split(R_all, S_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we created the variable `folds` which is, technically, a [generator](https://wiki.python.org/moin/Generators) object, but just think of it as a type of list (with indices) which is specialized for looping over it. Each entry in `folds` is a tuple with two elements: an array with train indices and an array with test indices. Let's demonstrate that\\*:\n",
    "\n",
    "-------------\n",
    "\\* Note that you can only run the cell below once. After running it, the `folds` generator object is \"exhausted\", and you'll need to call `skf.split(R_all, S_all)` again in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fold in enumerate(folds):\n",
    "    \n",
    "    print(\"Processing fold %i\" % (i + 1))\n",
    "    # Here, we unpack fold (a tuple) to get the train and test indices\n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    print(\"Train indices:\", train_idx)\n",
    "    print(\"Test indices:\", test_idx, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a proper analysis, you would fit a model on the train set, predict the labels for the test set, and compute the cross-validated accuracy for all $K$ folds separately. Note that, in the case of K-fold cross-validation, you technically estimating $K$ different models (i.e., the estimated model parameters are likely slightly different across folds). For most decoding purposes, this is not necessarily a problem, as we're often not interested in the model *parameters*, but the (cross-validated) model *performance*. As such, people usually compute the fold-wise model performance and subsequently average these values to get an average model score &mdash; which is exactly what you're going to do in the next ToDo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): In the code cell below, initialize a <tt>StratifiedKFold</tt> object with 4 folds and write a for-loop that iterates across the 4 folds. Use the following additional parameters when initializing the <tt>StratifiedKFold</tt> object: <tt>random_state=42</tt> (this is to be able to test your implementation) and <tt>shuffle=True</tt> (which will draw random folds). Store this object in a variable named <tt>skf_4f</tt>.\n",
    "\n",
    "In every iteration, divide the data into a train and test set, apply (cross-validated) standardization, fit the  model (you can reuse the <tt>LogisticRegression</tt> object from before) on the train set, predict the test set, and compute the accuracy. Store the accuracy for each iteration. After the loop, you should have 4 cross-validated accuracy scores. Average these and store the result (a single number) in a variable named <tt>acc_cv_average</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "941d7a3821399add5bcdaaf3a156da56",
     "grade": false,
     "grade_id": "cell-c63a7be264d833e9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement the ToDo here. '''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efd1e443d11c00a92608114351ac7c7e",
     "grade": true,
     "grade_id": "cell-b2a2ecc099dab6c8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_2 import test_skf_loop_with_seed    \n",
    "test_skf_loop_with_seed(R_all, S_all, scaler, clf, skf_4f, acc_cv_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Tip</b>: you might wonder how many folds you should choose. It is tempting to choose as many folds as possible, i.e. to make your train set as big as possible, such that your model has as much data as possible to train on. The flip side, however, is that your test set becomes smaller with larger train sets, which tends to lead to highly variable cross-validated model performance estimates (e.g., 0.9 in one fold, but 0.35 in the other fold). It has been recommended to use a test set size of about 10%-20% of the size of your entire dataset (i.e., using 5-10 folds; see <a href=\"https://www.sciencedirect.com/science/article/pii/S105381191630595X#s0055\">this article</a>). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidenote: scikit-learn `Pipelines`\n",
    "As you might have noticed in the previous ToDo, it takes quite some lines of code to fully cross-validate your standardization step and model: fit your scaler on the train set, transform the train set, transform the test set, fit your model on the train set, and finally predict your test set. This cross-validation routine will only become more complicated and cumbersome when you add extra preprocessing or transformation procedures to it (as we'll do in the next section). As such, let us introduce one of the most amazing features of scikit-learn: `Pipelines`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (0 points): Read through the documentation of the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">Pipeline class</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn `Pipeline`s allow you to \"bundle together\" a sequence of analysis steps (which may include preprocessing and feature transformation operations) that usually ends in a model (e.g., a `LogisticRegression` object). Then, you can fit all steps on a particular subset of data by calling the `Pipeline`'s `fit` method and subsequently call the `predict` method on another subset of data, which will *automatically cross-validate every step in your analysis pipeline*. Instead of initializing `Pipeline` objects directly, we'll use the convenience function `make_pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `make_pipeline` function accepts an arbitrary number of arguments which should all be either preprocessing or feature transformation objects (i.e., so-called [transformator](https://scikit-learn.org/stable/data_transforms.html) objects) or model objects (i.e., so-called `estimator` objects), such as a `LogisticRegression` object. Note that you can only have a single model object in your pipeline, which should be the *last* step in your pipeline.\n",
    "\n",
    "Let's create a very simple pipeline that involves standardization and a logistic regression model (like you implemented in the previous ToDo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-initialize these objects for clarity\n",
    "scaler = StandardScaler()\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# The make_pipeline function returns a Pipeline object\n",
    "pipe = make_pipeline(scaler, clf)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the data from the simple hold-out split from before to demonstrate the fiting and cross-validation of our complete pipeline. Just like a normal model, we can call the `fit` and `predict` methods to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train = R_all[0::2, :]\n",
    "R_test = R_all[1::2, :]\n",
    "S_train = S_all[0::2]\n",
    "S_test = S_all[1::2]\n",
    "\n",
    "# First, let's fit *all* the steps\n",
    "pipe.fit(R_train, S_train)\n",
    "\n",
    "# And now cross-validate *all* the steps\n",
    "preds = pipe.predict(R_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, right? Using pipelines saves you many lines of code and allows you to easily cross-validate entire pipelines. You'll practice with pipelines in the upcoming ToDo.\n",
    "\n",
    "Now, back to cross-validation routines. One notable variant of K-fold cross-validation is *repeated* stratified K-fold cross-validation, in which the cross-validation loop is repeated several times with different (random) folds. This way, the cross-validated model performance estimates usually become more stable (i.e., less variance). (Of course, scikit-learn contains a [RepeatedStratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold) class.) \n",
    "\n",
    "Another notable cross-validation scheme, especially for fMRI-based decoding analyses, is group-based cross-validation, in which folds are created based on a particular grouping variable. In fMRI-based decoding analyses, this type of cross-validation is often applied to cross-validate models across runs. Specifically, the leave-one-run-out technique is often used, in which a model is fit on all trials except the trials from a single run (the train set) and is cross-validated to the trials of the left-out run (the test set).\n",
    "\n",
    "This functionality is implemented in the `LeaveOneGroupOut` class in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cross-validation object is very similar to the other objects (e.g., `StratifiedKfold`) you have seen, except that when calling the `split` method, you need to provide an additional parameter `groups`, which should be an array/list with integers denoting the different groups:\n",
    "\n",
    "```python\n",
    "folds = logo.split(data, target, groups)\n",
    "```\n",
    "\n",
    "For our dataset, we can create a groups-variable based on the different runs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups =  np.concatenate([[i] * N_per_run for i in range(M)])\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (3 points): In this ToDo, you'll practice with both the Pipeline class as well as leave-on-group-out cross-validation. For this ToDo, you're going to create a new pipeline with a variant of the <tt>StandardScaler</tt> class &mdash; the <tt>RobustScaler</tt> class &mdash; and another model &mdash; the <tt>RidgeClassifier</tt> class (also from the <tt>linear_model</tt> module).\n",
    "    \n",
    "1. Create a new <tt>Pipeline</tt> object with the aforementioned <tt>RobustScaler</tt> and <tt>RidgeClassifier</tt> objects (which you have to import yourself) using the <tt>make_pipeline</tt> function;\n",
    "    \n",
    "2. Use the previously defined <tt>logo</tt> object to create a loop across folds, in which you should cross-validate your entire pipeline and compute each fold's cross-validated accuracy. \n",
    "    \n",
    "3. After the loop, average the four accuracy values and store this in a variable named <tt>acc_cv_logo</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbc4e2f2755f222c2805731167d6463e",
     "grade": false,
     "grade_id": "cell-8a97e3cb79b8eeac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3142993040044ef73861320544f4ef9",
     "grade": true,
     "grade_id": "cell-afdf3dad4c9cb8f1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_2 import test_logo_loop    \n",
    "test_logo_loop(R_all, S_all, logo, groups, acc_cv_logo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): The average performance for the leave-one-run-out cross-validation analysis is, unlike the previous stratified K-fold results (which should be considerably above chance), near chance-level, as you would expect for random noise data. Explain why run-wise cross-validation solves the bias that we observed when ignoring the trial structure across runs. Hint: look at the way we simulated data (which reflects what real data may look like).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6583374fc11c506c7c6033cdb3ac3feb",
     "grade": true,
     "grade_id": "cell-811af32a74ff2367",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance and model performance revisited\n",
    "Before moving on to more exciting aspects of decoding pipelines, let's discuss class imbalance. Class imbalance, the situation in which not all classes within your target variable have the same number of samples, can have a big impact on your classification model (note: this is not relevant for regression models, as they don't have *classes*!). This is not uncommon in decoding models, especially when you don't have control over the feature of interest, such as response-based features (e.g., whether the participant pressed left or right) or between-subjects variables (e.g., when dealing with clinical populations, which cannot always be perfectly balanced due to practical reasons).\n",
    "\n",
    "To illustrate this, let's look at what happens with random data and a imbalanced target variable. We'll simulate some random data (like our previously used data, this contains no \"signal\" at all: you'd expect 50% accuracy) a couple of times and we'll calculate the cross-validated accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate random (there is no effect!) data\n",
    "N = 100  # samples\n",
    "K = 3  # brain features\n",
    "ratio01 = 0.8  # ratio class 0 / class 1\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "iters = 10\n",
    "for i in range(iters):\n",
    "    R_random = np.random.normal(0, 1, size=(N, K))\n",
    "\n",
    "    # Simulate random target with prespecified imbalance ('ratio01')\n",
    "    S0 = np.zeros(int(np.round(N * ratio01)))  # 80% is class 0\n",
    "    S1 = np.ones(int(np.round(N * (1 - ratio01)))) # 20% is class 1\n",
    "    S_random = np.concatenate((S0, S1))\n",
    "\n",
    "    # Now, let's split it in a train- test-set\n",
    "    R_train = R_random[::2, :]\n",
    "    R_test = R_random[1::2, :]\n",
    "    S_train = S_random[::2]\n",
    "    S_test = S_random[1::2]\n",
    "\n",
    "    clf.fit(R_train, S_train)\n",
    "    S_pred = clf.predict(R_test)\n",
    "    this_acc = accuracy_score(S_test, S_pred)\n",
    "    print(\"Accuracy: %.2f\" % this_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the cell above, you should find that the models is able to consistently yield strong above-chance performance, which shouldn't happen because the data that we simulated is just random noise! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (0 points): Try setting the <tt>ratio01</tt> variable to other numbers (in between 0.1 - 1), for example 0.9, 0.2, or 0.5, and see what happens with the cross-validated accuracy score!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (1 point): Suppose that you have a dataset with a binary target variable ($S = \\{0, 1\\}$) a train set in which 80% of the samples are of class 0 and a test set in which 80% of the samples are of class 1. Assuming similar parameters ($N$, $K$, type of classifier, etc.) as used in the previous simulation, what do you think will be the (cross-validated) accuracy on the test set? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f974dd785306c8b6128ecce90333fef",
     "grade": true,
     "grade_id": "cell-89dac125c2dbfc6f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model \"learns\" to just predict the majority class (the class with the most samples)! In a way, class imbalance also provides the classifier with a source of \"information\" that it can use to derive accurate (but theoretically meaningless) predictions. \n",
    "\n",
    "Therefore, imbalanced datasets therefore need a different evaluation metric than accuracy. Fortunately, scikit-learn has many more performance metrics you can use, including metrics that \"correct\" for the (potential) bias due to class imbalance (including [f1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score), [ROC-AUC-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score), [balanced-accuracy score](http://scikit-learn.org/dev/modules/generated/sklearn.metrics.balanced_accuracy_score.html), and [Cohen's Kappa](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score)). \n",
    "\n",
    "Let's check out what happens with our performance estimate if we use a different ('class-imbalance-aware') metric, the \"ROC-AUC-score\" we discussed previously, which should take care of the bias induced by imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(iters):\n",
    "    R_random = np.random.normal(0, 1, size=(N, K))\n",
    "\n",
    "    # Simulate random target with prespecified imbalance ('ratio01')\n",
    "    S0 = np.zeros(int(np.round(N * ratio01)))  # 80% is class 0\n",
    "    S1 = np.ones(int(np.round(N * (1 - ratio01)))) # 20% is class 1\n",
    "    S_random = np.concatenate((S0, S1))\n",
    "\n",
    "    # Now, let's split it in a train- test-set\n",
    "    R_train = R_random[::2, :]\n",
    "    R_test = R_random[1::2, :]\n",
    "    S_train = S_random[::2]\n",
    "    S_test = S_random[1::2]\n",
    "\n",
    "    clf.fit(R_train, S_train)\n",
    "    S_pred = clf.predict(R_test)\n",
    "    this_acc = roc_auc_score(S_test, S_pred)\n",
    "    print(\"Accuracy: %.2f\" % this_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! In addition to `roc_auc_score`, there are other metrics (such as `balanced_accuracy` and `f1_score`) that deal appropriately with class imbalance.\n",
    "\n",
    "However, using class-imbalance-aware metrics only makes sure that the model performance estimate is unbiased (i.e., is not affected by class imbalance), but it doesn't prevent the model from actually \"learning\" the useless \"information\" related to class imbalance (instead of learning the true/useful signal in the data)! In our previous examples which used completely random (null) data, this is not a problem, but it *is* a problem when the data actually contains some effect. One way to counter this is by using the `class_weight` parameter that is available in mode scikit-learn models. Setting the parameter to \"balanced\" will weigh samples from the minority class more strongly than samples from the majority class, forcing the model to learn information that is independent from class frequency. Below, we initialize a logistic regression model with this setting enabled and show that it effectively reduces the influence of class imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "for i in range(iters):\n",
    "    R_random = np.random.normal(0, 1, size=(N, K))\n",
    "\n",
    "    # Simulate random target with prespecified imbalance ('ratio01')\n",
    "    S0 = np.zeros(int(np.round(N * ratio01)))  # 80% is class 0\n",
    "    S1 = np.ones(int(np.round(N * (1 - ratio01)))) # 20% is class 1\n",
    "    S_random = np.concatenate((S0, S1))\n",
    "\n",
    "    # Now, let's split it in a train- test-set\n",
    "    R_train = R_random[::2, :]\n",
    "    R_test = R_random[1::2, :]\n",
    "    S_train = S_random[::2]\n",
    "    S_test = S_random[1::2]\n",
    "\n",
    "    clf.fit(R_train, S_train)\n",
    "    S_pred = clf.predict(R_test)\n",
    "    this_acc = accuracy_score(S_test, S_pred)\n",
    "    print(\"Accuracy: %.2f\" % this_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this will remove all numpy arrays up to this point\n",
    "# from memory\n",
    "%reset -f array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection/extraction\n",
    "Now that we've dicussed cross-validation, which allows you to report an unbiased estimate of generalization performance. However, especially when your data contains many brain features (e.g., voxels), your model might still perform poorly simply because it has a hard time distinguishing signal from noise. One way to \"help\" our model a little is to apply feature selection and/or extraction techniques, which are meant to reduce the number of features to a smaller subset which, hopefully, contain more signal (and less noise).\n",
    "\n",
    "Feature reduction can be achieved in two principled ways:\n",
    "\n",
    "* feature extraction: transform your features into a set of lower-dimensional components;\n",
    "* feature selection: select a subset of features\n",
    "\n",
    "Examples of feature extraction are PCA (i.e. transform voxels to orthogonal components) and averaging features within brain regions from an atlas.\n",
    "\n",
    "Examples of feature selection are ROI-analysis (i.e. restricting your patterns to a specific ROI in the brain) and \"univariate feature selection\" (UFS). This latter method is an often-used data-driven method to select features based upon their univariate difference, which is basically like using a traditional whole-brain mass-univariate analysis to select potentially useful features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (1 point): Suppose a researcher wants to decode gratings with two different orientations from V1. To delineate V1, the subject underwent a retinotopy session in a <em>different</em> fMRI run. The data from this retinotopy session was subsequently used to extract (\"mask\") V1 by excluding non-significant voxels; the significant voxels were in turn used to base the orientation decoding analysis on. Is masking V1 using the retinotopy data a form of <em>feature selection</em> or <em>feature extraction</em>? Why? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17d22fdc3a570f42fa4bb4af7efd1ca9",
     "grade": true,
     "grade_id": "cell-d9fabc6bb06dbf84",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, scikit-learn has a bunch of feature selection/extraction objects for us to use. These objects (\"transformers\" in scikit-learn lingo) work similarly to models (\"estimators\"): they also have a `fit(R, S)` method, in which for example the univariate differences (in UFS) or PCA-components (in PCA-driven feature extraction) are computed. Then, instead of having a `predict(R)` method, transformers have a `transform(R)` method.\n",
    "\n",
    "Before going on, let's actually load in some *real* data. We'll use the data from a single \"face\" run from subject 03. We already estimated the single-trial patterns on Fmriprep-preprocessed data for you using Nilearn (you can check out the code [here](https://github.com/lukassnoek/NI-edu-data/blob/master/code/nipa/estimate_patterns.py)). Let's download these patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(os.path.expanduser('~'), 'NI-edu-data')\n",
    "print(\"Downloading patterns from sub-03, ses-1, run 1 (+- 40 MB) ...\")\n",
    "!aws s3 sync --no-sign-request s3://openneuro.org/ds003965 {data_dir} --exclude \"*\" --include \"derivatives/pattern_estimation/sub-03/ses-1/*task-face*run-1*\"\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just downloaded two nifti files: one ending in `_beta.nii.gz` and one ending in `_varbeta.nii.gz`, which represent the parameter estimates ($\\hat{\\beta}$) of the single-trial model and the _variance_ of the parameter estimates ($\\mathrm{var}[\\hat{\\beta}]$) respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get directory with data\n",
    "pattern_dir = os.path.join(data_dir, 'derivatives', 'pattern_estimation', 'sub-03', 'ses-1', 'patterns')\n",
    "\n",
    "# Check what's in the nibetaseries output directory\n",
    "print('\\n'.join(sorted(os.listdir(pattern_dir))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two files are both 4D nifti files, in which the fourth dimension does not represent time (as you may be used to), but trials! Below, we load them in and print its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image\n",
    "betas_path = os.path.join(pattern_dir, 'sub-03_ses-1_task-face_run-1_space-MNI152NLin2009cAsym_desc-trial_beta.nii.gz')\n",
    "R_4D = image.load_img(betas_path)\n",
    "\n",
    "print(f\"Shape of betas: {R_4D.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is, technically, not yet in the right format: it is formatted as an $X \\times Y \\times Z \\times N$ array ($N$ = number of trials), while we should format our data as a $N \\times K$ array (where $K$ is the product of the $X$, $Y$, and $Z$ dimensions). We could reshape our data ourselves (e.g., by casting the data to a numpy array and using the `reshape` method) but instead, we'll postpone this until the next section.\n",
    "\n",
    "Also, lastly, we need to define a target variable. For the rest of the notebook, we'll try to decode \"smiling\" faces from \"neutral\" faces (the variable \"expression\" in the event files). Note that the event file is included in the `pattern_estimation` directory we just downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load events-file corresponding to run 1\n",
    "events_file = os.path.join(data_dir, 'sub-03', 'ses-1', 'func', 'sub-03_ses-1_task-face_run-1_events.tsv')\n",
    "events = pd.read_csv(events_file, sep='\\t')\n",
    "\n",
    "# Filter out 'response' and 'rating' events\n",
    "events = events.loc[events['trial_type'].str.contains('STIM'), :]\n",
    "\n",
    "# Extract the \"expression\" column (containing either \"smiling\" or \"neutral\")\n",
    "S_expr = events['expression'].to_numpy()\n",
    "\n",
    "# Encode the string labels into integers\n",
    "S_expr = lab_enc.fit_transform(S_expr)\n",
    "S_expr  # smiling = 1, neutral = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): The patterns we just loaded in were already normalized to a common (MNI) template. This is convenient when we want to apply masks derived from atlases (which are often defined in MNI space). But can you also think of an argument for analyzing the data in subject-native space (i.e., data that has not been normalized to a common template)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52c3e8a37d9a0e35c95220196349aa38",
     "grade": true,
     "grade_id": "cell-599970b654c54c2f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI selection\n",
    "An often used method to reduce the number of brain features is to restrict analyses to a particular region-of-interest (ROI), which can either be defined anatomically (e.g., the amygdala) or functionally (e.g., the voxels that activate significantly more to faces than to houses in a separate localizer run). For this section, we'll focus on selecting a subset of voxels based on an anatomical ROI. What ROI we can use depends on the *space* of the data, i.e., whether the data has been normalized to a common template (usually MNI152) or not. Previously, we loaded in data that has been normalized to \"MNI152NLin2009cAsym\" space (the specific MNI flavor used by Fmriprep). As such, we can use ROIs from atlases that are defined in MNI space, such as the Harvard-Oxford atlas, which we can load using Nilearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_atlas_harvard_oxford\n",
    "\n",
    "# ho_atlas is a dictionary with the keys \"labels\" and \"maps\"\n",
    "# We use the subcortical \"maxprob\" atlas, thresholded at 25 probability\n",
    "ho_atlas = fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as you (hopefully) remember from the Nilearn notebook, we can check out which ROIs this atlas contains by looking at the 'labels' key from the atlas object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_atlas['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate ROI selection, let's focus on the right amygdala. To get the index of the right amygdala, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_amygd_idx = ho_atlas['labels'].index('Right Amygdala')\n",
    "print(\"Index of right amygdala:\", r_amygd_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the map stored in the \"maps\" key-value pair in the `ho_map` variable (a `Nifti1Image`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "ho_map = ho_atlas['maps']\n",
    "print(\"Shape of map:\", ho_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there seems to be a problem! The map has more voxels than our data ($73 \\times 86 \\times 66$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of our data:\", R_4D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the Harvard-Oxford atlas is defined in 2 millimeter space, while Fmriprep outputs preprocessed data in the original resolution of the functional data (here: 2.7 mm$^3$). To fix this, we can downsample the map to the resolution of our data using `resample_to_img` from Nilearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image\n",
    "ho_map_resamp = image.resample_to_img(ho_map, R_4D, interpolation='nearest')\n",
    "print(\"Shape of resampled map:\", ho_map_resamp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create an ROI by looking at which voxels in the Harvard-Oxford contain the label corresponding to our right amygdala label (i.e., 20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare map with right amygdala label, creating a boolean array (True, False)\n",
    "r_amygd_roi_bool = ho_map_resamp.get_fdata() == r_amygd_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Nilearn, however, (ROI) masks should be Nifti objects with only two values: ones for voxels included in the mask and zeros for excluded voxels. Our `r_amygd_roi` now has boolean values, `True` and `False`. Let's convert this boolean array to integers (0, 1) and then make it a `Nifti1Image` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method .astype allows you to cast data into a different data type (here: int)\n",
    "# We'll reuse the affine from the `ho_map_resamp` object\n",
    "r_amygd_roi = nib.Nifti1Image(r_amygd_roi_bool.astype(np.int32), affine=ho_map_resamp.affine, dtype=np.int32)\n",
    "print(\"Shape of amygdala ROI:\", r_amygd_roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we also explicitly need to set the `dtype` to `np.int32`. Now, let's plot it using Nilearn to check if everything worked as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "plotting.plot_roi(r_amygd_roi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now, *finally*, we can apply the ROI to our data in the same way we'd apply a mask: using the `apply_mask` function from Nilearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import masking\n",
    "R_ramyg = masking.apply_mask(R_4D, r_amygd_roi)\n",
    "print(\"Shape of indexed data:\", R_ramyg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that took some code, but we finally achieved what we wanted: a subset of voxels for our ROI (128 in total) for all our trials (40), neatly packaged in an $N \\times K$ array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Repeat the ROI indexing process for our data (<tt>R_4D</tt>), but this time using an ROI of the left hippocampus. Store the results (a 2D array) in a variable named <tt>R_lhippocampus</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d864588c1f94a63829c77442ac7c9c79",
     "grade": false,
     "grade_id": "cell-403705b75604648a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c4b64b01626d2e6aa0c1cf2071c3cb9",
     "grade": true,
     "grade_id": "cell-49df40efbcfd5be5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "assert(R_lhippocampus.shape == (40, 258))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "There are many methods for feature extraction, like \"downsampling\" to ROI-averages (i.e. averaging voxel patterns in brain regions) and dimensionality-reduction techniques like PCA. Scikit-learn provides some of these techniques as \"transformer\" objects, which again have a `fit()` and `transform` method. We'll showcase this on the amygdala-restricted patterns we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "pca.fit(R_ramyg)\n",
    "R_ramyg_pca10 = pca.transform(R_ramyg)\n",
    "print(\"Shape after PCA:\", R_ramyg_pca10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'><b>ToDo</b> (2 points): In the previous cell, we fitted the PCA to the entire dataset (all samples in <tt>R_ramyg</tt>), but again, ideally we'd add this a a step to our analysis pipeline and also cross-validate this procedure! Write a completely ) cross-validated decoding pipeline with 5 stratified folds that includes standardization (using <tt>RobustScaler</tt>), PCA dimenensionality reduction (using 10 components), and a logistic regression model which aims to predict expression (smiling vs. neutral). Keep track of the foldwise cross-validated accuracy on the test set and store the average accuracy in a variable named <tt>acc_av_todo</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffdd2003616b31a46f5ed2a7dde46d1d",
     "grade": false,
     "grade_id": "cell-b874619185bcf66c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement the ToDo here.'''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82f22adbd4c6f6ebc6f60441615dae4f",
     "grade": true,
     "grade_id": "cell-d082395e480b1df0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_2 import test_pca_pipe    \n",
    "test_pca_pipe(R_ramyg, S_expr, acc_av_todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region averaging\n",
    "So far, we have restricted our analyses mostly to voxel patterns within a single ROI. For some research questions, however, you might want to include whole-brain patterns (for example, if you believe your experimental feature of interest is represented in whole-brain networks). This means, however, that you might be dealing with a lot of voxels (i.e., columns in your pattern matrix)! Again, you could reduce the number of features using dimensionality reduction techniques such as PCA. An alternative is to not use the activity of *voxels* as features, but the region-average activity! These regions can for example be derived from an existing anatomical atlas or parcellation or even from your own data (by fitting a clustering algorithm on the underlying timeseries).\n",
    "\n",
    "You can do both (using existing atlases/parcellations or creating your own parcellation) using Nilearn. Here, we'll  demonstrate how to \"downsample\" your whole-brain voxel patterns to region-average patterns using the existing [Shaefer et al. (2018)](https://academic.oup.com/cercor/article/28/9/3095/3978804) parcellation, which we can load in using Nilearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
    "# It can return different number of ROIs; we'll set it to 100\n",
    "schaefer_parc = fetch_atlas_schaefer_2018(n_rois=100, resolution_mm=2, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Schaefer parcellation is defined MNI space with a 2 millimeter resolution. Our data has the original BOLD resolution ($2.7 \\times 2.7 \\times 2.97$), so in order to use it on our data, we need to resample it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set interpolation to nearest, because we're dealing with discrete integers\n",
    "schaefer_rois = image.resample_to_img(schaefer_parc['maps'], R_4D, interpolation='nearest')\n",
    "\n",
    "# Let's also plot it:\n",
    "plotting.plot_roi(schaefer_rois);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, unlike the probabilistic ROIs that we used, the Schaefer parcellation is discrete: every voxel belongs to only a single region. As such, the 'maps' map in the `schaef_parc` object is a single volume with integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_ints = schaefer_rois.get_fdata()\n",
    "\n",
    "# Let's check out the unique values in the volume\n",
    "np.unique(roi_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, here, voxels with the value 0 represent background voxels (not belonging to any region). Now, using the `NiftiLabelsMasker` class from Nilearn, we can easly compute the ROI-average activity for each of our samples across the 100 ROIs in the Schaefer parcellation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Read through the <a href=\"https://nilearn.github.io/modules/generated/nilearn.input_data.NiftiLabelsMasker.html#nilearn.input_data.NiftiLabelsMasker\">documentation</a> of the <tt>NiftiLabelsMasker</tt> class. Then, initialize a <tt>NiftiLabelsMasker</tt> object with <tt>labels_img=schaefer_rois</tt> and a \"mean\" <tt>strategy</tt>. Then, using this object, transform our whole-brain 4D patterns (the variable <tt>R_4D</tt>) into a 2D array (shape $40 \\times 100$) and store it in a variable named <tt>R_schaefer</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b34824a92c21e6560a5bd5cfe417405",
     "grade": false,
     "grade_id": "cell-417cd05b30091abe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a61d5f7d012784267f3cc4f4ec5ccf5",
     "grade": true,
     "grade_id": "cell-883786dce8112581",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''    \n",
    "from niedu.tests.nipa.week_2 import test_niftilabelsmasker\n",
    "test_niftilabelsmasker(schaefer_rois, R_4D, R_schaefer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searchlight analysis\n",
    "One last technique that we want to discuss before moving on to statistical testing of decoding results is *searchlight analysis* (although it's not *really* a feature selection/extraction technique). [Searchlight analysis](https://www.pnas.org/content/103/10/3863.short) is a decoding-based analysis that is kind of a hybrid between univariate and multivariate analysis: it analyzes local voxel *patterns* but does that for every location in the brain. It does so by defining a spherical \"searchlight\" of voxels around a particular center voxel (with a particular radius). Within this searchlight, a (cross-validated) decoding analysis is performed and the result, usually a foldwise average model performance score (e.g., accuracy), is assigned to the center voxel. Then, the searchlight moved to the next voxel and a new decoding analysis performed; this process is repeated until each voxel has been the center voxel (and has been assigned a model performance score). As such, at the end of the searchlight analysis, you have an entire volume of (average) model performances, not just a single one!\n",
    "\n",
    "Nilearn contains an efficient implementation of a searchlight analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import SearchLight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (0 points): Read through the <a href='https://nilearn.github.io/modules/generated/nilearn.decoding.SearchLight.html'>documentation</a> of the Searchlight estimator.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like other Nilearn (and scikit-learn) classes, we first need to initialize a `Searchlight` object. Upon initialization, the `Searchlight` class needs a couple of parameters:\n",
    "- `mask_img`: a binary mask that indicates which voxels contain actual brain;\n",
    "- `process_mask_img`: a binary mask that indicates which voxels should be analyzed;\n",
    "- `radius`: radius (in mm) of the searchlight sphere;\n",
    "- `estimator`: a scikit-learn compatible estimator (which can be a classifier/regression model or a `Pipeline`!)\n",
    "- `n_jobs`: how many CPUs to use\n",
    "- `scoring`: a string indicating which scoring method to (e.g., 'accuracy', 'roc_auc', etc.)\n",
    "- `cv`: either an integer (referring to the number of desired folds) or a cross-validation object (e.g., a `StratifiedKFold` object);\n",
    "- `verbose`: whether it should print out stuff while performing the fit (`True` or `False`)\n",
    "\n",
    "Now, let's go through these arguments one by one. First, to determine a brain mask, let's just simply include all voxels that contain at least *some* signal across trials (i.e., where the sum across trials is not 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_mask = image.math_img('(img.sum(axis=3) != 0).astype(np.int32)', img=R_4D)\n",
    "plotting.plot_roi(brain_mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): Why don't the inferior temporal lobes and the orbitofrontal cortex include any signal? Write your explanation in the text box below. Hint: think about the preprocessing tutorials from the previous course! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cb4082426cf23d544fb6b00f2235938",
     "grade": true,
     "grade_id": "cell-91689557bfde17f1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `process_mask_img`, you could of course select the same mask (`brain_mask`), but for this tutorial, that would take too long. Instead, we'll only analyze a single axial slice (the 22nd slice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty volume\n",
    "sl_mask = np.zeros(brain_mask.shape)\n",
    "\n",
    "# Set the 22nd slice to 1\n",
    "sl_mask[:, :, 21] = 1\n",
    "\n",
    "# Create a Nifti1Image object from it using the \"image.new_img_like\" function\n",
    "# from Nilearn\n",
    "sl_mask = image.new_img_like(brain_mask, sl_mask)\n",
    "\n",
    "# Of course, we only want to analyze the voxels from that slice that\n",
    "# are ALSO in the brain mask, so let's intersect them:\n",
    "sl_mask = masking.intersect_masks((brain_mask, sl_mask), threshold=1)\n",
    "\n",
    "# Let's plot it:\n",
    "plotting.plot_roi(sl_mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to create an \"estimator\" and a cross-validation object. Let's go for a simple pipeline with scaling and a `LogisticRegression` classifier and a `StratifiedKFold` CV scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "sl_estimator = make_pipeline(StandardScaler(), LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can initialize the `Searchlight` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = SearchLight(\n",
    "    mask_img=brain_mask,\n",
    "    process_mask_img=sl_mask,\n",
    "    radius=5,  # we'll use a 5 mm radius\n",
    "    estimator=sl_estimator,\n",
    "    n_jobs=1,  # use only 1 core (for your own analyses, you might want to increase this!)\n",
    "    scoring='roc_auc',  # use AUROC as model performance metric\n",
    "    cv=sl_cv,\n",
    "    verbose=True  # print a progressbar while fitting\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually perform the searchlight analysis, we call the `fit` method with our data (a 4D nifti file, with the samples, $N$, in the fourth dimension) and target (the variable it needs to predict). The cell below might take a minute or two to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl.fit(R_4D, S_expr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, the `Searchlight` object's `scores_` attribute contains the voxelwise decoding scores (here: average AUROC scores across the five folds) as a 3D numpy array (with the same shape as our brain mask):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape scores_:\", sl.scores_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert it to a `Nifti1Image` object so that we can plot it using Nilearn. We'll set the (somewhat arbitrarily chosen) threshold to 0.65:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_img = image.new_img_like(brain_mask, sl.scores_)\n",
    "plotting.plot_stat_map(score_img, threshold=0.65);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know more about searchlight analyses, we recommend [this article](https://www.sciencedirect.com/science/article/pii/S1053811913002917?casa_token=R268PUFghLUAAAAA:-BQTf_rGHdG-0HrGdfb23nwzUzyt65oO9cZhwNGfH7DSOAx7ZQGqRwqfhTrzKO7BZWHI-lWEW5I). That said, let's continue with the last part of this tutorial: permutation testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests and permutation testing\n",
    "Suppose you ran your decode pipeline on twenty subjects and accordingly collected a set of twenty model performance scores (e.g., accuracy for a two-class target). Inevitably, some scores will be higher (e.g., 0.9) than others (e.g., 0.7), and some may even be at or below chance level (e.g., 0.45). In the same vein, suppose you ran your decode pipeline across the gray-matter density patterns of a 100 subjects (of which 50 are diagnosed with major depression and 50 are healthy controls) in a 10-fold cross-validation scheme, yielding tne (foldwise) model performance scores. Again, the magnitude of these scores may differ. How do we determine if these scores are, in fact, significantly higher than we'd expect by chance? \n",
    "\n",
    "Significance tests for within-subject decoding analyses can be done in two ways. The \"easy\" way is to simply do a one-sample $t$-test against chance-level (e.g., 0.5 for a two-class target) on the subject-wise model performance scores. For example, suppose that our analyses have produced the following model performance scores (e.g., accuracy, averaged across folds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_scores are the average (across folds) accuracy score\n",
    "acc_scores = np.array([0.5, 0.65, 0.55, 0.7, 0.85, 0.65, 0.9, 0.45, 0.55, 0.50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is reasonable to assume that subjects are independent, we can do a one-sample $t$-test against a $H_{0}$ of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "tvalue, pvalue = ttest_1samp(acc_scores, popmean=0.5)\n",
    "print(\"T-value: %.3f, p-value: %.3f\" % (tvalue, pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd use a significance level ($\\alpha$) of 0.05, this would mean that our result is statistically significant &mdash; yay! This simple approach is, however, technically not a completely valid (random effects) analysis as shown by [Allefeld and colleagues (2016)](https://www.sciencedirect.com/science/article/pii/S1053811916303470?casa_token=XqSuI7NmrQMAAAAA:AKGyrgT-Z12RwzyRSzE7L5cTsezjtSu3VRSKddahRSNDsz4XltgsMp7kjnxlz5avIqsltySRQbw), because a one-sample t-test assumes a symmetric distribution centered at chance-level, but below chance-level model performance *in the population* is impossible. This is like testing commute time of people against a null-hypothesis of 0, which doesn't make sense, because you cannot have a negative commute time! Allefeld and colleagues propose a solution, which they call \"prevalence inference\", which involves intricate permutation testing procedures and is a bit too complicated for this course. If you ever need to evaluate decoding performance at the group-level, though, we highly recommend trying to implement this method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Tip</b>: If you want to know more about prevalence inference and want to see some example code, you can check out <a href=\"https://github.com/lukassnoek/random_notebooks/blob/master/prevalence_inference.ipynb\">this notebook</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That said, let's look at how we'd statistically test the results of *between-subject* analyses. We cannot use a simple one-sample $t$-test for between-subject results, because data points in between-subject analyses &mdash; usually the model performance scores associated with different folds in K-fold CV &mdash; are *not independent*, which is an important assumption of all parametric statistics (such as $t$-tests). Dependence between data points is created because the same samples may be included in different folds. As such, we need a different, non-parameteric approach to evaluate statistical significance: permutation testing.\n",
    "\n",
    "In the previous sections of this tutorial, we worked with within-subject data from a single subject, but for this section, we'll need some between-subject data. For this, we'll use voxel-based morphometry (VBM) data from 47 subjects. We'll use these voxelwise gray matter volume patterns in the bilateral caudate to predict subject gender (male vs. female). Although theoretically meaningless, this allows us to demonstrate permutation testing.\n",
    "\n",
    "First, let's load in and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vbm_analysis_data.npz', 'rb') as f_in:\n",
    "    data = np.load(f_in)\n",
    "    R, S = data['R'], data['S']\n",
    "\n",
    "print(\"R shape:\", R.shape)\n",
    "print(\"S shape:\", S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have data from 47 subjects with 901 voxels from the bilateral caudate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your *observed* performance score\n",
    "Before you start with anything related to permutation testing, you first need to get your performance score that you would like to get a $p$-values for. In between-subject decoding analyses, this is usually the average accuracy (or whatever other metric) across folds. So, when you would implement a 3-fold cross-validation scheme, your observed performance score would be the average of the accuracy-estimates across those 3 folds. You know what? Let's implement exactly that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(solver='lbfgs', class_weight='balanced'))\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# We'll pick 3 folds\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "performance = []\n",
    "for train_idx, test_idx in skf.split(R, S):\n",
    "    R_train, R_test = R[train_idx, :], R[test_idx, :]\n",
    "    S_train, S_test = S[train_idx], S[test_idx]\n",
    "    \n",
    "    pipe.fit(R_train, S_train)\n",
    "    preds = pipe.predict(R_test)\n",
    "    performance.append(accuracy_score(S_test, preds))\n",
    "    \n",
    "observed_acc = np.mean(performance)\n",
    "print(\"Mean accuracy across folds = %.3f\" % observed_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, not bad! 63.6% correct is higher than chance, but is it also *significantly* higher than chance? For that, we need to \"simulate\" a null-distribution through permutation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your *permuted* performance distribution\n",
    "The goal of permutation tests are to create a null-distribution of your observed measure, in this case: (average) classification accuracy. The null-distribution refers to the distribution of classification accuracies that would arise if you would repeat an experiment with noisy data in which there is no effect (i.e. the null-hypothesis is true). \n",
    "\n",
    "Thus, we need to somehow repeat the above \"experiment\" (i.e. the analysis which yielded the observed performance of 51.1%) yet while the null-hypothesis is true (i.e. there is no effect, only noise). Well, as the term \"permutation\" suggest, we can simply randomly shuffly the train-labels (`S_train`) to generate random labels. Now, if we would fit the classifier on {`R_train`, and `S_train`}, then practically it would fit on noise. In fact, by random shuffling of the train-labels, we simulated the scenario in which (on average) the null-hypothesis would be true.\n",
    "\n",
    "So, what we need to do in order to create a null-distribution of classification-accuracies is to run our original analysis (in the code cell above) many times (i.e. \"repeat the experiment\"), yet with random labeling of our train-samples. We expect that the mean of these null-accuracies would center around .5 (assumed chance level), but simply due to random noise, our simulated null-distribution will contain values (sometimes substantially) above and below chance level. \n",
    "\n",
    "Anyway, look at the code-cell below. It contains exactly the same analysis as in the above code-block, yet now it is looped 100 times, and just before fitting (within every fold separately) we shuffle the train-labels to generate a random mapping between `R_train` and `S_train`. After every \"experiment\" (or simply \"permutation\") we average the permuted accuracy scores across folds and store them (in the variable `permuted_accuracies`). Now, let's run the code cell below (may take minute or two):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_permutations = 100\n",
    "permuted_accuracies = np.zeros(n_permutations)\n",
    "\n",
    "# the tqdm thingie will print a nice progressbar below\n",
    "for i in tqdm(range(n_permutations)):\n",
    "\n",
    "    folds = skf.split(R, S)\n",
    "    fold_accuracies = np.zeros(skf.get_n_splits())\n",
    "\n",
    "    for ii, (train_idx, test_idx) in enumerate(folds):\n",
    "    \n",
    "        X_train, X_test = R[train_idx], R[test_idx]\n",
    "        y_train, y_test = S[train_idx], S[test_idx]\n",
    "    \n",
    "        # Here we shuffle the S-train labels!!!\n",
    "        np.random.shuffle(S_train)\n",
    "        \n",
    "        pipe.fit(R_train, S_train)\n",
    "        preds = pipe.predict(R_test)\n",
    "        fold_accuracies[ii] = accuracy_score(S_test, preds)\n",
    "    \n",
    "    permuted_accuracies[i] = np.mean(fold_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we now got our simulated null-distribution values stored in the variable `permuted_accuracies`. Let's look at how the distribution actually looks like (and how the observed accuracy relates to the permuted scores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Permuted null-distribution')\n",
    "plt.hist(permuted_accuracies)\n",
    "plt.xlabel('Average accuracy across folds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(observed_acc, c='r', ls='--')\n",
    "plt.legend(['Observed score'], frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the 'formula' for the $p$-value in a permutation test is as follows: given $P$ permutations, it's the following (in pseudo-notation):\n",
    "\n",
    "\\begin{align}\n",
    "p = \\frac{\\sum_{i=1}^{P}{(\\mathrm{permuted}_{i} \\geq \\mathrm{observed})} + 1}{P+1}\n",
    "\\end{align}\n",
    "\n",
    "where $(\\mathrm{permuted}_{i} \\geq \\mathrm{observed})$ evaluates to 1 if true, otherwise 0. Put differently: the $p$-value, here, expresses the proportion of permutation-values that is equal to or higher than the observed value. Graphically, we can visualize the $p$-value as the area of the (simulated!) null-distribution right of the dotted red line in the figure above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1 point): \n",
    "Calculate the $p$-value corresponding to the observed accuracy given our permuted null-distribution. Store it in a variable named <tt>pval_permutation</tt>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ab7b251f232a97b7327beed431f5360",
     "grade": false,
     "grade_id": "cell-56b9aaafc482e59e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# calculate p-value\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a34ac8641285b524d0b8dfca90ff8c2",
     "grade": true,
     "grade_id": "cell-649fdd5ef820be76",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo.'''\n",
    "from niedu.tests.nipa.week_2 import test_perm_pval\n",
    "test_perm_pval(permuted_accuracies, observed_acc, pval_permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): For this final ToDo, we are going to throw you in at the deep end. Instead of a classification analysis, you are going to implement a regression analysis, in which you'll try to predict 'average attractiveness' (i.e., the attractiveness ratings for our stimuli given by an external set of subjects), which we defined for you already (the variable <tt>S</tt>, below). For the patterns, you can reuse the <tt>R_4D</tt> variable. However, you need to index this with a \"Frontal Orbital Cortex\" ROI from the Harvard-Oxford atlas (also defined below: <tt>ho_atlas</tt>). Note, you have to define the ROI yourself (including resampling to match the resolution of the patterns). <br><br>Then, construct a pipeline with a <tt>StandardScaler</tt> and a <tt>Ridge</tt> regression model (from the <tt>linear_model</tt> module; do not use any specific arguments during initialization). Using a 4-fold cross-validation routine (using the <tt>KFold</tt> class from the <tt>model_selection</tt> module), compute the cross-validated mean squared error (either manually or using the <tt>mean_squared_error</tt> function from the <tt>metrics</tt> module) each iteration. Finally, average the four mean squared error values and store this in a variable named <tt>av_mse</tt>. <br>\n",
    "    \n",
    "A couple of pointers:\n",
    "- You need to import all (new) classes and functions yourself;\n",
    "- Realize that scikit-learn uses a consistent interface for its classes, whether they are regression-related or classification-related (e.g., a regression model has largely the same methods as classification models);\n",
    "    \n",
    "Good luck!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97e3fcc00f952ecfbd3ea7f6d16b4880",
     "grade": false,
     "grade_id": "cell-ff75af2031eba131",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "S = events['average_attractiveness'].values\n",
    "ho_atlas = fetch_atlas_harvard_oxford('cort-maxprob-thr50-2mm')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b50d610ec8757ebefab5dfddb3a5cc5",
     "grade": true,
     "grade_id": "cell-4183b4841959bf25",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_2 import test_regression_todo\n",
    "test_regression_todo(R_4D, events, av_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook discussed the basics of decoding analyses. There are many more topics that we didn't discuss (such as regularization, hyperparameter optimalization, prevalence inference), but hopefully you can get started with implementing your own decoding analyses after having completed this notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "326px",
    "width": "521px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
