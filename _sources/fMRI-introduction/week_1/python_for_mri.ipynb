{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with MRI data in Python\n",
    "In this tutorial we will discuss how to interact with *Nifti* files &mdash; the file format used most in the MRI community &mdash; using the Python package *Nibabel*. We assume that you have experience with basic Python syntax and Numpy and Matplotlib functionality. If not, please consult [this page](https://lukas-snoek.com/NI-edu/other/python_recap.html) for an overview of resources.\n",
    "\n",
    "**What you'll learn**: after this week's lab ... \n",
    "\n",
    "* you know how to load, manipulate, and analyze Nifti-files\n",
    "\n",
    "**Estimated time to complete**: 1-2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is \"nifti\"?\n",
    "Every scanner manufacturer (Philips, Siemens, GE, etc.) has their own proprietary data format. For example, here at the University of Amsterdam, we have a Philips scanner and we often export our data as PAR/REC files. However, to streamline the development and use of neuroimaging software packages, the Neuroimaging InFormatics Technology Initiative came up with a new, standardized format, *nifti*, that most neuroimaging packages should support. As such, usually the first step in any (f)MRI preprocessing pipeline is to convert the scanner-specific files (e.g., PAR/RECs) to nifti. This is beyond the scope of this course, but if at one point you need to do this yourself, we highly recommed the [dcm2niix](https://github.com/rordenlab/dcm2niix) package!\n",
    "\n",
    "Throughout these tutorials you'll work with these nifti-files (also called nifti images), which you can recognize by their extension of *.nii* or its compressed version *.nii.gz*. This file-format is supported by most neuroimaging software packages (such as FSL, which you'll use later in this course!). \n",
    "\n",
    "However, we'd like to inspect and analyze nifti images in Python as well! *Nibabel* is an awesome Python package that allows us to read and load nifti images, and convert them to numpy arrays in a straightforward manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load some other packages we need\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nibabel as nib # common way of importing nibabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in an example anatomical MRI scan (`anat.nii.gz`) from the current directory using nibabel below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_file = 'anat.nii.gz'\n",
    "img = nib.load(mri_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the type of `img`: the `Nifti1Image` class. This is a custom class just like a Numpy array, with its own attributes and methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, one attribute of `Nifti1Image` objects is the `shape` (which is similar to the numpy array attribute with the same name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `shape` attribute is telling us that this is a 3D (anatomical) scan and has 240 voxels in the first dimension, 240 voxels in the second dimension, and 220 voxels in the third dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The three parts of nifti images\n",
    "Nifti images can be roughly divided into three \"parts\":\n",
    "1. The header with metadata;\n",
    "2. The image data;\n",
    "3. The affine matrix\n",
    "\n",
    "All three parts are of course represented in nibabel's `Nifti1Image` class. Let's go through them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The header\n",
    "The header of nifti files contain metadata about the scan, such as the units of measurement, the voxel size, etc. In `Nifti1Images`, the header is an attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we're storing the header attribute in a new variable, hdr, for easy of use\n",
    "hdr = img.header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps confusingly, the header is a custom object (a `Nifti1Header` object) as well, with its own methods and attributes. For example, it has a method called `get_zooms()`, which returns the voxel size (and optionally the sampling rate, if it's a fMRI file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr.get_zooms()  # it's a 1x1x1 mm MRI file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is the `get_xyzt_units` which returns the units of the measurements (here: voxel size in millimeter and time in seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr.get_xyzt_units()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load in a functional MRI file (`func.nii.gz`) to see the difference with an anatomical MRI file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_file = 'func.nii.gz'\n",
    "f_img = nib.load(fmri_file)\n",
    "print(f_img.shape)\n",
    "print(f_img.header.get_zooms())\n",
    "print(f_img.header.get_xyzt_units())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of fMRI files, the fourth dimension (almost) always represents the \"time\" dimension. So you can assume that a nifti image of an fMRI file has 4 dimensions, with the first three being the spatial dimensions (similar to the anatomical MRI file: $X \\times Y \\times Z$) and the last (fourth) being the time dimension ($T$).\n",
    "\n",
    "So for the above file, you can assume that it has 50 timepoints and has a sampling rate of 0.7 seconds (i.e., a new volume was scanned every 0.7 seconds).\n",
    "\n",
    "Moreover, you can infer that this file contains data from $80 \\times 80 \\times 44$ voxels with dimensions $2.7 \\times 2.7 \\times 2.97$ (in millimeters). To be honest, in practice, you won't deal a lot with the header (as you are generally aware of the dimensions/units of your data), so let's look at the *actual* data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "When we loaded in the data and created a `Nifti1Image` object, we actually didn't load in the *actual* data (i.e., voxel intensities) in memory yet! This is postponed because it's quite a memory-intensive operation: remember, loading in an fMRI scan with dimensions $80 \\times 80 \\times 44 \\times 50$ is effectively loading in over 14 million numbers (which is, in this case, more than 50MB in size)! By postponing loading the data, you can, for example, peek at the image dimensions without loading all the data in memory.\n",
    "\n",
    "Anyway, to actually load in the data, you can call the `get_fdata()` method, which will return a numpy array with the same dimensions as the image data. We'll take a look at the anatomical MRI data (`anat.nii.gz`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = img.get_fdata()\n",
    "print(type(img_data))  # it's a numpy array!\n",
    "print(img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so `img_data` is a 3D numpy array with ... what exactly? Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just a bunch of numbers! This is important to remember: **like any image (like your standard png, jpeg, or gif), MRI scans are also just (3D or 4D) arrays of numbers**! The higher the number, the more signal the scanner recorded at that voxel. (It's actually a little more complex than this, but that's beyond the scope of this course!) \n",
    "\n",
    "Often, the absolute value of the signal intensities is not necessarily most or only important thing. The *relative differences* between different voxels in space or within voxels across time is also important. For example, for anatomical scans, apart from a high overall signal intensity, you often want a good *contrast* between white and gray matter (i.e., different signal intensities between voxels in these two tissue types). For functional MRI, apart from a clear tissue contrast and a high overall signal intensity, you often also want little (non-experimentally related) differences in signal intensity within a voxel across time.\n",
    "\n",
    "Anyway, when we printed this 3D array (of $240 \\times 240 \\times 220$), the notebook chose not to display all 12,672,000 numbers but instead truncated it. The reason it only printed zeros is because the first and last couple of numbers in each dimension likely doesn't contain any (signal related to) brain, just empty space!\n",
    "\n",
    "So, let's index a small $3 \\times 3 \\times 3$ patch of voxels in the middle of the brain to check what values these voxels contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_vox = img_data[118:121, 118:121, 108:111]\n",
    "print(mid_vox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! The exact values are not necessarily directly interpretable, i.e., you cannot say that the value 61978.46 is good (or bad), because the exact *scale* of the signal intensities (whether it goes from 0-100 or from 0-100,000) depends on the specific scanner hardware and specific nifti conversion software.\n",
    "\n",
    "Like any image, (f)MRI scans can be visualized by plotting the numbers and assigning colors to the numbers. Often we visualize brain images in brain and white, with higher signal intensities being brighter and lower signal intensities being darker. Importantly, remember that our data here cannot directly be plotted as a (2D) image, because our data (an anatomical scan) is 3D! However, we *can* just plot a single *slice* of the 3D volume, for example, the middle slice of our first voxel axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_slice_x = img_data[119, :, :]\n",
    "print(mid_slice_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use matplotlib to plot this slice as an image using the `imshow` function you're seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the transpose the slice (using the .T attribute).\n",
    "# This is because imshow plots the first dimension on the y-axis and the\n",
    "# second on the x-axis, but we'd like to plot the first on the x-axis and the\n",
    "# second on the y-axis. Also, the origin to \"lower\", as the data was saved in\n",
    "# \"cartesian\" coordinates.\n",
    "plt.imshow(mid_slice_x.T, cmap='gray', origin='lower')\n",
    "plt.xlabel('First axis')\n",
    "plt.ylabel('Second axis')\n",
    "plt.colorbar(label='Signal intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, time to practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1 point): Remember numpy indexing? For this ToDo, extract the second entry from the <em>last</em> dimension of <tt>img_data</tt> and store it in a new variable called <tt>second_slice</tt>. Tip: this variable should have a shape of $240 \\times 240$!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6d96002275ff3c93990e7f3388cdca9",
     "grade": false,
     "grade_id": "cell-8096e7a1b14b0ade",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0abca7789b3f86146f4d584f9005fb18",
     "grade": true,
     "grade_id": "cell-90deb1fc4d7bf3af",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo '''\n",
    "from niedu.tests.nii.week_1 import test_extract_second_slice\n",
    "test_extract_second_slice(img_data, second_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only looked at *structural* (3D) MRI data. This course is mainly about analyzing *functional* data, so let's look at the data from an fMRI scan. We'll use the `Nifti1Image` object `func_img` from before. We'll load the memory with the `get_fdata` method. Note that this takes a little longer than loading in the structural data, because instead of ($240 \\times 240 \\times 220 = $) 12,672,000 datapoints, we're loading in ($80 \\times 80 \\times 44 \\times 347 =$) 97,715,200 datapoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_img_data = f_img.get_fdata()\n",
    "print(f_img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can think about this 4D array as a series of 3D arrays (volumes), in which the fourth dimension represents time (as shown in the image below).\n",
    "<br><br>\n",
    "\n",
    "![img](https://nilearn.github.io/_images/niimgs.jpg)\n",
    "*Source: [nilearn website](https://nilearn.github.io/_images/niimgs.jpg)*\n",
    "\n",
    "These separate 3D arrays (from the first three dimensions) in fMRI files are often called \"volumes\" or \"dynamics\". So, for example, the first volume refers to the first 3D array that was recorded.\n",
    "\n",
    "Just like the anatomical MRI data, the values in the fMRI data just represent numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing a small 3x3x3 volume of voxels from the first timepoint\n",
    "print(f_img_data[38:41, 38:41, 20:23, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can visualize these numbers as images, at least for the first three dimensions, as these represent the spatial dimensions (the fourth dimension represents time). For example, we can visualize a single slice (e.g., $x = 39$) of the first volume ($t = 0$) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_slice_x_fmri = f_img_data[39, :, :, 0]  # x = 39, t = 0\n",
    "print(\"Shape of slice: %s\" % (mid_slice_x_fmri.shape,))\n",
    "\n",
    "plt.imshow(mid_slice_x_fmri.T, cmap='gray', origin='lower')\n",
    "plt.xlabel('First axis')\n",
    "plt.ylabel('Second axis')\n",
    "plt.colorbar(label='Signal intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also look at fMRI data from a different perspective, that is, from the time dimension! For example, we could extract a single voxel's *time series* (i.e., how the signal intensity varies across time) and plot the signal intensity values of that voxel across time. First, let's extract the time series of one particular voxel (e.g., the middle one across all spatial dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_vox_ts = f_img_data[39, 39, 21, :]  # note the \":\", saying: give me ALL the timepoints\n",
    "print(\"Voxel timeseries shape: %s\" % (mid_vox_ts.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we effectively did in the previous cell is to extract the signal intensity at the *same* spatial coordinates ($x=39, y=39, z=21$) across all 347 timepoints. We show this visually in the next cell, where the show the coordinates as a red box across 20 timepoints (not all 347, because that would clutter the notebook too much):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "\n",
    "fig, axes = plt.subplots(ncols=5, nrows=4, figsize=(20, 10))  # 20 timepoints\n",
    "# Loop over the first 20 volumes/timepoints\n",
    "for t, ax in enumerate(axes.flatten()):    \n",
    "    ax.imshow(f_img_data[39, :, :, t].T, cmap='gray', origin='lower')  # index with t!\n",
    "    rect = patches.Rectangle((38, 20), 2, 2, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('t = %i' % t, fontsize=20)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is *really* hard to spot whether the image of the slice is actually different between different time points! To the naked eye, it just seems like the same image. This is because activity fluctations (over time) in fMRI are **very** small - most of the time just 1-3% compared to baseline (average) activity. That's why it's hard to spot activity differences by looking at the pictures alone (without any scaling).\n",
    "\n",
    "It's actually easier to see these time-by-time fluctuations in signal intensity by plotting the time series directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 3))\n",
    "plt.plot(mid_vox_ts, 'o-', ms=4)\n",
    "plt.xlim(-1, mid_vox_ts.size)\n",
    "plt.ylim(38000, 55000)\n",
    "plt.ylabel('Signal intensity', fontsize=15)\n",
    "plt.xlabel('Time (nr. of volumes)', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the blue dots are the signal intensity values (y-axis), plotted across time (x-axis). The blue lines interconnecting the blue dots are just interpolated values in between the different datapoints (the dots) and serve mostly for visualization purposes (it just looks prettier).\n",
    "\n",
    "Time for a ToDo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): From the the variable <tt>f_img_data</tt>, extract the <em>odd</em> timepoints (i.e., 1, 3, 5, etc.) of this 4D nummpy and store it in a new variable called <tt>f_img_data_odd</tt>. This should be of shape $80 \\times 80 \\times 44 \\times 25$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f512d0b31ff3f7cec4d7f765c71e9527",
     "grade": false,
     "grade_id": "cell-087f4842a17387f7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4685ce2bc2da53aea6de08423e526a8",
     "grade": true,
     "grade_id": "cell-93c449262f025ca4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nii.week_1 import test_extract_voxel_timeseries\n",
    "if 'f_img_data_odd' not in dir():\n",
    "    raise ValueError(\"Could not find any variable named 'f_img_odd'; did you spell it correctly?\")\n",
    "\n",
    "test_extract_voxel_timeseries(f_img_data, f_img_data_odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The affine\n",
    "Each nifti file contains, in addition to the (meta)data, als an *affine* matrix, which relates the position of the image coordinates to real word coordinates. This may sound vague (it definitely did to us at first!), so let's sketch a scenario in which the image's affine becomes important.\n",
    "\n",
    "Suppose you were given a nifti file by a colleague (we'll pretend that the `img` variable represents this mysterious nifti file). You have no idea whether it's even a brain scan, so you decide to plot three slices - one from each of the three axes. You pick slice 70 (index 69, because Python is zero-based; first axis), 100 (second axis), and again 100 (third axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].imshow(img_data[69, :, :].T, origin='lower', cmap='gray')\n",
    "ax[0].set_xlabel('Second dim voxel coords.', fontsize=12)\n",
    "ax[0].set_ylabel('Third dim voxel coords', fontsize=12)\n",
    "ax[0].set_title('First dimension, slice nr. 70', fontsize=15)\n",
    "\n",
    "ax[1].imshow(img_data[:, 99, :].T, origin='lower', cmap='gray')\n",
    "ax[1].set_xlabel('First dim voxel coords.', fontsize=12)\n",
    "ax[1].set_ylabel('Third dim voxel coords', fontsize=12)\n",
    "ax[1].set_title('Second dimension, slice nr. 100', fontsize=15)\n",
    "\n",
    "ax[2].imshow(img_data[:, :, 99].T, origin='lower', cmap='gray')\n",
    "ax[2].set_xlabel('First dim voxel coords.', fontsize=12)\n",
    "ax[2].set_ylabel('Second dim voxel coords', fontsize=12)\n",
    "ax[2].set_title('Third dimension, slice nr. 100', fontsize=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so, you figured out it's definitely a (structural) MRI scan.You can clearly see that the first voxel axis represents the sagittal dimension (left &larr;&rarr; right), the second voxel axis represents the coronal dimension (anterior &larr;&rarr; posterior), and the third voxel axis represents the axial dimension (inferior &larr;&rarr; superior).\n",
    "\n",
    "So far, so good!\n",
    "\n",
    "Now, if we were to ask you: \"In the first plot, are we looking at the left or right side of the brain?\" Due to the left-right symmetry of the brain, this is actually impossible to tell! As such, we need a way to tell what is left and right (and anterior/posterior and inferior/superior) in our voxel space - this is what the affine is for! It will tell us, basically, whether \"left\" in our \"voxel space\" is also \"left\" in the \"real world\".\n",
    "\n",
    "Here, the real world space refers to the position of the voxels (in millimeters) *relative to the isocenter of the scanner*. In this world space, the coordinate $(0, 0, 0)$ refers to the isocenter (see image below). Furthermore, nibabel `Nifti1Images` assume that the first axis goes from left to right, the second axis goes posterior to anterior, and the third axis from inferior to superior. A short-hand for this convention is *RAS+*, which basically says that coordinates to the Right, Anterior, and Superior of the isocenter should be positive (+). It follows that coordinates to the left, posterior, and inferior of the isocenter should be negative (-).\n",
    "\n",
    "![img](http://www.grahamwideman.com/gw/brain/fs/coords/FSCoords_RAS01.gif)\n",
    "(Source: Graham Wideman, [link](http://www.grahamwideman.com/gw/brain/fs/coords/fscoords.htm))\n",
    "\n",
    "Alright, but how does the affine help us recover the real word coordinates of our voxels? It turns out you only need a simple matrix operation: for a set of voxel coordinates $(i, j, k)$, appended with a single 1, and an $4 \\times 4$ affine matrix $A$, you can get the real word coordinates (in RAS+) by the dot product (matrix multiplication) of $A$ and $(i, j, k, 1)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} x & y & z & 1 \\end{bmatrix} = A\\begin{bmatrix}\n",
    "           i \\\\\n",
    "           j \\\\\n",
    "           k \\\\\n",
    "           1\n",
    "         \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "So, our image's affine should be a $4 \\times 4$ matrix. Let's see whether that's the case. The affine of a `Nifti1Image` is an attribute named, well, `affine`, and is a 2D numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=3)  # suppresses scientific notation\n",
    "A = img.affine\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose that you want to know whether the sagittal slice from the leftmost plot in the previous figure is on the left side or the right side. So we want to know the real world $x$ coordinate for our voxel coordinate $i=69$. Let's pick the middle voxel index for our other two dimensions ($j=119, k=109$) (we're plotting this 3D coordinate below in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].imshow(img_data[69, :, :].T, origin='lower', cmap='gray')\n",
    "ax[0].set_xlabel('Second dim voxel coords.', fontsize=12)\n",
    "ax[0].set_ylabel('Third dim voxel coords', fontsize=12)\n",
    "ax[0].set_title('First dimension, slice nr. 70', fontsize=15)\n",
    "rect = patches.Rectangle((119, 109), 3, 3, linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax[0].add_patch(rect)\n",
    "\n",
    "ax[1].imshow(img_data[:, 99, :].T, origin='lower', cmap='gray')\n",
    "ax[1].set_xlabel('First dim voxel coords.', fontsize=12)\n",
    "ax[1].set_ylabel('Third dim voxel coords', fontsize=12)\n",
    "ax[1].set_title('Second dimension, slice nr. 100', fontsize=15)\n",
    "rect = patches.Rectangle((69, 109), 3, 3, linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax[1].add_patch(rect)\n",
    "\n",
    "ax[2].imshow(img_data[:, :, 99].T, origin='lower', cmap='gray')\n",
    "ax[2].set_xlabel('First dim voxel coords.', fontsize=12)\n",
    "ax[2].set_ylabel('Second dim voxel coords', fontsize=12)\n",
    "ax[2].set_title('Third dimension, slice nr. 100', fontsize=15)\n",
    "rect = patches.Rectangle((69, 119), 3, 3, linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax[2].add_patch(rect)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the real world coordinate, we need to evaluate the following:\n",
    "\\begin{align}\n",
    "x, y, z, 1 = A\\begin{bmatrix}\n",
    "           69 \\\\\n",
    "           119 \\\\\n",
    "           109 \\\\\n",
    "           1\n",
    "         \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Let's do that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz1 = A @ np.array([69, 119, 109, 1])\n",
    "print(xyz1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the real world $x$ coordinate is 48.472, which means that this coordinate is 48.472 millimeters to the *right* of the isocenter (remember *RAS+*!).\n",
    "\n",
    "So, why is this important, you might ask? Well, actually, for your statistical analysis, it doesn't! The general linear model (GLM), which we'll use as our main statistical workhorse in this course, doesn't care whether a voxel is on the left or right side of the brain. But when it comes to visualization and reporting, it does! Suppose that you find unilateral amygdala activity for a particular experimental condition - you want to know whether this is on the left or right side, both for your figures and for your paper that you might write about it!\n",
    "\n",
    "Moreover, the affine will become important when we'll talk about realignment of different fMRI volumes (in motion correction) and registration of the fMRI volumes to the structural MRI volume or standard space (which might be in different orientations!), which is the topic of week 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1 point):\n",
    "    \n",
    "Suppose that you want to know whether the how much the middle of the image (i.e., the center in voxel coordinates) deviates from the isocenter of the scanner. In other words, you want to check whether the center of the image was in the isocenter of the scanner. You should do this for the <tt>img_data</tt> array!\n",
    "\n",
    "Store these world coordinates in a new array named <tt>xyz_middle</tt>, which should be of size 3 (representing the three coordinates, so make sure to remove the extra <tt>1</tt>). Remember, indexing is Python is zero-based (so the first index is 0).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82c405e5d80a01ed5014c2e4957e9bf2",
     "grade": false,
     "grade_id": "cell-6bdbef7c8b05e4f9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55def6b1d99ad27d178fcf48421b10a5",
     "grade": true,
     "grade_id": "cell-cecf1217053438e0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo ''' \n",
    "from niedu.tests.nii.week_1 import test_affine_middle_coords\n",
    "if len(xyz_middle) != 3:\n",
    "    raise ValueError(\"Did you remove the last 1?\")\n",
    "\n",
    "test_affine_middle_coords(A, xyz_middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (3 points):\n",
    "    \n",
    "Alright, almost done! Let's practice some of the skills that you learned in this tutorial! For this assignment, you goal is to <em>plot an histogram of the average activity value across time for all non-zero voxels in the brain</em> from the <tt>f_img_data</tt> array. So, what you have to do is the following:\n",
    "\n",
    "1. Average the <tt>func_img_data</tt> variable across time (resulting array should by of shape (80, 80, 44)\n",
    "2. Remove all voxels which are zero (use boolean indexing!)\n",
    "3. Make a histogram of all the resulting non-zero voxels (make sure the histogram has 50 bins)\n",
    "\n",
    "Do this in the code-cell below. Also, give the plot sensible labels for the axes. This is a manually graded assignment, so there is no test-cell/feedback.\n",
    "\n",
    "The resulting histogram should show a \"bimodal distribution\" of average activity values &mdash; roughly with a peak around $x = 0$ and a peak round $x = 42,000$. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "289f5e8cca85c6225c0068ce6913fde4",
     "grade": true,
     "grade_id": "cell-00ab5da92a127d0c",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement to ToDo here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that's it! You finished the notebook of this week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Tip</b>: In this notebook, we used several basic Python packages for data processing, analysis, manipulation and plotting (such as numpy, scipy, and matplotlib). We'll make heavy use of this \"stack\" for the rest of the course. Many procedures we'll discuss in the rest of the course (such as fMRI preprocessing and analysis) can also, and more efficiently, be implemented using <a href=\"https://nilearn.github.io/\">Nilearn</a>, a Python package for fast and easy statistical learning on neuroimaging data. This package and its functionality is discussed in detail in week 7.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
