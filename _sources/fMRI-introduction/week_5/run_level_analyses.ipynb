{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run-level analyses\n",
    "In this notebook, we will explain how to aggregate data from different fMRI runs using \"run-level analyses\". We will use both simulated and real data to explain this concept and show how to implement this in Python as well as FSL.\n",
    "\n",
    "If you haven't done the other two notebooks (`linux_and_the_CMD.ipynb` and `first_level_analyses.ipynb` yet), please go through these first.\n",
    "\n",
    "**What you'll learn**: after this lab, you'll be able to ...\n",
    "\n",
    "* set up a run-level model in FSL FEAT\n",
    "\n",
    "**Estimated time needed to complete**: 1-2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run-level analyses\n",
    "More often than not, researchers split their experiment across different fMRI *runs* (a period of continuous fMRI acquisition). These runs may be grouped within a particular *session* (a set of MRI scans within a particular period that the participant is in the scanner) or split across different sessions (e.g., run 1 and 2 are done on day 1, and run 3 and 4 are done on day 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToDo</b> (1 point): Name one reason why researchers often rather split their experiment across different runs than having a single (longer) run. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71e1fd1a01ee698d3de33f95e395f5f7",
     "grade": true,
     "grade_id": "cell-e70c5bf74774fbce",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before, splitting your experiment in different runs (across multiple sessions) generates a new \"level\" within data. If we analyzed each run/session in separate first-level models, we need to somehow aggregate (or average) the effects ($c\\hat{\\beta}$) and their variance ($\\mathrm{var}[c\\hat{\\beta}]$) across these different runs. **If your experiment indeed contains multiple runs, then the next step in the \"summary statistics\" approach will be to combine their results**. \n",
    "\n",
    "Before discussing how the summary statistics approach would be implemented for fMRI data with multiple runs, let's focus on what a \"traditional\" hierarchical/multilevel model for this type of data would look like.\n",
    "\n",
    "### The traditional multilevel model\n",
    "In traditional hierarchical/multilevel (frequentists) GLM models, the data ($y$) and design matrices ($\\mathbf{X}$) across different levels are simply concatenated. For example, for single-subject fMRI data with multiple runs, the signals ($y$) and design matrices ($\\mathbf{X}$) are concatenated in time.\n",
    "\n",
    "We'll show you how this is done for some example data. For example, suppose we have an experimental paradigm with two conditions (\"A\" and \"B\"), which we spread over 4 runs of 200 seconds/volumes (assuming a TR of 1 for simplicity).\n",
    "\n",
    "We'll generate such a signal below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niedu.utils.nii import simulate_signal  # ignore the warning!\n",
    "\n",
    "duration = 200\n",
    "onsets = np.linspace(0, duration, 10, endpoint=False)\n",
    "n_run = 4\n",
    "Xs, ys = [], []\n",
    "for run in range(n_run):  # simulate 4 signals / design matrices\n",
    "    y, X = simulate_signal(\n",
    "        onsets=onsets, conditions=np.tile(['A', 'B'], len(onsets) // 2),\n",
    "        TR=1, duration=duration, params_canon=[2, 0.5], std_noise=2, plot=False, rnd_seed=run\n",
    "    )\n",
    "    ys.append(y)\n",
    "    Xs.append(X[:, :3])  # remove tderivs\n",
    "\n",
    "print(\"Number of signals: %i\" % len(ys))\n",
    "print(\"Size of signal for each run: %i\\n\" % ys[0].size)\n",
    "\n",
    "print(\"Number of design matrices: %i\" % len(Xs))\n",
    "print(\"Shape of design matrix per run: %s\" % (Xs[0].shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the signals from the separate runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 8))\n",
    "for i in range(n_run):\n",
    "    plt.plot(ys[i] + i*10, np.arange(ys[i].size))\n",
    "\n",
    "plt.ylim(ys[i].size, 0)    \n",
    "plt.xticks(np.arange(n_run) * 10, np.arange(1, n_run+1))\n",
    "plt.ylabel(\"Time (sec.)\", fontsize=20)\n",
    "plt.xlabel(\"Run\", fontsize=20)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a \"traditional\" multilevel model, we need to concatenate the signals ($y$) in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate(ys)\n",
    "print(\"Size of signal: %i\" % y.size)\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# Let's plot the concatenated signal (but color-code the runs)\n",
    "plt.figure(figsize=(3, 16))\n",
    "for i, ytmp in enumerate(ys):\n",
    "    t = np.arange(duration * i, duration * (i + 1))\n",
    "    plt.plot(ytmp, t, c=colors[i])\n",
    "\n",
    "plt.ylim(y.size, 0)\n",
    "plt.ylabel(\"Time (sec.)\", fontsize=20)\n",
    "plt.xlabel(\"Signal amplitude (A.U.)\", fontsize=20)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we need to stack the run-wise design matrices, but these are stacked both vertically (in time) and horizontally (as separate predictors). In other words, each predictor (including the intercept!) in each run gets its own column (i.e., predictor) in our multilevel design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(Xs) * duration, len(Xs) * 3))\n",
    "for i in range(len(Xs)):\n",
    "    t = np.arange(duration * i, duration * (i + 1))\n",
    "    X[t, i*3:(i+1)*3] = Xs[i]\n",
    "    \n",
    "# number of columns = number of conditions * number of runs\n",
    "print(\"Shape of concatenated design matrix: %s\" % (X.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's probably easier to understand it if it's plotted. Below, we'll plot the concatenated signal ($y_{all}$) and the concatenated design matrix ($X_{all}$) in a single figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=len(Xs) + 1, figsize=(15, 15), sharey=True, sharex=False)\n",
    "\n",
    "for i, ytmp in enumerate(ys):\n",
    "    t = np.arange(duration * i, duration * (i + 1))\n",
    "    axes[0].plot(ytmp, t, c=colors[i])\n",
    "    axes[0].set_ylim(duration * n_run, 0)\n",
    "    axes[0].spines['right'].set_visible(False)\n",
    "    axes[0].spines['top'].set_visible(False)\n",
    "    \n",
    "    axes[i].grid()\n",
    "\n",
    "    axes[i+1].set_title(r\"$X_{run\\ %i}$\" % (i+1), fontsize=25)\n",
    "    for ii in range(3):\n",
    "        pred = X[:, (i*3)+ii]\n",
    "        t = np.arange(duration * n_run)\n",
    "        axes[i+1].plot(pred + (ii*2), t, c=colors[i], lw=2)\n",
    "    \n",
    "    axes[i+1].spines['right'].set_visible(False)\n",
    "    axes[i+1].spines['top'].set_visible(False)\n",
    "    axes[i+1].set_xticks([0, 2, 4])\n",
    "    axes[i+1].set_xticklabels(\n",
    "        [r'$icept_{%i}$' % (i+1), r'$A_{%i}$' % (i+1),r'$B_{%i}$' % (i+1)], fontsize=15\n",
    "    )\n",
    "\n",
    "plt.figtext(0.61, 0.92, r'$X_{all}$', fontsize=30)\n",
    "axes[0].set_title(r'$y_{all}$', fontsize=30, y=1.045)\n",
    "\n",
    "axes[0].set_ylabel(\"Time (volumes)\", fontsize=20)\n",
    "axes[i+1].grid()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points)\n",
    "    \n",
    "Given the concatenated signal (the variable <tt>y</tt>) and concatenated design matrices (the variable <tt>X</tt>), you can run a single GLM to get the parameters for the different conditions across the four runs (i.e., $icept_{1}$, $icept_{2}$, $A_{1}$, $A_{2}$, $B_{1}$, $B_{2}$, etc.). Do this for our data (using <tt>y</tt> and <tt>X</tt>) and store the estimated parameters (there should be 12) in a new variable named <tt>av_betas</tt>. \n",
    "\n",
    "Then, you can in fact specify a specific contrast vector that gives you the *average* effect (i.e., $\\hat{\\beta}_{A}$ or $\\hat{\\beta}_{B}$). Try to think of which particular contrast (which should also be of size 12) would \"compute\" the average effect. Create a separate contrast vector for the average effect of A (store this in a variable named <tt>cvec_a</tt>) and the average effect of B (store this in a variable named <tt>cvec_b</tt>).\n",
    "\n",
    "Hint: you can of course check whether your contrast-vectors in fact yield the mean effect ($\\hat{\\beta}$) by comparing it to the result when you manually compute the mean!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b9027665db27876b39603833e69bcb2",
     "grade": false,
     "grade_id": "cell-cc4d9ab0e4563317",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# Implement the ToDo here!\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ff35af17186ea98b7da0dd42a6669a2",
     "grade": true,
     "grade_id": "cell-459046a0d27c60e7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from niedu.tests.nii.week_5 import test_ffx_glm\n",
    "test_ffx_glm(X, y, av_betas, cvec_a, cvec_b, n_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the previous ToDo only concerned the estimation of the run-average *effects* (and the contrast), but not the estimation of the *variance of the run-average effect*. The exact way this is computed actually depends on whether you want to use a fixed-effect, random-effect, or mixed-effects approach. We will discuss this later in this notebook. First, let's take a look at how fMRI studies usually deal with multilevel data: using the summary statistics approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The summary statistics approach\n",
    "Most fMRI studies dealing with multilevel data often do not use the traditional multilevel model, but use a slightly different approach: instead of running one single GLM for the concatenated data, it runs a GLM *per* unit within each level and subsequently \"averages\" the contrast estimates using the \"summary statistics\" approach. For example, for multi-run data, a first-level model for each separate run is evaluated (instead of a single, concatenated model). This is also the approach FSL takes.\n",
    "\n",
    "After estimating a first-level model for each run separately, the summary statistics approach uses the results (i.e., $c\\hat{\\beta}$, representing the \"summary statistics\") of the previous level as the \"data-to-be-explained\" (i.e., $y$) within the current level. Using a \"run-level\" GLM with a new design matrix, the data from the previous level can be aggregated as desired, where the results (i.e., $c\\hat{\\beta}$) of the *current* level GLM represent the aggregated data. This is both the case for aggregating results across runs (in run-level analyses) and across subjects (in group-level analyses). The global idea of this summary statistics approach is visualized in the figure below.\n",
    "\n",
    "![](https://docs.google.com/drawings/d/e/2PACX-1vQxCH3WU3nTqFlHUZb49rf9zioivGQ-flVfRpwmXQx7OF5Wm_1T6gFMYQqpqt-NPITNHUaRoVYEREgT/pub?w=1442&h=1168)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put it another way, to aggregate first-level results across runs, we are going to construct another GLM at the \"run-level\". To distinguish components of run-level GLMs from first-level GLMs, we will add a superscript asterisk ($^{*}$) to the run-level GLM components.\n",
    "\n",
    "So, in our run-level GLM, the first-level results ($c\\hat{\\beta}$) become our new dependent variable ($y^{*}$): \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{y}^{*} = c\\hat{\\beta}\n",
    "\\end{align}\n",
    "\n",
    "which is, similar to first-level GLMs, modeled using a (run-level) GLM with a specific run-level design matrix, $\\mathbf{X}^{*}$, and associated run-level parameters, $\\hat{\\beta}^{*}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{y}^{*} = \\mathbf{X}^{*}\\beta^{*} + \\epsilon^{*}\n",
    "\\end{align}\n",
    "\n",
    "These run-level parameters are usually estimated using OLS:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta}^{*} = (\\mathbf{X}^{*T}\\mathbf{X}^{*})^{-1}\\mathbf{X}^{*T}\\mathbf{y}^{*}\n",
    "\\end{align}\n",
    "\n",
    "Importantly, the way you specify the run-level design matrix, $\\mathbf{X}^{*}$, dictates the way the data is aggregated, which we'll come back to later. For now, it is important to understand that the summary statistics approach entails using the results from the previous level ($c\\hat{\\beta}$) as the target/dependent variable ($\\mathbf{y}^{*}$) in the current level.\n",
    "\n",
    "In what follows, we'll explain the process using the previously simulated dataset (with two conditions, \"A\" and \"B\"). First of all, we need to run our \"first-level\" models for the four runs separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): Within a loop, compute the contrast against baseline for both condition \"A\" ($\\beta_{A} > 0$) and condition \"B\" ($\\beta_{B} > 0$) per run and store these results in the pre-allocated <tt>runwise_cb</tt> array, where the first row should represent the $\\beta_{A} > 0$ results and the second row should represent the $\\beta_{B} > 0$ results. \n",
    "\n",
    "Use the variables <tt>Xs</tt> and <tt>ys</tt>, which are both <em>lists</em> of length 4, corresponding to the four runs. Each element in <tt>Xs</tt> contains a $200$ (N) $\\times\\ 2$ (P) array. Each element in <tt>ys</tt> contains a 1D array of size $200$ (N).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257f0ddd9a501fea6dd60b7b40aaf3aa",
     "grade": false,
     "grade_id": "cell-28ea5be45e6bbadd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# array has shape: num contrasts x N_runs\n",
    "runwise_cb = np.zeros((2, len(Xs)))\n",
    "\n",
    "# Implement your loop here, in which you fill the \n",
    "# runwise_cb array with the different \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0f0715d320eddc492657212194082fa",
     "grade": true,
     "grade_id": "cell-410015e34aef5e16",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nii.week_5 import test_ss_ffx_glm\n",
    "test_ss_ffx_glm(Xs, ys, runwise_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you couldn't figure out the previous ToDo, we'll load in the (approximately) correct `runwise_cb` values below, so we can continue the explanation (note that this will overwrite your own answer, and will give an error if you run the test cell above, but during grading we will only use your own implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runwise_cb = np.load('runwise_cb_answer.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so within our summary statistics approach, the $c\\hat{\\beta}$ values will become our new data ($y^{*}$). For now, let's only focus on the condition \"A\" against baseline contrast (the first row in `runwise_cb`). Let's redefine this as `y_rl` (\"y, run-level\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rl = runwise_cb[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined our dependent variable, but what should our run-level design matrix ($\\mathbf{X}^{*}$) be? This, of course, depends on what you are interested in. Often, people simply want to aggregate the results across different runs into a single \"average\" estimate. \n",
    "\n",
    "It turns out that using a vector of ones as your design matrix ($\\mathbf{X}^{*}$) will yield run-level parameter estimates ($\\hat{\\beta}^{*}$) that correspond to the average of your first-level results. So, when you use a vector of ones as your run-level design matrix, $\\mathbf{X}^{*}$ ...\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X}^{*} = \\begin{bmatrix}\n",
    "             1 \\\\\n",
    "             1 \\\\\n",
    "             \\vdots \\\\\n",
    "             1\n",
    "         \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "then the parameter of this run-level GLM will correspond to the average of the first-level results ($c\\hat{\\beta}$):\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{average}(c\\hat{\\beta}) = \\hat{\\beta}^{*} = (\\mathbf{X}^{*T}\\mathbf{X}^{*})^{-1}\\mathbf{X}^{*T}y^{*}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why is this design &mdash; in which the average is just a vector of ones for each run &mdash; modelling the average of the first-level results? Well, let's investigate this using our previously simulated data. Now, we know that our GLM (with a vector of ones) should give us the mean. Let's calculate the mean 'manually' so that we can check later whether the GLM gives us the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_mean = np.mean(y_rl)\n",
    "print(manual_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (1 point): Create a design-matrix of shape $(4, 1)$, in which the single column contains all ones (you can use the <tt>np.ones</tt> function for this!). Then run linear regression using the variable <tt>y_rl</tt> as our target and the using the design-matrix you just created (representing the \"$\\mathbf{X}$\"). Store the resulting parameter-value in a variable named <tt>glm_mean</tt>.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0da04d445b8a5078cd4f776e658b4789",
     "grade": false,
     "grade_id": "cell-c24ac0e52ab1d7d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here.'''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84b5ec78b6285786d5c1c519f774e258",
     "grade": true,
     "grade_id": "cell-accdfbaaff6d02bf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo.'''\n",
    "if isinstance(glm_mean, np.ndarray):\n",
    "    ans = glm_mean[0]\n",
    "else:\n",
    "    ans = glm_mean\n",
    "\n",
    "np.testing.assert_almost_equal(ans, manual_mean)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done the previous ToDo correctly, you've seen that, as expected, the parameter calculated by the GLM when using a predictor with all ones (a run-level intercept, basically) reflects the mean of the dependent-variable. In fact, in the context of the GLM &mdash; which aims to minimize the residuals between the predictor(s) and the target &mdash; this makes sense! We show you this in the plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnge = np.max(y_rl) - np.min(y_rl)\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12, 11), sharey=True, sharex=True)\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(y_rl, np.arange(1, y_rl.size+1), marker='o', ms=12, lw=2)\n",
    "    ax.plot(np.ones(n_run), np.arange(1, n_run+1), ls='--', lw=2)\n",
    "    ax.legend([r'$Y^{*}$', r'$X^{*}$'], fontsize=15, frameon=False)\n",
    "    ax.set_yticks(np.arange(1, n_run+1)[::-1])\n",
    "    ax.set_yticklabels(np.arange(1, n_run + 1), fontsize=20)\n",
    "    ax.grid()\n",
    "    ax.set_xlim(0, np.max(y_rl) + 0.3 * rnge)\n",
    "    ax.set_xlabel(r\"Contrast-values ($c\\beta$)\", fontsize=20)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r\"Runs\", fontsize=20)\n",
    "        ax.set_title(\"Run-level GLM model\", fontsize=25)\n",
    "    else:\n",
    "        ax.set_title(\"Run-level *fitted* GLM model\", fontsize=25)\n",
    "        ax.plot(np.repeat(manual_mean, n_run), np.arange(1, n_run+1), c='tab:orange', lw=3)\n",
    "        for i in range(n_run):\n",
    "            ax.plot((y_rl[i], manual_mean), (i+1, i+1), c='tab:red', ls='--', lw=3)\n",
    "            \n",
    "        ax.annotate(text='', xy=(0, 2.5), xytext=(manual_mean, 2.5), arrowprops=dict(arrowstyle='<->', lw=4))\n",
    "        ax.text(0.5, 2.3, r'$\\hat{\\beta}^{*}$', fontsize=30)\n",
    "        ax.legend([r'$Y^{*}$', r'$X^{*}$', r'$\\hat{Y}^{*} = \\hat{\\beta}^{*}$', r'$residuals^{*}$'],\n",
    "                  fontsize=15, frameon=False, loc='lower left')\n",
    "        \n",
    "fig.tight_layout()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, you can nicely see that the vector of ones nicely models the mean of the dependent variable (the first-level $\\beta_{A} > 0$ contrasts). Just like in first-level models, we can specify particular **run-level** contrasts. Implicitly, we used a **run-level** contrast against baseline here (because for our single run-level predictor, when $c^{*} = [1]$, then $c\\hat{\\beta}^{*} = \\hat{\\beta}^{*}$), but other contrasts are definitely possible (we'll take a look at that later).\n",
    "\n",
    "But what if we want to test more than one *first-level* contrast, eventually, at the group-level? For example, suppose we also want to test $\\beta_{B} > 0$. Should we then just run a *separate* run-level analysis for each first-level contrast? You surely can, but often it's easier to do it in a single model in which we concatenate the contrast-values from the different contrasts. We do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order: A1, A2, A3, A4, B1, B2, B3, B4\n",
    "y_AB = np.concatenate([runwise_cb[0, :], runwise_cb[1, :]])\n",
    "\n",
    "plt.figure(figsize=(8, 15))\n",
    "plt.plot(runwise_cb[0, :], np.arange(n_run), marker='o', ms=12, lw=2)\n",
    "plt.plot(runwise_cb[1, :], np.arange(n_run, n_run * 2), c='tab:green', marker='o', ms=12, lw=2)\n",
    "plt.yticks(np.arange(n_run * 2), ['Run ' + str(i) for i in np.tile(range(1, n_run+1), 2)], fontsize=15)\n",
    "plt.xlim(-1, 5)\n",
    "plt.ylabel(\"Runs\", fontsize=20)\n",
    "plt.xlabel(r\"Contrast-values ($c\\hat{\\beta}$)\", fontsize=20)\n",
    "plt.title(\"Concatenated contrast-values, A and B\", fontsize=25)\n",
    "plt.axvline(0, c='black', ls='--', lw=0.5)\n",
    "plt.legend([r'$Y_{A}$', r'$Y_{B}$'], fontsize=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): Now, given the concatenated data (the variable <tt>y_AB</tt>), can you come up with a design-matrix ($X$) that models both the mean of the contrast-values of A-against-baseline (first four values) and the contrast-values of B-against-baseline (last four values)? Name your design-matrix <tt>X_concat_data</tt> (hint: it needs two columns), which should be a 2D numpy array. Then, run linear regression on the concatenated data using your design-matrix (<tt>X_concat_data</tt>); you can check your answer by verifying that the two parameters from the run-level regression model are the same as the mean across the columns of the <tt>runwise_cb</tt> array.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d92ac1eee8a9d5231b1f23334e6e845c",
     "grade": false,
     "grade_id": "cell-765a8d4b3a629060",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee8f6953ec214cfb04351d29d365d164",
     "grade": true,
     "grade_id": "cell-8b23a3af6197c61a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nii.week_5 import test_concat_design\n",
    "test_concat_design(n_run, X_concat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Right now, we only have the <em>first-level</em> contrasts against baseline to our disposal ($\\beta_{A} > 0$ and $\\beta_{B} > 0$). But suppose that I'm actually interested to see which voxels significantly activate in response to both stimuli across runs, i.e., $(\\beta_{A} > 0)\\ \\&\\ (\\beta_{B} > 0)$. We can actually specify a particular <em>run-level</em> contrast that tests this. Create an array (with size 2) that implements this contrast vector, and store it in a variable named <em>cvec_A_and_B</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f313f893c73d11774d844109a30652bd",
     "grade": false,
     "grade_id": "cell-3aca1ff32da5935a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13bfc42148098bfee053dfc69be23bd7",
     "grade": true,
     "grade_id": "cell-e654754746ad549e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the ToDo above. '''\n",
    "from niedu.tests.nii.week_5 import test_cvec_A_and_B    \n",
    "test_cvec_A_and_B(cvec_A_and_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (optional, bonus point): Suppose that, for some reason, I'm interested in investigating whether there are voxels that respond more strongly to condition \"A\" in the first two sessions than the last two sessions. Given our first-level results (stored in <tt>y_AB</tt>), how should my run-level design matrix ($\\mathbf{X}_{rl}$) and run-level contrast vector look like? For a bonus point, create this matrix and contrast vector below, run linear regression on the run-level data (<tt>y_AB</tt>) using your design matrix, and compute the run-level contrast using your contrast vector. Store the result of this run-level contrast (i.e., a single nubmer) in a variable named <tt>runlevel_cb_optional</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa7e987f7e59eba9e12da3ab6d3f0a2b",
     "grade": false,
     "grade_id": "cell-66285059ba62ea2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement the (optional) ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46c579b21179ef4aa256f98b4568921e",
     "grade": true,
     "grade_id": "cell-8f606a543d7d545c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the ToDo above. '''\n",
    "from niedu.tests.nii.week_5 import test_runlevel_cb_optional\n",
    "test_runlevel_cb_optional(runlevel_cb_optional, y_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed vs. random vs. mixed effects\n",
    "Remember that we told you that hierarchical data, such as multi-subject fMRI data, is often analyzed with mixed models which compute the variance (uncertainty) of the effects using the variance estimates at each level within your model. This specific type of analysis, in which the final variance estimate is computed using variance components across all levels, is often called a **mixed-effects analysis**.\n",
    "\n",
    "In the run-level models so far, though, we only focused on the run-level *effects* ($c\\hat{\\beta}_{rl}$), not their variance! In fact, for run-level analyses, it is common to \"ignore\" the run-level variance and simply average the first-level variance estimates across runs. In other words, this is treating the effect as \"fixed\" across runs and assuming the variance per run is similar (so that these variance estimates can be average). This specific analysis, in which the effect is assumed to be fixed across observations within an analysis level (e.g., across runs), is often called a **fixed-effects analysis**. Just like the mixed-effects analysis, they are all (variations on) the GLM with specific ways in which they estimate the variance component of our effects.\n",
    "\n",
    "In the context of analysis of hierarchical fMRI data, **fixed-effects analyses** at the run-level are completely reasonable (at least, that's what most people do in practice). However, this is not true for the next level in our summary statistics approach: the group-level. Here, you actually need to incorporate the variance component from the group-level in order to draw valid population inferences. When you incorporate this group-level variance component, but ignore the lower-level variance components (i.e., at the first-level or run-level), this type of analysis is not a full mixed-effects analysis, but is often called a **random-effects analysis**. This is the approach that SPM takes (which is fine, given some assumptions). FSL, on the other hand, offers both random-effects as well as a full mixed-effects option, in which the latter is estimated by incorporating both first-level (or run-level) and group-level variance estimates (which approximates the \"proper\" mixed model without the summary statistics procedure). \n",
    "\n",
    "But let's not get distracted (group-level models is the topic of next week!) and continue with this notebook's last section on how to implement run-level (fixed-effects) analyses in FSL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run-level analyses in FSL\n",
    "In FSL, any analysis that is not at the first-level is called a \"higher-level analysis\". In this section, we are going to set up such a higher-level analysis with the goal to average our first-level `flocBLOCKED` results. In particular, we are going to do this specifically for the following contrasts:\n",
    "\n",
    "* $4\\times\\beta_{\\mathbf{face}} - \\beta_{body} - \\beta_{place} - \\beta_{character} - \\beta_{object} > 0$ contrast (this was contrast number 4 in our original first-level analysis)\n",
    "* $4\\times\\beta_{\\mathbf{place}} - \\beta_{body} - \\beta_{face} - \\beta_{character} - \\beta_{object} > 0$ contrast (this was contrast number 5 in our original first-level analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (not graded): Open FEAT and change the type of analysis from \"First-level analysis\" to \"Higher-level analysis\". \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the \"Pre-stats\" and \"Registration\" tabs become unavailable, as FEAT assumes these things have been done in the first-level analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (not graded): First, go to the \"Stats\" tab and change the analysis-type from \"Mixed effects: FLAME 1\" to \"Fixed effects\". \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tell FEAT which data we want to analyze. For the upcoming ToDos, we're going to average the \"face > other\" and \"place > other\" contrasts. The two first-level analyses (one for each run) have been run already and are located here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(os.path.expanduser(\"~\"), 'NI-edu-data')\n",
    "print(\"Starting download of FEAT directory (+- 287MB) ...\\n\")\n",
    "!aws s3 sync --no-sign-request s3://openneuro.org/ds003965 {data_dir} --exclude \"*\" --include \"derivatives/fsl/sub-03/flocBLOCKED/ses-2.feat/*\"\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_r1 = os.path.join(data_dir, 'derivatives', 'fsl', 'sub-03', 'flocBLOCKED', 'ses-1.feat')\n",
    "print(\"The run 1 results are here: %s\" % feat_r1)\n",
    "feat_r2 = os.path.join(data_dir, 'derivatives', 'fsl', 'sub-03', 'flocBLOCKED', 'ses-2.feat')\n",
    "print(\"The run 2 results are here: %s\" % feat_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo</b> (not graded): \n",
    "\n",
    "Change the setting \"Inputs are lower-level FEAT directories\" to \"Inputs are 3D cope images from FEAT directories\". Set the \"Number of inputs\" to 4. Then, press the \"Select cope images\" button and add the different first-level contrast estimate files. The order should be: \"face > other\" (session 1), \"face > other\" (session 2), \"place > other\" (session 1), and \"place > other\" (session 2):\n",
    "\n",
    "\\begin{align}\n",
    "y^{*} = \n",
    "\\begin{bmatrix}\n",
    "c\\hat{\\beta}_{\\mathrm{face>other,\\ session\\ 1}} \\\\\n",
    "c\\hat{\\beta}_{\\mathrm{face>other,\\ session\\ 2}} \\\\\n",
    "c\\hat{\\beta}_{\\mathrm{place>other,\\ session\\ 1}} \\\\\n",
    "c\\hat{\\beta}_{\\mathrm{place>other,\\ session\\ 2}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Check the ToDo in section 4.1.4. to see which contrast (\"COPE\") numbers these first-level contrasts refer to.\n",
    "Set the \"Output directory\" to the \"week 5\" folder. And, under the \"Post-stats\" tab, set the thresholding to \"Uncorrected\" at a threshold at $p < 0.005$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, instead of assuming that \"Inputs are 3D cope images from FEAT directions\", you could use the other option \"Inputs are lower-level FEAT directories\". This option allows you to apply a particular higher-level design ($\\mathbf{X}^{*}$) and higher-level contrasts ($\\mathbf{c}^{*}$) for *all first-level contrasts* at the same time. Here, we don't use this option as it is less clear what is happening \"under the hood\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "<b>Assignment</b> (2 points)<br>\n",
    "\n",
    "In the \"Stats\" tab, click on the \"Full model setup\" button. Now, you can directly specify your design matrix ($\\mathbf{X}^{*}$) here in the \"EVs\" tab. Here, create a design matrix that allow you to average the first-level \"face > other\" and \"place > other\" contrasts within a (single) run-level GLM (hint: you might need to change the \"Number of main EVs\"). Then, go to the \"Contrasts and F-tests\" tab and create the contrasts that you need in order to average the \"face > other\" first-level contrast across runs and the \"place > other\" first-level contrast across runs. Give the EVs and contrasts sensible names.\n",
    "\n",
    "When you're happy with your setup, save your setup in your <tt>week_5</tt> directory; give it the name <tt>setup_feat_runlevel</tt>. Then, add the following line to the text-cell below:\n",
    "\n",
    "`![](setup_feat_runlevel.png)`\n",
    "\n",
    "After adding this line and running the cell, the run-level design should be visible. If it is not, you probably saved the design in the wrong directory.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cfae9cfd7aaa7305da5826fd0266ee2",
     "grade": true,
     "grade_id": "cell-3cf0bba30bf4c3bc",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking out results from run-level FEAT analyses\n",
    "We actually already ran the run-level analysis, of which the results are| downloaded in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting download of runlevel FEAT directory (+- 40MB) ...\\n\")\n",
    "!aws s3 sync --no-sign-request s3://openneuro.org/ds003965 {data_dir} --exclude \"*\" --include \"derivatives/fsl/sub-03/flocBLOCKED/runlevel_Week5.gfeat/*\"\n",
    "print(\"\\nDone!\")\n",
    "\n",
    "runlevel_dir = os.path.join(data_dir, 'derivatives', 'fsl', 'sub-03', 'flocBLOCKED', 'runlevel_Week5.gfeat')\n",
    "print(runlevel_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b>: Open a terminal and navigate to the above <tt>runlevel_Week5.gfeat</tt> directory (using <tt>cd</tt>). Then, print its contents to the terminal window (using <tt>ls</tt>)!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gfeat` suffix stands for \"group FEAT\" and is appended to each higher-level analysis output directory (i.e., any analysis that is not a first-level analysis).\n",
    "\n",
    "In this `gfeat` directory, you should see similar files as you've seen in first-level FEAT directories: HTML-files with analysis summaries, files with design information, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (ungraded): In your terminal, open the <tt>report.html</tt> file in the <tt>gfeat</tt> directory with Firefox, which should open a new Firefox window. Then, click on the \"Registration summary\" tab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The registration tab conveniently shows us the \"functional &rarr; standard space\" registration results again, but now for all four runs at the same time. Here you can easily check for possible large registration differences between runs/sessions. Note that this particular transformation is only executed in higher-level analyses. While the (linear and non-linear) registration parameters are already *computed* in the first-level analysis, they are *applied* in the higher-level analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (ungraded): Can you think of a reason why spatial resampling to standard space is postponed to the higher-level stage?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you scroll further down, you see a figure with the caption \"Sum of all input masks after transformation to standard space\". This shows you how well the functional brain masks (delineating brain from non-brain matter, i.e., the skullstripping result for the different functional files) align. It uses a red-yellow colormap from 1 (red) to 4 (yellow), showing where all masks overlap (yellow) and where this is not the case (relatively red voxels). There are some voxels not included in all masks in orbitofrontal cortex (visible in the first row of brain slices), which might be caused by different amounts of signal dropout across runs, but that isn't much of a problem.\n",
    "\n",
    "The figure below (\"Unique missing-mask voxels\") is a similar quality check, where it plots voxels missing in precisely *one* mask, which allows you to easily spot whether a single registration did not work as expected. Here, everything looks alright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (ungraded): Click on the \"Results\" tab. Note that the image of the design matrix is missing, because we have deleted this (as it would give away the answer to the previous assignment). Now, click on the \"Higher-level FEAT results\" link, which will open the \"post-stats\" tab of the higher-level FEAT analysis. Here, you can see two figures, showing the (thresholded) results of our run-level contrasts ($c\\hat{\\beta}^{*}$).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the `gfeat1` directory, of particular interest is the `cope1.feat` directory. This subdirectory contains the actual *results* of the run-level analysis. The reason this directory is called `cope1.feat` is because the other input option (\"Inputs are lower-level FEAT directories\") allows you to apply the higher-level design to multiple lower-level contrasts at the same time, which would create separate `cope*.feat` subdirectories (`cope1.feat`, `cope2.feat`, `cope3.feat`, etc.). The \"Inputs are 3D cope images from FEAT directories\" option only creates a single directory which is by default called `cope1.feat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (ungraded): In your terminal, navigate into the <tt>cope1.feat</tt> directory and print its contents to the terminal window.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you see that this `cope1.feat` subdirectory has the exact same structure and files as the first-level `.feat` directories that we inspected earlier! \n",
    "\n",
    "Let's view the results in FSLeyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (ungraded): Close any active overlays (Overlay &rarr; Remove all). Then, click on File &rarr; Add standard &rarr; click on <tt>MNI152_T1_2mm_brain.nii.gz</tt>, which is the standard space image that FSL used as the group template. Then, add the <tt>thresh_zstat1.nii.gz</tt> image from the <tt>cope1.feat</tt> directory (i.e., <tt>runlevel.gfeat/cope1.feat/thresh_zstat1.nii.gz</tt>). This file corresponds to the (thresholded) fixed-effects \"average\" effect of our lower-level $\\beta_{face} > 0$ contrasts.\n",
    "\n",
    "Change the colormap to \"Red-Yellow\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing results from run-level analyses is not fundamentally different from first-level analyses, with one exception: the data is now in \"standard space\" (i.e., MNI152, 2mm space). This allows us to use *atlases* to connect the location of our effects to particular anatomical brain regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (ungraded): In the top menu, click on Settings &rarr; Ortho View 1 &rarr; Atlas panel. This should open a new panel in between the Overlay list and the Location panel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Atlases panel by default has to to atlases loaded: the [Harvard-Oxford Cortical Structural atlas](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlases) and the Harvard-Oxford Subcortical Structural atlas, which are based on the individual segmentations of 37 T1-weighted scans. \n",
    "\n",
    "Importantly, these atlases are *probabilistic*, meaning that, for each voxel, they give a *probability* (or, actually, percentage) of belonging to a particular (set of) region(s). The exact percentage is based on the number of people (out of 37) in which that voxel belonged to that region. So, if, for a particular voxel, the atlas shows you \"82% left amygdala\", that means that that voxel was part of the left amygdala for 81% of those 37 people.\n",
    "\n",
    "Consequently, most voxels actually have multiple labels (e.g., 81% left amygdala, 19% left hippocampus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (ungraded): Go to voxel $[25, 39, 25]$ and check out the regions listed in the atlas panel. Do you expect to find this region for this particular contrast/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Tip!</b>\n",
    "    Before handing in your notebooks, we recommend restarting your kernel (<em>Kernel</em> &rarr; <em>Restart & Clear Ouput</em>) and running all your cells again (manually, or by <em>Cell</em> &rarr; <em>Run all</em>). By running all your cells one by one (from \"top\" to \"bottom\" of the notebook), you may spot potential errors that are caused by accidentally overwriting your variables or running your cells out of order (e.g., defining the variable 'x' in cell 28 which you then use in cell 15).\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
