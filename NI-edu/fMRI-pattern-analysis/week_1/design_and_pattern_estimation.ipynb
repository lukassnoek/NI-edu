{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental design and pattern estimation\n",
    "This week's lab will be about the basics of pattern analysis of (f)MRI data. We assume that you've worked through both the `nilearn` and `nilearn_stats` notebooks already. \n",
    "\n",
    "Functional MRI data are most often stored as 4D data, with 3 spatial dimensions ($X$, $Y$, and $Z$) and 1 temporal dimension ($T$). But most pattern analyses assume that data are formatted in 2D: trials ($N$) by patterns (often a subset of $X$, $Y$, and $Z$). Where did the time dimension ($T$) go? And how do we \"extract\" the patterns of the $N$ trials? In this lab, we'll take a look at various methods to estimate patterns from fMRI time series. Because these methods often depend on your experimental design (and your research question, of course), the first part of this lab will discuss some experimental design considerations. After this more theoretical part, we'll dive into how to estimate patterns from fMRI data.\n",
    "\n",
    "**What you'll learn**: At the end of this tutorial, you ...\n",
    "\n",
    "* Understand the most important experimental design factors for pattern analyses;\n",
    "* Understand and are able to implement different pattern estimation techniques\n",
    "\n",
    "**Estimated time needed to complete**: 8-12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to limit the amount of threads numpy can use, otherwise\n",
    "# it tends to hog all the CPUs available when using Nilearn\n",
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental design\n",
    "Before you can do any fancy machine learning or representational similarity analysis (or any other pattern analysis), there are several decisions you need to make and steps to take in terms of study design, (pre)processing, and structuring your data. Roughly, there are three steps to take:\n",
    "\n",
    "1. Design your study in a way that's appropriate to answer your question through a pattern analysis; this, of course, needs to be done *before* data acquisition!\n",
    "2. Estimate/extract your patterns from the (functional) MRI data;\n",
    "3. Structure and preprocess your data appropriately for pattern analyses;\n",
    "\n",
    "While we won't go into all the design factors that make for an *efficient* pattern analysis (see [this article](http://www.sciencedirect.com/science/article/pii/S105381191400768X) for a good review), we will now discuss/demonstrate some design considerations and how they impact the rest of the MVPA pipeline.\n",
    "\n",
    "### Within-subject vs. between-subject analyses\n",
    "As always, your experimental design depends on your specific research question. If, for example, you're trying to predict schizophrenia patients from healthy controls based on structural MRI, your experimental design is going to be different than when you, for example, are comparing fMRI activity patterns in the amygdala between trials targeted to induce different emotions. Crucially, with *design* we mean the factors that you as a researcher control: e.g., which schizophrenia patients and healthy control to scan in the former example and which emotion trials to present at what time. These two examples indicate that experimental design considerations are quite different when you are trying to model a factor that varies *between subjects* (the schizophrenia vs. healthy control example) versus a factor that varies *within subjects* (the emotion trials example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>ToDo/ToThink</b> (1.5 points): before continuing, let's practice a bit. For the three articles below, determine whether they used a within-subject or between-subject design.<br>\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://www.nature.com/articles/nn1444\">https://www.nature.com/articles/nn1444</a> (machine learning based)</li>\n",
    "    <li><a href=\"http://www.jneurosci.org/content/33/47/18597.short\">http://www.jneurosci.org/content/33/47/18597.short</a> (RSA based)</li>\n",
    "    <li><a href=\"https://www.sciencedirect.com/science/article/pii/S1053811913000074\">https://www.sciencedirect.com/science/article/pii/S1053811913000074</a> (machine learning based)</li>\n",
    "</ol>\n",
    "\n",
    "Assign either 'within' or 'between' to the variables corresponding to the studies above (i.e., <tt>study_1</tt>, <tt>study_2</tt>, <tt>study_3</tt>).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93447797607862eb6b87e3f15ffbd82c",
     "grade": false,
     "grade_id": "cell-c994411b786af824",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "''' Implement the ToDo here. '''\n",
    "study_1 = ''  # fill in 'within' or 'between'\n",
    "study_2 = ''  # fill in 'within' or 'between'\n",
    "study_3 = ''  # fill in 'within' or 'between'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f4ba9d477d220fbccf831cd8a21e696",
     "grade": true,
     "grade_id": "cell-db31f8504a15c6cd",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''    \n",
    "for this_study in [study_1, study_2, study_3]:\n",
    "    if not this_study:  # if empty string\n",
    "        raise ValueError(\"You haven't filled in anything!\")\n",
    "    else:\n",
    "        if this_study not in ['within', 'between']:\n",
    "            raise ValueError(\"Fill in either 'within' or 'between'!\")\n",
    "    \n",
    "print(\"Your answer will be graded by hidden tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, while we think it is a useful way to think about different types of studies, it is possible to use \"hybrid\" designs and analyses. For example, you could compare patterns from a particular condition (within-subject) across different participants (between-subject). This is, to our knowledge, not very common though, so we won't discuss it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<b>ToThink</b> (1 point)<br>\n",
    "Suppose a researcher wants to implement a decoding analysis in which he/she aims to predict schizophrenia (vs. healthy control) from gray-matter density patterns in the orbitofrontal cortex. Is this an example of a within-subject or between-subject pattern analysis? Can it be either one? Why (not)? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ecd5dff3386f3c6ab00c2397d6fac70",
     "grade": true,
     "grade_id": "cell-95201d990331344d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That said, let's talk about something that is not only important for univariate MRI analyses, but also for pattern-based multivariate MRI analyses: confounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confounds\n",
    "For most task-based MRI analyses, we try to relate features from our experiment (stimuli, responses, participant characteristics; let's call these $\\mathbf{S}$) to brain features (this is not restricted to \"activity patterns\", see section 1.x; let's call these $\\mathbf{R}$\\*). Ideally, we have designed our experiment that any association between our experimental factor of interest ($\\mathbf{S}$) and brain data ($\\mathbf{R}$) can *only* be due to our experimental factor, not something else. If another factor besides our experimental factor of interest can explain this association, this \"other factor\" may be a *confound* (let's call this $\\mathbf{C}$). If we care to conclude anything about our experimental factor of interest and its relation to our brain data, we should try to minimize any confounding factors in our design. (Note that, in some situations, you may only be interested in maximizing your explanatory/predictive power; in that case, you could argue that confounds are not a problem. See [Hebart & Baker, 2018](https://www.sciencedirect.com/science/article/pii/S1053811917306523) for an excellent overview.)\n",
    "\n",
    "Statistically speaking, you should design your experiment in such a way that there are no associations (correlations) between $\\mathbf{R}$ and $\\mathbf{C}$, such that any association between $\\mathbf{S}$ and $\\mathbf{R}$ can *only* be due to $\\mathbf{R}$. Note that this is not trivial, because this presumes that you (1) know which factors might confound your study and (2) if you know these factors, that they are measured properly.\n",
    "\n",
    "Minimizing confounds in between-subject studies is notably harder than in within-subject designs, especially when dealing with clinical populations that are hard to acquire, because it is simply easier to experimentally control within-subject factors (especially when they are stimulus- rather than response-based). There are ways to deal with confounds post-hoc, but ideally you prevent confounds in the first place. For an overview of confounds in (multivariate/decoding) neuroimaging analyses and a proposed post-hoc correction method, see [this article](https://www.sciencedirect.com/science/article/pii/S1053811918319463) (apologies for the shameless self-promotion) and [this follow-up article](https://www.biorxiv.org/content/10.1101/2020.08.17.255034v1.abstract).\n",
    "\n",
    "In sum, as with *any* (neuroimaging) analysis, a good experimental design is one that minimizes the possibilities of confounds, i.e., associations between factors that are not of interest ($\\mathbf{C}$) and experimental factors that *are* of interest ($\\mathbf{S}$).\n",
    "\n",
    "---\n",
    "\\* Note that the notation for experimental variables ($\\mathbf{S}$) and brain features ($\\mathbf{R}$) is different from what we used in the previous course, in which we used $\\mathbf{X}$ for experimental variables and $\\mathbf{y}$ for brain signals. We did this to conform to the convention to use $\\mathbf{X}$ for the set of independent variables and $\\mathbf{y}$ for dependent variables. In some pattern analyses (such as RSA), however, this independent/dependent variable distintion does not really apply, so that's why we'll stick to the more generic $\\mathbf{R}$ (for brain features) and $\\mathbf{S}$ (for experimental features) terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (0 points): Suppose that you are interested in the neural correlates of ADHD. You want to compare multivariate resting-state fMRI networks between ADHD patients and healthy controls. What is the experimental factor ($\\mathbf{S}$)? And can you think of a factor that, when unaccounted for, presents a major confound ($\\mathbf{C}$) in this study/analysis? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): Suppose that you're interested in the neural representation of \"cognitive effort\". You think of an experimental design in which you show participants either easy arithmetic problems, which involve only single-digit addition/subtraction (e.g., $2+5-4$) or hard(er) arithmetic problems, which involve two-digit addition/subtraction and multiplication (e.g., $12\\times4-2\\times11$), for which they have to respond whether the solution is odd (press left) or even (press right) as fast as possible. You then compare patterns during the between easy and hard trials. What is the experimental factor of interest ($\\mathbf{S}$) here? And what are <em>possible</em> confounds ($\\mathbf{C}$) in this design? Name at least two. (Note: this is a separate hypothetical experimental from the previous ToThink.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "505754d8b6e2e735231e5d47e32cafb3",
     "grade": true,
     "grade_id": "cell-e455edd324498006",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. What makes up a \"pattern\"?\n",
    "So far, we talked a lot about \"patterns\", but what do we mean with that term? There are different options with regard to *what you choose as your unit of measurement* that makes up your pattern. The far majority of pattern analyses in functional MRI use patterns of *activity estimates*, i.e., the same unit of measurement &mdash; relative (de)activation &mdash; as is common in standard mass-univariate analyses. For example, decoding object category (e.g., images of faces vs. images of houses) from fMRI activity patterns in inferotemporal cortex is an example of a pattern analysis that uses *activity estimates* as its unit of measurement. \n",
    "\n",
    "However, you are definitely not limited to using *activity estimates* for your patterns. For example, you could apply pattern analyses to structural data (e.g., patterns of voxelwise gray-matter volume values, like in [voxel-based morphometry](https://en.wikipedia.org/wiki/Voxel-based_morphometry)) or to functional connectivity data (e.g., patterns of time series correlations between voxels, or even topological properties of brain networks). (In fact, the connectivity examples from the Nilearn tutorial represents a way to estimate these connectivity features, which can be used in pattern analyses.) In short, pattern analyses can be applied to patterns composed of *any* type of measurement or metric!\n",
    "\n",
    "Now, let's get a little more technical. Usually, as mentioned in the beginning, pattern analyses represent the data as a 2D array of brain patterns. Let's call this $\\mathbf{R}$. The rows of $\\mathbf{R}$ represent different instances of patterns (sometimes called \"samples\" or \"observations\") and the columns represent different brain features (e.g., voxels; sometimes simply called \"features\"). Note that we thus lose all spatial information by \"flattening\" our patterns into 1D rows!\n",
    "\n",
    "Let's call the number of samples $N$ and the number of brain features $K$. We can thus represent $\\mathbf{R}$ as a $N\\times K$ array:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{R} = \n",
    "\\begin{bmatrix}\n",
    " R_{1,1} & R_{1,2} & R_{1,3} & \\dots & R_{1,K}\\\\\n",
    " R_{2,1} & R_{1,2} & R_{1,3} & \\dots & R_{2,K}\\\\\n",
    " R_{3,1} & R_{1,2} & R_{1,3} & \\dots & R_{3,K}\\\\\n",
    " \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    " R_{N,1} & R_{1,2} & R_{1,3} & \\dots & R_{N,K}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "As discussed before, the values themselves (e.g., $R_{1,1}$, $R_{1,2}$, $R_{3,6}$) represent whatever you chose for your patterns (fMRI activity, connectivity estimates, VBM). What is represented by the rows (samples/observations) of $\\mathbf{R}$ depends on your study design: in between-subject studies, these are usually participants, while in within-subject studies, these samples represent trials (or averages of trials or sometimes runs). The columns of $\\mathbf{R}$ represent the different (brain) features in your pattern; for example, these may be different voxels (or sensors/magnetometers in EEG/MEG), vertices (when working with cortical surfaces), edges in functional brain networks, etc. etc. \n",
    "\n",
    "Let's make it a little bit more concrete. We'll make up some random data below that represents a typical data array in pattern analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 100  # e.g. trials\n",
    "K = 250   # e.g. voxels\n",
    "\n",
    "R = np.random.normal(0, 1, size=(N, K))\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(R, aspect='auto')\n",
    "plt.xlabel('Brain features', fontsize=15)\n",
    "plt.ylabel('Samples', fontsize=15)\n",
    "plt.title(r'$\\mathbf{R}_{N\\times K}$', fontsize=20)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Feature value', fontsize=13, rotation=270, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Extract the pattern of the 42nd trial and store it in a variable called <tt>trial42</tt>. Then, extract the values of 187th brain feature across all trials and store it in a variable called <tt>feat187</tt>. Lastly, extract feature value of the 60th trial and the 221nd feature and store it in a variable called <tt>t60_f221</tt>. Remember: Python uses zero-based indexing (first value in an array is indexed by 0)!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a569de56ac66e62750f8c0e2ddecdcdb",
     "grade": false,
     "grade_id": "cell-e4918f8885c8c4fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement the ToDo here.'''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8bf5bf944d7575bb1aaffc096de2ab6",
     "grade": true,
     "grade_id": "cell-0ae49e26650d93cf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_1 import test_R_indexing\n",
    "test_R_indexing(R, trial42, feat187, t60_f221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, to practice a little bit more. We included whole-brain VBM data for 20 subjects in the `vbm/` subfolder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sorted(os.listdir('vbm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VBM data represents spatially normalized (to MNI152, 2mm), whole-brain voxelwise gray matter volume estimates (read more about VBM [here](https://en.wikipedia.org/wiki/Voxel-based_morphometry)).\n",
    "\n",
    "Let's inspect the data from a single subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "sub_01_vbm_path = os.path.join('vbm', 'sub-01.nii.gz')\n",
    "sub_01_vbm = nib.load(sub_01_vbm_path)\n",
    "print(\"Shape of Nifti file: \", sub_01_vbm.shape)\n",
    "\n",
    "# Let's plot it as well\n",
    "plotting.plot_anat(sub_01_vbm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the VBM data is a 3D array of shape 91 ($X$) $\\times$ 109 ($Y$) $\\times$ 91 ($Z$) (representing voxels). As it is structural (not functional!) data, there is no time dimension ($T$).\n",
    "\n",
    "Now, suppose that we want to do a pattern analysis on the data of all 20 subjects. We should then create a 2D array of shape 20 (subjects) $\\times\\ K$ (number of voxels, i.e., $91 \\times 109 \\times 91$). To do so, we need to create a loop over all files, load them in, \"flatten\" the data, and ultimately stack them into a 2D array. \n",
    "\n",
    "Before you'll implement this as part of the next ToDo, we will show you a neat Python function called `glob`, which allows you to simply find files using \"[wildcards](https://en.wikipedia.org/wiki/Wildcard_character)\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works as follows:\n",
    "\n",
    "```\n",
    "list_of_files = glob('path/with/subdirectories/*/*.nii.gz')\n",
    "```\n",
    "\n",
    "Importantly, the string you pass to `glob` can contain one or more wildcard characters (such as `?` or `*`). Also, *the returned list is not sorted*! Let's try to get all our VBM subject data into a list using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a \"search string\"; we'll use the os.path.join function\n",
    "# to make sure this works both on Linux/Mac and Windows\n",
    "search_str = os.path.join('vbm', 'sub-*.nii.gz')\n",
    "vbm_files = glob(search_str)\n",
    "\n",
    "# this is also possible: vbm_files = glob(os.path.join('vbm', 'sub-*.nii.gz'))\n",
    "\n",
    "# Let's print the returned list\n",
    "print(vbm_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the list is not alphabetically sorted, so let's fix that with the `sorted` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbm_files = sorted(vbm_files)\n",
    "print(vbm_files)\n",
    "# Note that we could have done that with a single statement\n",
    "# vbm_files = sorted(glob(os.path.join('vbm', 'sub-*.nii.gz')))\n",
    "# But also remember: shorter code is not always better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): Create a 2D array with the vertically stacked subject-specific (flattened) VBM patterns, in which the first subject should be the first row. You may want to pre-allocate this array before starting your loop (using, e.g., <tt>np.zeros</tt>). Also, the <tt>enumerate</tt> function may be useful when writing your loop. Try to google how to flatten an N-dimensional array into a single vector. Store the final 2D array in a variable named <tt>R_vbm</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a511e54b7fe57af12d529ed4ef03522",
     "grade": false,
     "grade_id": "cell-366b3bade1d94c8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement the ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a184c99f1760633dc50e1a607dba4e76",
     "grade": true,
     "grade_id": "cell-5f133fb38bed91ea",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_1 import test_R_vbm_loop\n",
    "test_R_vbm_loop(R_vbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Tip</b>: While it is a good exercise to load in the data yourself, you can also easily load in and concatenate a set of Nifti files using Nilearn's <a href=\"https://nilearn.github.io/modules/generated/nilearn.image.concat_imgs.html\">concat_imgs</a> function (which returns a 4D <tt>Nifti1Image</tt>, with the different patterns as the fourth dimension). You'd still have to reorganize this data into a 2D array, though.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after you're done with the ToDo\n",
    "# This will remove the all numpy arrays from memory,\n",
    "# clearing up RAM for the next sections\n",
    "%reset -f array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns as \"points in space\"\n",
    "Before we continue with the topic of pattern estimation, there is one idea that we'd like to introduce: thinking of patterns as points (i.e., coordinates) in space. Thinking of patterns this way is helpful to understanding both machine learning based analyses and representational similarity analysis. While for some, this idea might sound trivial, we believe it's worth going over anyway. Now, let's make this idea more concrete. \n",
    "\n",
    "Suppose we have estimated fMRI activity patterns for 20 trials (rows of $\\mathbf{R}$). Now, we will also assume that those patterns consist of only two features (e.g., voxels; columns of $\\mathbf{R}$), because this will make visualizing patterns as points in space easier than when we choose a larger number of features.\n",
    "\n",
    "Alright, let's simulate and visualize the data (as a 2D array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2  # features (voxels)\n",
    "N = 20  # samples (trials)\n",
    "\n",
    "R = np.random.multivariate_normal(np.zeros(K), np.eye(K), size=N)\n",
    "\n",
    "print(\"Shape of R:\", R.shape)\n",
    "\n",
    "# Plot 2D array as heatmap\n",
    "fig, ax = plt.subplots(figsize=(2, 10))\n",
    "mapp = ax.imshow(R)\n",
    "cbar = fig.colorbar(mapp, pad=0.1)\n",
    "cbar.set_label('Feature value', fontsize=13, rotation=270, labelpad=15)\n",
    "ax.set_yticks(np.arange(N))\n",
    "ax.set_xticks(np.arange(K))\n",
    "ax.set_title(r\"$\\mathbf{R}$\", fontsize=20)\n",
    "ax.set_xlabel('Voxels', fontsize=15)\n",
    "ax.set_ylabel('Trials', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we mentioned that each pattern (row of $\\mathbf{R}$, i.e., $\\mathbf{R}_{i}$) can be interpreted as a \"point in space\". With space, here, we mean a space where each feature (e.g., voxel; column of $\\mathbf{R}$, i.e., $\\mathbf{R}_{j}$) represents a separate axis. In our simulated data, we have two features (e.g., voxel 1 and voxel 2), so our space will have two axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"A two-dimensional space\", fontsize=15)\n",
    "plt.grid()\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.xlabel('Activity voxel 1', fontsize=13)\n",
    "plt.ylabel('Activity voxel 2', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this space, each of our patterns (samples) represents a point. The values of each pattern represent the *coordinates* of its location in this space. For example, the coordinates of the first pattern are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, we can plot this pattern as a point in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"A two-dimensional space\", fontsize=15)\n",
    "plt.grid()\n",
    "\n",
    "# We use the \"scatter\" function to plot this point, but\n",
    "# we could also have used plt.plot(R[0, 0], R[0, 1], marker='o')\n",
    "plt.scatter(R[0, 0], R[0, 1], marker='o', s=75)\n",
    "plt.axhline(0, c='k')\n",
    "plt.axvline(0, c='k')\n",
    "plt.xlabel('Activity voxel 1', fontsize=13)\n",
    "plt.ylabel('Activity voxel 2', fontsize=13)\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do this for all patterns, we get an ordinary scatter plot of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"A two-dimensional space\", fontsize=15)\n",
    "plt.grid()\n",
    "\n",
    "# We use the \"scatter\" function to plot this point, but\n",
    "# we could also have used plt.plot(R[0, 0], R[0, 1], marker='o')\n",
    "plt.scatter(R[:, 0], R[:, 1], marker='o', s=75)\n",
    "plt.axhline(0, c='k')\n",
    "plt.axvline(0, c='k')\n",
    "plt.xlabel('Activity voxel 1', fontsize=13)\n",
    "plt.ylabel('Activity voxel 2', fontsize=13)\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize that both perspectives &mdash; as a 2D array and as a set of points in $K$-dimensional space &mdash; represents the same data! Practically, pattern analysis algorithms usually expect the data as a 2D array, but (in our experience) the operations and mechanisms implemented by those algorithms are easiest to explain and to understand from the \"points in space\" perspective.\n",
    "\n",
    "You might think, \"but how does this work for data with more than two features?\" Well, the idea of patterns as points in space remains the same: each feature represents a new dimension (or \"axis\"). For three features, this means that a pattern represents a point in 3D (X, Y, Z) space; for four features, a pattern represents a point in 4D space (like a point moving in 3D space) ... but what about a pattern with 14 features? Or 500? Actually, this is impossible to visualize or even make sense of mentally. As the famous artificial intelligence researcher Geoffrey Hinton put it:\n",
    "\n",
    "> \"To deal with ... a 14 dimensional space, visualize a 3D space and say 'fourteen' very loudly. Everyone does it.\" (Geoffrey Hinton)\n",
    "\n",
    "The important thing to understand, though, is that most operations, computations, and algorithms that deal with patterns do not care about whether your data is 2D (two features) or 14D (fourteen features) &mdash; we just have to trust the mathematicians that whatever we do on 2D data will generalize to $K$-dimensional data :-)\n",
    "\n",
    "That said, people still try to visualize >2D data using *dimensionality reduction* techniques. These techniques try to project data to a lower-dimensional space. For example, you can transform a dataset with 500 features (i.e., a 500-dimensional dataset) to a 2D dimensional dataset using techniques such as principal component analysis (PCA), Multidimensional Scaling (MDS), and t-SNE. For example, PCA tries to a subset of uncorrelated lower-dimensional features (e.g., 2) from  linear combinations of high-dimensional features (e.g., 4) that still represent as much variance of the high-dimensional components as possible. We'll show you an example below using an implementation of PCA from the machine learning library [scikit-learn](https://scikit-learn.org/stable/), which we'll use extensively in next week's lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Let's create a dataset with 100 samples and 4 features\n",
    "R4D = np.random.normal(0, 1, size=(100, 4))\n",
    "print(\"Shape R4D:\", R4D.shape)\n",
    "\n",
    "# We'll instantiate a PCA object that will\n",
    "# transform our data into 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data from 4D to 2D\n",
    "R2D = pca.fit_transform(R4D)\n",
    "print(\"Shape R2D:\", R2D.shape)\n",
    "\n",
    "# Plot the result\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(R2D[:, 0], R2D[:, 1], marker='o', s=75)\n",
    "plt.axhline(0, c='k')\n",
    "plt.axvline(0, c='k')\n",
    "plt.xlabel('PCA component 1', fontsize=13)\n",
    "plt.ylabel('PCA component 2', fontsize=13)\n",
    "plt.grid()\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (optional): As discussed, PCA is a specific dimensionality reduction technique that uses linear combinations of features to project the data to a lower-dimensional space with fewer \"components\". Linear combinations are simply weighted sums of high-dimensional features. In a 4D dimensional space that is project to 2D, PCA component 1 might be computed as $\\mathbf{R}_{j=1}\\theta_{1}+\\mathbf{R}_{j=2}\\theta_{2}+\\mathbf{R}_{j=3}\\theta_{3}+\\mathbf{R}_{j=4}\\theta_{4}$, where $R_{j=1}$ represents the 4th feature of $\\mathbf{R}$ and $\\theta_{1}$ represents the <em>weight</em> for the 4th feature.\n",
    "    \n",
    "The weights of the fitted PCA model can be accessed by, confusingly, <tt>pca.components_</tt> (shape: $K_{lower} \\times K_{higher}$. Using these weights, can you recompute the lower-dimensional features from the higher-dimensional features yourself? Try to plot it like the figure above and check whether it matches.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e77b4ffa75f43f219fbb92d4c1cd493f",
     "grade": false,
     "grade_id": "cell-3e4692e19e7fe8ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement the (optional) ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that dimensionality reduction is often used for visualization, but it can also be used as a preprocessing step in pattern analyses. We'll take a look this in more detail next week.\n",
    "\n",
    "Alright, back to the topic of pattern extraction/estimation. You saw that preparing VBM data for (between-subject) pattern analyses is actually quite straightforward, but unfortunately, preparing functional MRI data for pattern analysis is a little more complicated. The reason is that we are dealing with time series in which different trials ($N$) are \"embedded\". The next section discusses different methods to \"extract\" (estimate) these trial-wise patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating patterns\n",
    "As we mentioned before, we should prepare our data as an $N$ (samples) $\\times$ $K$ (features) array. With fMRI data, our data is formatted as a $X \\times Y \\times Z \\times T$ array; we can flatten the $X$, $Y$, and $Z$ dimensions, but we still have to find a way to \"extract\" patterns for our $N$ trials from the time series (i.e., the $T$ dimension). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important side note: single trials vs. (runwise) average trials\n",
    "In this section, we often assume that our \"samples\" refer to different *trials*, i.e., single instances of a stimulus or response (or another experimentally-related factor). This is, however, not the only option. Sometimes, researchers choose to treat multiple repetitions of a trial as a single sample or multiple trials within a condition as a single sample. For example, suppose you design a simple passive-viewing experiment with images belonging two one of three conditions: faces, houses, and chairs. Each condition has ten exemplars (face1, face2, ..., face10, house1, house2, ..., house10, chair1, chair2, ... , chair10) and each exemplar/item is repeated six times. So, in total there are 3 (condition) $\\times$ 10 (examplars) $\\times$ 6 (repetitions) = 180 trials. Because you don't want to bore the participant to death, you split the 180 trials into two runs (90 each).\n",
    "    \n",
    "Now, there are different ways to define your samples. One is to treat every single trial as a sample (so you'll have a 180 samples). Another way is to treat each exemplar as a sample. If you do so, you'll have to \"pool\" the pattern estimates across all 6 repetitions (so you'll have $10 \\times 3 = 30$ samples). And yet another way is to treat each condition as a sample, so you'll have to pool the pattern estimates across all 6 repetitions and 10 exemplars per condition (so you'll end up with only 3 samples). Lastly, with respect to the latter two approaches, you may choose to only average repetitions and/or exemplars *within* runs. So, for two runs, you end up with either $10 \\times 3 \\times 2 = 60$ samples (when averaging across repetitions only) or $3 \\times 2 = 6$ samples (when averaging across examplars and repetitions).\n",
    "\n",
    "Whether you should perform your pattern analysis on the trial, examplar, or condition level, and whether you should estimate these patterns across runs or within runs, depends on your research question and analysis technique. For example, if you want to decode exemplars from each other, you obviously should not average across exemplars. Also, some experiments may not have different exemplars per condition (or do not have categorical conditions at all). With respect to the importance of analysis technique: when applying machine learning analyses to fMRI data, people often prefer to split their trials across many (short) runs and &mdash; if using a categorical design &mdash; prefer to estimate a single pattern per run. This is because samples across runs are not temporally autocorrelated, which is an important assumption in machine learning based analyses. Lastly, for any pattern analysis, averaging across different trials will increase the signal-to-noise ratio (SNR) for any sample (because you average out noise), but will decrease the statistical power of the analysis (because you have fewer samples). \n",
    "\n",
    "Long story short: whatever you treat as a sample &mdash; single trials, (runwise) exemplars or (runwise) conditions &mdash; depends on your design, research question, and analysis technique. In the rest of the tutorial, we will usually refer to samples as \"trials\", as this scenario is easiest to simulate and visualize, but remember that this term may equally well refer to (runwise) exemplar-average or condition-average patterns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the issue of estimating patterns from time series a little more concrete, let's simulate some signals. We'll assume that we have a very simple experiment with two conditions (A, B) with ten trials each (interleaved, i.e., ABABAB...AB), a trial duration of 1 second, spaced evenly within a single run of 200 seconds (with a TR of 2 seconds, so 100 timepoints). Note that you are not necessarily limited to discrete categorical designs for all pattern analyses! While for machine learning-based methods (topic of week 2) it is common to have a design with a single categorical feature of interest (or some times a single continuous one),  representional similarity analyses (topic of week 3) are often applied to data with more \"rich\" designs (i.e., designs that include many, often continuously varying, factors of interest). Also, using twenty trials is probably way too few for any pattern analysis, but it'll make the examples (and visualizations) in this section easier to understand. \n",
    "\n",
    "Alright, let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = 2\n",
    "N = 20  # 2 x 10 trials\n",
    "T = 200  # duration in seconds\n",
    "\n",
    "# t_pad is a little baseline at the \n",
    "# start and end of the run\n",
    "t_pad = 10\n",
    "\n",
    "onsets = np.linspace(t_pad, T - t_pad, N, endpoint=False)\n",
    "durations = np.ones(onsets.size)\n",
    "conditions = ['A', 'B'] * (N // 2)\n",
    "\n",
    "print(\"Onsets:\", onsets, end='\\n\\n')\n",
    "print(\"Conditions:\", conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `simulate_signal` function used in the introductory course to simulate the data. This function is like a GLM in reverse: it assumes that a signal ($R$) is generated as a linear combination between (HRF-convolved) experimental features ($\\mathbf{S}$) weighted by some parameters ( $\\beta$ ) plus some additive noise ($\\epsilon$), and simulates the signal accordingly (you can check out the function by running `simulate_signal??` in a new code cell). \n",
    "\n",
    "Because we simulate the signal, we can use \"ground-truth\" activation parameters ( $\\beta$ ). In this simulation, we'll determine that the signal responds more strongly to trials of condition A ($\\beta = 0.8$) than trials of condition B ($\\beta = 0.2$) in *even* voxels (voxel 0, 2, etc.) and vice versa for *odd* voxels (voxel 1, 3, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_even = np.array([0.8, 0.2])\n",
    "params_odd = 1 - params_even"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (0 points): Given these simulation parameters, how do you think that the corresponding $N\\times K$ pattern array ($\\mathbf{R}$) would roughly look like visually (assuming an efficient pattern estimation method)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, We simulate some data for, let's say, four voxels ($K = 4$). (Again, you'll usually perform pattern analyses on many more voxels.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niedu.utils.nii import simulate_signal\n",
    "K = 4\n",
    "\n",
    "ts = []\n",
    "for i in range(K):\n",
    "\n",
    "    # Google \"Python modulo\" to figure out\n",
    "    # what the line below does!\n",
    "    is_even = (i % 2) == 0\n",
    "    \n",
    "    sig, _ = simulate_signal(\n",
    "        onsets,\n",
    "        conditions,\n",
    "        duration=T,\n",
    "        plot=False,\n",
    "        std_noise=0.25,\n",
    "        params_canon=params_even if is_even else params_odd\n",
    "    )\n",
    "\n",
    "    ts.append(sig[:, np.newaxis])\n",
    "\n",
    "# ts = timeseries\n",
    "ts = np.hstack(ts)\n",
    "print(\"Shape of simulated signals: \", ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot these voxels. We'll show the trial onsets as arrows (red = condition A, orange = condition B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(ncols=K, sharex=True, sharey=True, figsize=(10, 12))\n",
    "t = np.arange(ts.shape[0])\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # Plot signal\n",
    "    ax.plot(ts[:, i], t, marker='o', ms=4, c='tab:blue')\n",
    "    # Plot trial onsets (as arrows)\n",
    "    for ii, to in enumerate(onsets):\n",
    "        color = 'tab:red' if ii % 2 == 0 else 'tab:orange'\n",
    "        ax.arrow(-1.5, to / TR, dy=0, dx=0.5, color=color, head_width=0.75, head_length=0.25)\n",
    "\n",
    "    ax.set_xlim(-1.5, 2)\n",
    "    ax.set_ylim(0, ts.shape[0])\n",
    "    ax.grid(b=True)\n",
    "    ax.set_title(f'Voxel {i+1}', fontsize=15)\n",
    "    ax.invert_yaxis()\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Time (volumes)\", fontsize=20)\n",
    "    \n",
    "# Common axis labels\n",
    "fig.text(0.425, -.03, \"Activation (A.U.)\", fontsize=20)\n",
    "fig.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Tip</b> Matplotlib is a very flexible plotting package, but arguably at the expense of how fast you can implement something. <a href=\"https://seaborn.pydata.org/\">Seaborn</a> is a great package (build on top of Matplotlib) that offers some neat functionality that makes your life easier when plotting in Python. For example, we used the <tt>despine</tt> function to remove the top and right spines to make our plot a little nicer. In this course, we'll mostly use Matplotlib, but we just wanted to make you aware of this awesome package.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we can start discussing methods for pattern estimation! Unfortunately, as pattern analyses are relatively new, there no concensus yet about the \"best\" method for pattern estimation. In fact, there exist many different methods, which we can roughly divided into two types:\n",
    "\n",
    "1. Timepoint-based method (for lack of a better name) and\n",
    "2. GLM-based methods\n",
    "\n",
    "We'll discuss both of them, but spend a little more time on the latter set of methods as they are more complicated (and are more popular)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Timepoint-based methods\n",
    "Timepoint-based methods \"extract\" patterns by simply using a single timepoint (e.g., 6 seconds after stimulus presentation) or (an average of) multiple timepoints (e.g., 4, 6, and 8 seconds after stimulus presentation). \n",
    "\n",
    "Below, we visualize how a single-timepoint method would look like (assuming that we'd want to extract the timepoint 6 seconds after stimulus presentation, i.e., around the assumed peak of the BOLD response). The stars represent the values that we would extract (red when condition A, orange when condition B). Note, we only plot the first 60 volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, sharex=True, sharey=True, figsize=(10, 12))\n",
    "t_fmri = np.linspace(0, T, ts.shape[0], endpoint=False)\n",
    "t = np.arange(ts.shape[0])\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # Plot signal\n",
    "    ax.plot(ts[:, i], t, marker='o', ms=4, c='tab:blue')\n",
    "    # Plot trial onsets (as arrows)\n",
    "    for ii, to in enumerate(onsets):\n",
    "        plus6 = np.interp(to+6, t_fmri, ts[:, i])\n",
    "        color = 'tab:red' if ii % 2 == 0 else 'tab:orange'\n",
    "        ax.arrow(-1.5, to / TR, dy=0, dx=0.5, color=color, head_width=0.75, head_length=0.25)\n",
    "        ax.plot([plus6, plus6], [(to+6) / TR, (to+6) / TR], marker='*', ms=15, c=color)\n",
    "    \n",
    "    ax.set_xlim(-1.5, 2)\n",
    "    ax.set_ylim(0, ts.shape[0] // 2)\n",
    "    ax.grid(b=True)\n",
    "    ax.set_title(f'Voxel {i+1}', fontsize=15)\n",
    "    ax.invert_yaxis()\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Time (volumes)\", fontsize=20)\n",
    "\n",
    "# Common axis labels\n",
    "fig.text(0.425, -.03, \"Activation (A.U.)\", fontsize=20)\n",
    "fig.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, extracting these timepoints 6 seconds after stimulus presentation is easy when this timepoint is a multiple of the scan's TR (here: 2 seconds). For example, to extract the value for the first trial (onset: 10 seconds), we simply take the 8th value in our timeseries, because $(10 + 6) / 2 = 8$. But what if our trial onset + 6 seconds is *not* a multiple of the TR, such as with trial 2 (onset: 19 seconds)? Well, we can interpolate this value! We will use the same function for this operation as we did for slice-timing correction (from the previous course): `interp1d` from the `scipy.interpolate` module.\n",
    "\n",
    "To refresh your memory: this function takes the timepoints associated with the values (or \"frame_times\" in Nilearn lingo) and the values itself to generate a new object which we'll later use to do the actual (linear) interpolation. First, let's define the timepoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fmri = np.linspace(0, T, ts.shape[0], endpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): The above timepoints assume that all data was acquired at the onset of the volume acquisition ($t=0$, $t=2$, etc.). Suppose that we actually slice-time corrected our data to the middle slice, i.e., the 18th slice (out of 36 slices) &mdash; create a new array (using <tt>np.linspace</tt> with timepoints that reflect these slice-time corrected acquisition onsets) and store it in a variable named <tt>t_fmri_middle_slice</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0db00645cf0b9141d64c1ee3faa0b00",
     "grade": false,
     "grade_id": "cell-641985c85cc5e0a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0833535ea69ac5bcd48cabec53b51c63",
     "grade": true,
     "grade_id": "cell-36a2dcf33ee302ab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_1 import test_frame_times_stc\n",
    "test_frame_times_stc(TR, T, ts.shape[0], t_fmri_middle_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's assume that all data was actually acquired at the start of the volume ($t=0$, $t=2$, etc.). We can \"initialize\" our interpolator by giving it both the timepoints (`t_fmri`) and the data (`ts`). Note that `ts` is not a single time series, but a 2D array with time series for four voxels (across different columns). By specifying `axis=0`, we tell `interp1d` that the first axis represents the axis that we want to interpolate later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "interpolator = interp1d(t_fmri, ts, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can give the `interpolator` object any set of timepoints and it will return the linearly interpolated values associated with these timepoints for all four voxels. Let's do this for our trial onsets plus six seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onsets_plus_6 = onsets + 6\n",
    "R_plus6 = interpolator(onsets_plus_6)\n",
    "print(\"Shape extracted pattern:\", R_plus6.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 10))\n",
    "mapp = ax.imshow(R_plus6)\n",
    "cbar = fig.colorbar(mapp)\n",
    "cbar.set_label('Feature value', fontsize=13, rotation=270, labelpad=15)\n",
    "ax.set_yticks(np.arange(N))\n",
    "ax.set_xticks(np.arange(K))\n",
    "ax.set_title(r\"$\\mathbf{R}$\", fontsize=20)\n",
    "ax.set_xlabel('Voxels', fontsize=15)\n",
    "ax.set_ylabel('Trials', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we have extracted our first pattern! Does it look like what you expected given the known mean amplitude of the trials from the two conditions ($\\beta_{A,even} = 0.8, \\beta_{B,even}=0.2$ and vice versa for odd voxels)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (3 points): An alternative to the single-timepoint method is to extract, per trial, the <em>average</em> activity within a particular time window, for example 5-7 seconds post-stimulus. One way to do this is by perform interpolation in steps of (for example) 0.1 within the 5-7 post-stimulus time window (i.e., $5.0, 5.1, 5.2, \\dots , 6.8, 6.9, 7.0$) and subsequently averaging these values, per trial, into a single activity estimate. Below, we defined these different steps (<tt>t_post_stimulus</tt>) for you already. Use the <tt>interpolator</tt> object to extract the timepoints for these different post-stimulus times relative to our onsets (<tt>onsets</tt> variable) from our data (<tt>ts</tt> variable). Store the extracted patterns in a new variable called <tt>R_av</tt>.\n",
    "    \n",
    "Note: this is a relatively difficult ToDo! Consider skipping it if it takes too long.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca16d3e3c81441425ec69fdc6241caa2",
     "grade": false,
     "grade_id": "cell-552b0b7c51800f2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "t_post_stimulus = np.linspace(5, 7, 21, endpoint=True)\n",
    "print(t_post_stimulus)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ddb3d1e2408aed2a9bba2708e4b6887",
     "grade": true,
     "grade_id": "cell-536cfeae3051009e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_1 import test_average_extraction\n",
    "test_average_extraction(onsets, ts, t_post_stimulus, interpolator, R_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These timepoint-based methods are relatively simple to implement and computationally efficient. Another variation that you might see in the literature is that extracted (averages of) timepoints are baseline-subtracted ($\\mathbf{R}_{i} - \\mathrm{baseline}_{i}$) or baseline-normalized ($\\frac{\\mathbf{R}_{i}}{\\mathrm{baseline}_{i}}$), where the baseline is usually chosen to be at the stimulus onset or a small window before the stimulus onset. This technique is, as far as we know, not very popular, so we won't discuss it any further in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM-based methods\n",
    "One big disadvantage of timepoint-based methods is that it cannot disentangle activity due to different sources (such as trials that are close in time), which is a major problem for fast (event-related) designs. For example, if you present a trial at $t=10$ and another at $t=12$ and subsequently extract the pattern six seconds post-stimulus (at $t=18$ for the second trial), then the activity estimate for the second trial is definitely going to contain activity due to the first trial because of the sluggishness of the HRF. \n",
    "\n",
    "As such, nowadays GLM-based pattern estimation techniques, which *can* disentangle the contribution of different sources, are more popular than timepoint-based methods. (Although, technically, you can use timepoint-based methods using the GLM with FIR-based designs, but that's beyond the scope of this course.) Again, there are multiple flavors of GLM-based pattern estimation, of which we'll discuss the two most popular ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least-squares all (LSA)\n",
    "The most straightforward GLM-based pattern estimation technique is to fit a single GLM with a design matrix that contains one or more regressors for each sample that you want to estimate (in addition to any confound regressors). The estimated parameters ($\\hat{\\beta}$) corresponding to our samples from this GLM &mdash; representing the relative (de)activation of each voxel for each trial &mdash; will then represent our patterns! \n",
    "\n",
    "This technique is often reffered to as \"least-squares all\" (LSA). Note that, as explained before, a sample can refer to either a single trial, a set of repetitions of a particuar exemplar, or even a single condition. For now, we'll assume that samples refer to single trials. Often, each sample is modelled by a single (canonical) HRF-convolved regressor (but you could also use more than one regressor, e.g., using a basis set with temporal/dispersion derivatives or a FIR-based basis set), so we'll focus on this approach.\n",
    "\n",
    "Let's go back to our simulated data. We have a single run containing 20 trials, so ultimately our design matrix should contain twenty columns: one for every trial. We can use the `make_first_level_design_matrix` function from Nilearn to create the design matrix. Importantly, we should make sure to give a separate and unique \"trial_type\" values for all our trials. If we don't do this (e.g., set trial type to the trial condition: \"A\" or \"B\"), then Nilearn won't create separate regressors for our trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "\n",
    "# We have to create a dataframe with onsets/durations/trial_types\n",
    "# No need for modulation!\n",
    "events_sim = pd.DataFrame(onsets, columns=['onset'])\n",
    "events_sim.loc[:, 'duration'] = 1\n",
    "events_sim.loc[:, 'trial_type'] = ['trial_' + str(i).zfill(2) for i in range(1, N+1)]\n",
    "\n",
    "# lsa_dm = least squares all design matrix\n",
    "lsa_dm = make_first_level_design_matrix(\n",
    "    frame_times=t_fmri,  # we defined this earlier for interpolation!\n",
    "    events=events_sim,\n",
    "    hrf_model='glover',\n",
    "    drift_model=None  # assume data is already high-pass filtered\n",
    ")\n",
    "\n",
    "# Check out the created design matrix\n",
    "# Note that the index represents the frame times\n",
    "lsa_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the design matrix contains 21 regressors: 20 trialwise regressors and an intercept (the last column). Let's also plot it using Nilearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_design_matrix\n",
    "plot_design_matrix(lsa_dm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, while we're at it, plot it as time series (rather than a heatmap):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "for i in range(lsa_dm.shape[1]):\n",
    "    ax.plot(i + lsa_dm.iloc[:, i], np.arange(ts.shape[0]))\n",
    "\n",
    "ax.set_title(\"LSA design matrix\", fontsize=20)\n",
    "ax.set_ylim(0, lsa_dm.shape[0]-1)\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticks(np.arange(N+1))\n",
    "ax.set_xticklabels(['trial ' +  str(i+1) for i in range(N)] + ['icept'], rotation=-90)\n",
    "ax.invert_yaxis()\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"Time (volumes)\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo/ToThink</b> (2 points): One \"problem\" with LSA-type design matrices, especially in fast event-related designs, is that they are not very statistically <em>efficient</em>, i.e., they lead to relatively high variance estimates of your parameters ($\\hat{\\beta}$), mainly due to relatively high predictor variance. Because we used a fixed inter-trial interval (here: 9 seconds), the correlation between \"adjacent\" trials are (approximately) the same. <br>\n",
    "    \n",
    "Compute the correlation between, for example, the predictors associated with trial 1 and trial 2, using the <tt>pearsonr</tt> function imported below, and store it in a variable named <tt>corr_t1t2</tt> (1 point). Then, try to think of a way to improve the efficiency of this particular LSA design and write it down in the cell below the test cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "026c185c2f7535fb44a670d775208e05",
     "grade": false,
     "grade_id": "cell-4d73d4cde2dbed3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDO here. '''\n",
    "# For more info about the `pearsonr` function, check\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\n",
    "# Want a challenge? Try to compute the correlation from scratch!\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f019ce336261559ffa3be9966a4f89f",
     "grade": true,
     "grade_id": "cell-3ec0ab8df9fed33b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the ToDo above. '''\n",
    "from niedu.tests.nipa.week_1 import test_t1t2_corr\n",
    "test_t1t2_corr(lsa_dm, corr_t1t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dff365ba80dac0d5218bb2d3218898c0",
     "grade": true,
     "grade_id": "cell-b9b52f9ef36117c4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's actually fit the model! When dealing with real fMRI data, we'd use Nilearn to fit our GLM, but for now, we'll just use our own implementation of an (OLS) GLM. Note that we can actually fit a *single* GLM for all voxels at the same time by using `ts` (a $T \\times K$ matrix) as our dependent variable due to the magic of linear algebra. In other words, we can run $K$ OLS models at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use 'X', because it's shorter\n",
    "X = lsa_dm.values\n",
    "\n",
    "# Note we can fit our GLM for all K voxels at \n",
    "# the same time! As such, betas is not a vector,\n",
    "# but an n_regressor x k_voxel matrix!\n",
    "beta_hat_all = np.linalg.inv(X.T @ X) @ X.T @ ts\n",
    "print(\"Shape beta_hat_all:\", beta_hat_all.shape)\n",
    "\n",
    "# Ah, the beta for the intercept is still in there\n",
    "# Let's remove it\n",
    "beta_icept = beta_hat_all[-1, :]\n",
    "beta_hat = beta_hat_all[:-1, :]\n",
    "print(\"Shape beta_hat (intercept removed):\", beta_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's visualize the estimated parameters ($\\hat{\\beta}$). We'll do this by plotting the scaled regressors (i.e., $X_{j}\\hat{\\beta}_{j}$) on top of the original signal. Each differently colored line represents a different regressor (so a different trial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, sharex=True, sharey=True, figsize=(10, 12))\n",
    "t = np.arange(ts.shape[0])\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # Plot signal\n",
    "    ax.plot(ts[:, i], t, marker='o', ms=4, lw=0.5, c='tab:blue')\n",
    "    # Plot trial onsets (as arrows)\n",
    "    for ii, to in enumerate(onsets):\n",
    "        color = 'tab:red' if ii % 2 == 0 else 'tab:orange'\n",
    "        ax.arrow(-1.5, to / TR, dy=0, dx=0.5, color=color, head_width=0.75, head_length=0.25)\n",
    "\n",
    "    # Compute x*beta for icept only \n",
    "    scaled_icept = lsa_dm.iloc[:, -1].values * beta_icept[i]\n",
    "    for ii in range(N):\n",
    "        this_x = lsa_dm.iloc[:, ii].values\n",
    "        # Compute x*beta for this particular trial (ii)\n",
    "        xb = scaled_icept + this_x * beta_hat[ii, i]\n",
    "        ax.plot(xb, t, lw=2)\n",
    "\n",
    "    ax.set_xlim(-1.5, 2)\n",
    "    ax.set_ylim(0, ts.shape[0] // 2)\n",
    "    ax.grid(b=True)\n",
    "    ax.set_title(f'Voxel {i+1}', fontsize=15)\n",
    "    ax.invert_yaxis()\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Time (volumes)\", fontsize=20)\n",
    "\n",
    "# Common axis labels\n",
    "fig.text(0.425, -.03, \"Activation (A.U.)\", fontsize=20)\n",
    "fig.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, though, the estimated GLM parameters are just another way to estimate our pattern array ($\\mathbf{R}$) &mdash; this time, we just estimated it using a different method (GLM-based) than before (timepoint-based). Therefore, let's visualize this array as we did with the other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2, 10))\n",
    "mapp = ax.imshow(beta_hat)\n",
    "cbar = fig.colorbar(mapp)\n",
    "cbar.set_label(r'$\\hat{\\beta}$', fontsize=25, rotation=0, labelpad=10)\n",
    "ax.set_yticks(np.arange(N))\n",
    "ax.set_xticks(np.arange(K))\n",
    "ax.set_title(r\"$\\mathbf{R}$\", fontsize=20)\n",
    "ax.set_xlabel('Voxels', fontsize=15)\n",
    "ax.set_ylabel('Trials', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (optional, 0 points): It would be nice to visualize the patterns, but this is very hard because we have four dimenions (because we have four voxels)! :-( <br>PCA to the rescue! Run PCA on the estimated patterns (<tt>beta_hat</tt>) and store the PCA-transformed  array (shape: $20 \\times 2$) in a variable named <tt>beta_hat_2d</tt>. Then, try to plot the first two components as a scatterplot. Make it even nicer by plotting the trials from condition A as red points and trials from condition B als orange points. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55b91263fb9cf92944962e6d49591413",
     "grade": false,
     "grade_id": "cell-3761ab8fbc02e01f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "483cbcc7675aea5b4c0ae7593106dcff",
     "grade": true,
     "grade_id": "cell-a678573cb4d98fa4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from niedu.tests.nipa.week_1 import test_pca_beta_hat\n",
    "test_pca_beta_hat(beta_hat, beta_hat_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise normalization\n",
    "One often used preprocessing step for pattern analyses (using GLM-estimation methods) is to use \"noise normalization\" on the estimated patterns. There are two flavours: \"univariate\" and \"multivariate\" noise normalization. In univariate noise normalization, the estimated parameters ($\\hat{\\beta}$) are divided (normalized) by the standard deviation of the estimated parameters &mdash; which you might recognize as the formula for $t$-values (for a contrast against baseline)!\n",
    "\n",
    "\\begin{align}\n",
    "t_{c\\hat{\\beta}} = \\frac{c\\hat{\\beta}}{\\sqrt{\\hat{\\sigma}^{2}c(X^{T}X)^{-1}c^{T}}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat{\\sigma}^{2}$ is the estimate of the error variance (sum of squared errors divided by the degrees of freedom) and $c(X^{T}X)^{-1}c^{T}$ is the \"design variance\". Sometimes, especially in designs with fixed inter-stimulus intervals, people disregard the design variance and the degrees of freedom (DF) and instead only use the standard deviation of the noise: \n",
    "\n",
    "\\begin{align}\n",
    "t_{c\\hat{\\beta}} \\approx \\frac{c\\hat{\\beta}}{\\sqrt{\\sum (y_{i} - X_{i}\\hat{\\beta})^{2}}}\n",
    "\\end{align}\n",
    "\n",
    "Either way, this univariate noise normalization is a way to \"down-weigh\" the uncertain (noisy) parameter estimates. Although this type of univariate noise normalization seems to lead to better results in both decoding and RSA analyses (e.g., [Misaki et al., 2010](https://www.ncbi.nlm.nih.gov/pubmed/20580933)), the jury is still out on this issue.\n",
    "\n",
    "Multivariate noise normalization will be discussed in week 3 (RSA), so let's focus for now on the implementation of univariate noise normalization using the approximate method (which disregards design variance). To compute the standard deviation of the noise ($\\sqrt{\\sum (y_{i} - X_{i}\\hat{\\beta})^{2}}$), we first need to compute the noise, i.e., the unexplained variance ($y - X\\hat{\\beta}$) also known as the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = ts - X @ beta_hat_all\n",
    "print(\"Shape residuals:\", residuals.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for each voxel ($K=4$), we have a timeseries ($T=100$) with unexplained variance (\"noise\"). Now, to get the standard deviation across all voxels, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_noise = np.std(residuals, axis=0)\n",
    "print(\"Shape noise std:\", std_noise.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the actual normalization step, we simply divide the columns of the pattern matrix (`beta_hat`, which we estimated before) by the estimated noise standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unn = univariate noise normalization\n",
    "# Note that we don't have to do this for each trial (row) separately\n",
    "# due to Numpy broadcasting!\n",
    "R_unn = beta_hat / std_noise\n",
    "print(\"Shape R_unn:\", R_unn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2, 10))\n",
    "mapp = ax.imshow(R_unn)\n",
    "cbar = fig.colorbar(mapp)\n",
    "cbar.set_label(r'$t$', fontsize=25, rotation=0, labelpad=10)\n",
    "ax.set_yticks(np.arange(N))\n",
    "ax.set_xticks(np.arange(K))\n",
    "ax.set_title(r\"$\\mathbf{R}_{unn}$\", fontsize=20)\n",
    "ax.set_xlabel('Voxels', fontsize=15)\n",
    "ax.set_ylabel('Trials', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): In fact, univariate noise normalization didn't really change the pattern matrix much. Why do you think this is the case for our simulation data? Hint: check out the parameters for the simulation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "777029e3f312b9e60b4f70a511759cca",
     "grade": true,
     "grade_id": "cell-816e2fc64b38fe78",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA on real data\n",
    "Alright, enough with all that fake data &mdash; let's work with some real data! We'll use the face perception task data from the *NI-edu* dataset, which we briefly mentioned in the fMRI-introduction course.\n",
    "\n",
    "In the face perception task, participants were presented with images of faces (from the publicly available [Face Research Lab London Set](https://figshare.com/articles/Face_Research_Lab_London_Set/5047666)). In total, frontal face images from 40 different people (\"identities\") were used, which were either without expression (\"neutral\") or were smiling. Each face image (from in total 80 faces, i.e., 40 identities $\\times$ 2, neutral/smiling) was shown, per participant, 6 times across the 12 runs (3 times per session). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>Mini ToThink</b> (0 points): Why do you think we show the same image multiple times?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identities were counterbalanced in terms of biological sex (male vs. female) and ethnicity (Caucasian vs. East-Asian vs. Black). The Face Research Lab London Set also contains the age of the people in the stimulus dataset and (average) attractiveness ratings for all faces from an independent set of raters. In addition, we also had our own participants rate the faces on perceived attractiveness, dominance, and trustworthiness after each session (rating each face, on each dimension, four times in total for robustness). The stimuli were chosen such that we have many different attributes that we could use to model brain responses (e.g., identity, expression, ethnicity, age, average attractiveness, and subjective/personal perceived attractiveness/dominance/trustworthiness).\n",
    "\n",
    "In this paradigm, stimuli were presented for 1.25 seconds and had a fixed interstimulus interval (ISI) of 3.75 seconds. While sub-optimal for univariate \"detection-based\" analyses, we used a fixed ISI &mdash; rather than jittered &mdash; to make sure it can also be used for \"single-trial\" multivariate analyses, which is the topic of our follow-up \"pattern analysis\" course). Each run contained 40 stimulus presentations. To keep our subjects attentive, a random selection of 5 stimuli (out of 40) were followed by a rating on either perceived attractiveness, dominance, or trustworthiness using a button-box with eight buttons (four per hand) lasting 2.5 seconds. After the rating, a regular ISI of 3.75 seconds followed. See the figure below for a visualization of the paradigm.\n",
    "\n",
    "![face_paradigm](https://docs.google.com/drawings/d/e/2PACX-1vQ0FlwZLI_XMHaKkaNchzZvgqT0JXjZAPbH9fccmNvgey-RYR5bKolh85Wctc2YLrjOLtE3Zkd7WXdu/pub?w=1429&h=502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up all the data that we need for our LSA model. Let's see where our data is located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niedu.global_utils import get_data_dir\n",
    "data_dir = get_data_dir('pattern-analysis')\n",
    "print(\"Data is located in\", data_dir)\n",
    "print(\"Contents:\", os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it contains both \"raw\" (not-preprocessed) subject data (e.g., sub-03) and derivatives, which include Fmriprep-preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprep_sub03 = os.path.join(data_dir, 'derivatives', 'fmriprep', 'sub-03')\n",
    "print(\"Contents derivatives/fmriprep/sub-03:\", os.listdir(fprep_sub03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is preprocessed anatomical data and session-specific functional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprep_sub03_ses1_func = os.path.join(fprep_sub03, 'ses-1', 'func')\n",
    "print(\"Contents ses-1/func:\", '\\n'.join(os.listdir(fprep_sub03_ses1_func)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of data! Importantly, we will only use the \"face\" data (\"task-face\") in T1 space (\"space-T1w\"), meaning that this dat has not been normalized to a common template (unlike the \"task-MNI152NLin2009cAsym\" data). Here, we'll only analyze the first run (\"run-1\") data. Let's define the functional data, the associated functional brain mask (a binary image indicating which voxels are brain and which are not), and the file with timepoint-by-timepoint confounds (such as motion parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = os.path.join(fprep_sub03_ses1_func, 'sub-03_ses-1_task-face_acq-Mb4Mm27Tr700_run-1_space-T1w_desc-preproc_bold.nii.gz')\n",
    "\n",
    "# Notice this neat little trick: we use the string method \"replace\" to define\n",
    "# the functional brain mask\n",
    "func_mask = func.replace('desc-preproc_bold', 'desc-brain_mask')\n",
    "\n",
    "confs = os.path.join(fprep_sub03_ses1_func, 'sub-03_ses-1_task-face_acq-Mb4Mm27Tr700_run-1_desc-confounds_regressors.tsv')\n",
    "confs_df = pd.read_csv(confs, sep='\\t')\n",
    "confs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the events-file with onsets, durations, and trial-types for this particular run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = os.path.join(data_dir, 'sub-03', 'ses-1', 'func', 'sub-03_ses-1_task-face_acq-Mb4Mm27Tr700_run-1_events.tsv')\n",
    "events_df = pd.read_csv(events, sep='\\t')\n",
    "events_df.query(\"trial_type != 'rating' and trial_type != 'response'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's up to you to use this data to fit an LSA model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): in this first ToDo, you define your events and the confounds you want to include.<br>\n",
    "    \n",
    "1. You first need to filter the events DataFrame to get rid of the \"response\" and \"rating\" events (i.e., events with the <tt>trial_type</tt> \"response\" or \"rating\") using your knowledge of Pandas! Also, remove all columns except \"onset\", \"duration\", and \"trial_type\". You should end up with a DataFrame with 40 rows and 3 columns. You can check this with the <tt>.shape</tt> attribute of the DataFrame. (Note that, technically, you could model the reponse and rating-related events as well! For now, we'll exclude them.) Name this filtered DataFrame <tt>events_df_filt</tt>.\n",
    "    \n",
    "2. You also need to select specific columns from the confounds DataFrame, as we don't want to include <em>all</em> confounds! For now, include only the motion parameters (<tt>trans_x, trans_y, trans_z, rot_x, rot_y, rot_z</tt>). You should end up with a confounds DataFrame with 342 rows and 6 columns. Name this filtered DataFrame <tt>confs_df_filt</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6709378a52e94f8e55dc96961e4c0b36",
     "grade": false,
     "grade_id": "cell-3deefaae7fd6c083",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7e0338f21abad8322400c6b0634dace",
     "grade": true,
     "grade_id": "cell-c767561caf74e13b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "assert(events_df_filt.shape == (40, 3))\n",
    "assert(events_df_filt.columns.tolist() == ['onset', 'duration', 'trial_type'])\n",
    "assert(confs_df_filt.shape == (342, 6))\n",
    "assert(all('trans' in col or 'rot' in col for col in confs_df_filt.columns))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): in this Todo, you'll fit your model! Define a <tt>FirstLevelModel</tt> object, name this <tt>flm_todo</tt> and make sure you do the following:<br>\n",
    "    \n",
    "1. Set the correct TR (this is 0.7)\n",
    "2. Set the slice time reference to 0.5\n",
    "3. Set the mask image to the one we defined before\n",
    "4. Use a \"glover\" HRF\n",
    "5. Use a \"cosine\" drift model with a cutoff of 0.01 Hz\n",
    "6. Do not apply any smoothing\n",
    "7. Set minimize_memory to true\n",
    "8. Use an \"ols\" noise model\n",
    "\n",
    "Then, fit your model using the functional data (<tt>func</tt>), filtered confounds, and filtered events we defined before. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d60411419b58dc89817bc75e8529fcf",
     "grade": false,
     "grade_id": "cell-f514e371d2e125de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# Ignore the DeprecationWarning!\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa3fdd223c9fb057633fa98dcd73c610",
     "grade": true,
     "grade_id": "cell-235ab1374d1651f8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_1 import test_lsa_flm\n",
    "test_lsa_flm(flm_todo, func_mask, func, events_df_filt, confs_df_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): in this final Todo, you'll run the single-trial contrasts (\"against baseline\"). To do so, write a for-loop in which you call the <tt>.compute_contrast</tt> method every iteration with a new contrast definition for a new trial. Note that the <tt>compute_contrast</tt> method returns the \"unmasked\" results (i.e., from all voxels). Make sure that, for each trial, you mask the results using the <tt>func_mask</tt> variable and the <tt>apply_mask</tt> function from Nilearn. Save these masked results (which should be patterns of 66298 voxels) for each trial. After the loop, stack all results in a 2D array with the different trials in different rows and the (flattened) voxels in columns. This array should be of shape 40 (trials) by 66298 (nr. of masked voxels). The variable name of this array should be <tt>R_todo</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b2ad7a58bcffc0a5c0f7a6bfaf4b79c",
     "grade": false,
     "grade_id": "cell-89855d42e59da394",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4733f9d7a35692f3779de97e47337386",
     "grade": true,
     "grade_id": "cell-3674c196f5a35cd6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_1 import test_lsa_R\n",
    "test_lsa_R(R_todo, events_df_filt, flm_todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Disclaimer</b> In this ToDo, we asked you <em>not</em> to spatially smooth the data. This is often recommended for pattern analyses, as they arguably use information that is encoded in finely distributed patterns. However, several studies have shown that smoothing may sometimes benefit pattern analyses (e.g., <a href=\"https://www.frontiersin.org/articles/10.3389/fneur.2017.00222/full\">Hendriks et al., 2017</a>). In general, in line with the <a href=\"https://en.wikipedia.org/wiki/Matched_filter\">matched filter theorem</a>, we recommend smoothing your data with a kernel equal to how finegrained you think your experimental feature is encoded in the brain patterns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that was it for this lab! We have covered the basics of experimental design and pattern estimation techniques for fMRI data. Note that there are many other (more advanced) things related to pattern estimation that we haven't discussed, such as standardization of patterns, multivariate noise normalization, [temporal decorrelation](https://www.sciencedirect.com/science/article/pii/S1053811919310407), [hyperalignment](https://www.sciencedirect.com/science/article/pii/S0896627311007811), etc. etc. Some of these topics will be discussed in week 2 (decoding) or week 3 (RSA).\n",
    "\n",
    "For those looking for a programming challenge, take a look at the next (optional) section on a variant of LSA: least-squares separate (for more info, see [this article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3251697/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least-squares separate (LSS)\n",
    "This section is about a slight modifcation of the LSA design: the least-squares separate (LSS) design. In LSS, you fit a separate model per trial. Each model contains one regressor for the trial that you want to estimate and, per condition (in case of a categorical design, e.g., A vs B vs C) another regressors containing all other trials. So, suppose you have a run with 30 trials across 3 conditions (A, B, and C); using an LSS approach, you'd fit 30 different models, each containing four regressors (one for the single trial, one for all (other) trials of condition A, one for all (other) trials of condition B, and one for all (other) trials of condition C). The apparent upside of this is that it strongly reduces the collinearity of trials close in time, which in turn makes the trial parameters more efficient to estimate.\n",
    "\n",
    "We'll show this for our example data (using plain Python code, not Nilearn). It's a bit complicated (and not necessarily the best/fastest/clearest way), but the comments will explain what it's doing. Essentially, what we're doing, for each trial, is to extract that regressor for a standard LSA design and, for each condition, create a single regressor by summing all single-trial regressors from that condition together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, well make a standard LSA design matrix\n",
    "lsa_dm = make_first_level_design_matrix(\n",
    "    frame_times=t_fmri,  # we defined this earlier for interpolation!\n",
    "    events=events_sim,\n",
    "    hrf_model='glover',\n",
    "    drift_model=None  # assume data is already high-pass filtered\n",
    ")\n",
    "\n",
    "# Then, we will loop across trials, making a single GLM\n",
    "lss_dms = []  # we'll store the design matrices here\n",
    "\n",
    "# Do not include last column, the intercept, in the loop\n",
    "for i, col in enumerate(lsa_dm.columns[:-1]):    \n",
    "    # Extract the single-trial predictor\n",
    "    single_trial_reg = lsa_dm.loc[:, col]\n",
    "\n",
    "    # Now, we need to create a predictor per condition\n",
    "    # (one for A, one for B). We'll store these in \"other_regs\"\n",
    "    other_regs = []\n",
    "\n",
    "    # Loop across unique conditions (\"A\" and \"B\")\n",
    "    for con in np.unique(conditions):\n",
    "        # Which columns belong to the current condition?\n",
    "        idx = con == np.array(conditions)\n",
    "        \n",
    "        # Make sure NOT to include the trial we're currently estimating!\n",
    "        idx[i] = False\n",
    "        \n",
    "        # Also, exclude the intercept (last column)\n",
    "        idx = np.append(idx, False)\n",
    "        \n",
    "        # Now, extract all N-1 regressors\n",
    "        con_regs = lsa_dm.loc[:, idx]\n",
    "        \n",
    "        # And sum them together!\n",
    "        # This creates a single predictor for the current\n",
    "        # condition\n",
    "        con_reg_all = con_regs.sum(axis=1)\n",
    "            \n",
    "        # Save for later\n",
    "        other_regs.append(con_reg_all)\n",
    "\n",
    "    # Concatenate the condition regressors (one of A, one for B)\n",
    "    other_regs = pd.concat(other_regs, axis=1)\n",
    "    \n",
    "    # Concatenate the single-trial regressor and two condition regressors\n",
    "    this_dm = pd.concat((single_trial_reg, other_regs), axis=1)\n",
    "\n",
    "    # Add back an intercept!\n",
    "    this_dm.loc[:, 'intercept'] = 1\n",
    "    \n",
    "    # Give it sensible column names\n",
    "    this_dm.columns = ['trial_to_estimate'] + list(set(conditions)) + ['intercept']\n",
    "\n",
    "    # Save for alter\n",
    "    lss_dms.append(this_dm)\n",
    "\n",
    "print(\"We have created %i design matrices!\" % len(lss_dms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now let's check out the first four design matrices, which should estimate the first four trials and both contain 4 regressors (one for the single trial, two for the separate conditions, and one for the intercept):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, figsize=(15, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    plot_design_matrix(lss_dms[i], ax=ax)\n",
    "    ax.set_title(\"Design matrix for trial %i\" % (i+1), fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (optional; 1 bonus point): Can you implement an LSS approach to estimate our patterns on the real data? You can reuse the <tt>flm_todo</tt> you created earlier; the only thing you need to change each time is the design matrix. Because we have 40 trials, you need to fit 40 different models (which takes a while). Note that our experimental design does not necessarily have discrete categories, so your LSS design matrices should only have 3 columns: one for the trial to estimate, one for all other trials, and one for the intercept. After fitting each model, compute the trial-against-baseline contrast for the single trial and save the parameter (\"beta\") map. Then, after the loop, create the same pattern matrix as the previous ToDo, which should also have the same shape, but name it this time <tt>R_todo_lss</tt>. Note, this is a <em>very</em> hard ToDo, but a great way to test your programming skills :-)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a0146546564d81a5bfa07ab16c6053c",
     "grade": false,
     "grade_id": "cell-554ffaf49a462e46",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. Note that we already created the LSA design matrix for you. '''\n",
    "func_img = nib.load(func)\n",
    "n_vol = func_img.shape[-1]\n",
    "lsa_dm = make_first_level_design_matrix(\n",
    "    frame_times=np.linspace(0, n_vol * 0.7, num=n_vol, endpoint=False),\n",
    "    events=events_df_filt,\n",
    "    drift_model=None\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee50ae9b5644ba73bd0b1feaf70fec43",
     "grade": true,
     "grade_id": "cell-38c42537ef715b72",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from niedu.tests.nipa.week_1 import test_lss\n",
    "test_lss(R_todo_lss, func, flm_todo, lsa_dm, confs_df_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "    <b>Tip</b>: Programming your own pattern estimation pipeline allows you to be very flexible and is a great way to practice your programming skills, but if you want a more \"pre-packaged\" tool, I recommend the <a href=\"https://nibetaseries.readthedocs.io/en/stable/\">nibetaseries</a> package. The package's name is derived from a specific analysis technique called \"beta-series correlation\", which is a type of analysis that allows for resting-state like connectivity analyses of task-based fMRI data (which we won't discuss in this course). For this technique, you need to estimate single-trial activity patterns &mdash; just like we need to do for pattern analyses! I've used this package to estimate patterns for pattern analysis and I highly recommend it!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
